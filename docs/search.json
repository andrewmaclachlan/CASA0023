[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "CASA0023 Remotely Sensing Cities and Environments",
    "section": "Welcome",
    "text": "Welcome\nHello  and welcome to the Term 2 module Remotely Sensing Cities and Environments.\nSimilarly, to my Term 1 MSc module, CASA0005, this website holds all the practical instructions for the module. CASA0005 Geographic Information Systems and Science (or a similar module) is a pre-requisite of this module so concepts taught there will mainly be assumed here."
  },
  {
    "objectID": "index.html#acknowledgement",
    "href": "index.html#acknowledgement",
    "title": "CASA0023 Remotely Sensing Cities and Environments",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThanks to the following academics who inspired the creating of the module and various concepts within it:\n\nDr Ellie Biggs\nDr Gareth Roberts\nDr Bryan Boruff\nProfessor Ted Milton\nDr Laurie Chisholm\n\nThanks again to the following people who have either contributed directly or provided code in repositories that have helped me style this book:\n\n\nR Studio\n\nSTAT 545\nrstudio4edu\nHadley Wickham\nAlison Presmanes Hill\nDesirée De Leon\nYihui Xie\nJulia Silge\nJenny Bryan\nOthers\n\nRobin Lovelace\nTwitter for R programmers\nMatt Ng\nStatQuest with Josh Starmer\nGarrick Aden‑Buie\n\n\n\nThe R package and analysis artwork used within this book has been produced by allison_horst, whilst artwork used in information boxes has been produced by Desirée De Leon. You can find Allison’s images on the stats illustration GitHub repository and Desirée’s on the rstudio4edu GitHub repository.\nI’ve certainly learnt a lot from their open code repositories!"
  },
  {
    "objectID": "00-course_info.html#books",
    "href": "00-course_info.html#books",
    "title": "Hello Remote Sensing",
    "section": "Books",
    "text": "Books\nThere are two main books used in this module.\n\nIntroductory Digital Image Processing: A Remote Sensing Perspective, Jensen (2015). Available as a physical copy or online via the UCL library\nEarth Engine Fundamentals and Applications (author’s live version) and associated website"
  },
  {
    "objectID": "00-course_info.html#schedule",
    "href": "00-course_info.html#schedule",
    "title": "Hello Remote Sensing",
    "section": "Schedule",
    "text": "Schedule\n\n\nWeek\nLecture\nPresentation\nPractical\nPractical material\n\n\n\n1\nAn Introduction to Remote Sensing\n\nGetting started with remote sensing\n\n\n\n2\nPortfolio tools: Xaringan and Quarto\n\nPortfolio\n\n\n\n3\nRemote sensing data\n\nCorrections\n\n\n\n4\nPolicy applications\n\nPolicy\n\n\n\n5\nGoogle Earth Engine\n\nGoogle Earth Engine\n\n\n\n\nReading week\n\n\n\n\n\n6\nClassification\n\nClassification I\n\n\n\n7\nClassification the big questions (lecture 5 continued) and accuracy\n\nClassification II\n\n\n\n8\nTemperature and policy\n\nTempearture\n\n\n\n9\nSAR\n\nSAR in GEE\n\n\n\n10\nLearning diary Q and A\n\nGroup presentations"
  },
  {
    "objectID": "00-course_info.html#intended-course-learning-outcomes",
    "href": "00-course_info.html#intended-course-learning-outcomes",
    "title": "Hello Remote Sensing",
    "section": "Intended course learning outcomes",
    "text": "Intended course learning outcomes\nAt the end of this module students will be able to:\n\nCreate a reproducible online portfolio workbook\nExplain and evaluate common issues with urban and environmental policies at the local, national and international level that fail to consider spatial data\nRevise vague and ambiguous development targets\nAppropriately pre-process Earth observation imagery ready for analysis\nApply published methodologies to extract meaning from Earth observation data\nCombine a variety of spatial data to demonstrate the benefits of data-informed governance and planning.\nCreate and design a reproducible workflow for consistent monitoring of urban and environmental metrics\nCritique and optimise recently developed metropolitan climate mitigation strategies using appropriate spatial data, optimizing financial investment and environmental outcomes"
  },
  {
    "objectID": "00-course_info.html#how-to-use-this-book",
    "href": "00-course_info.html#how-to-use-this-book",
    "title": "Hello Remote Sensing",
    "section": "How to use this book",
    "text": "How to use this book\nThis website is hosted on GitHub and holds all the practical instructions and data. Data used within the practicals is available online, however occasionally websites can undergo maintenance or be inaccessible due to political factors such as government shutdowns.\nTo get the most out of this book spend a few minutes learning how to control it, in the top right of this webpage you will see the following tools:\n\n search the entire book for a specific word\n control the side / menu bars\n changes to light or dark mode\n GitHub repository for this book"
  },
  {
    "objectID": "00-course_info.html#software-installation",
    "href": "00-course_info.html#software-installation",
    "title": "Hello Remote Sensing",
    "section": "Software installation",
    "text": "Software installation\nThis course uses a range of software, some of which we have encountered before:\nSNAP\nSentinels Application Platform (SNAP) is open source software and has common tools and methods used on earth observation (EO) data. It’s like QGIS for EO data, developed by the European Space Agency (ESA).\nDownload the all toolboxes option for your operating system\nGoogle Earth Engine (GEE)\nGoogle Earth Engine isn’t software as such, but we will be using it within the module. You need:\n\nA free Google account\nTo sign up for GEE\n\nR\nYou should have both R and RStudio installed from CASA0005\nQGIS\nYou should have QGIS installed from CASA0005"
  },
  {
    "objectID": "intro.html#learning-outcomes",
    "href": "intro.html#learning-outcomes",
    "title": "\n1  Getting started with remote sensing\n",
    "section": "\n1.1 Learning outcomes",
    "text": "1.1 Learning outcomes\nBy the end of this practical you should be able to:\n\nSource, load and articulate the differences between Landsat and Sentinel data\nUndertake basic raster image statistics and processing\nEvaluate the (dis)advantages of each type of software you have used\nPull out and statistically compare spectral signatures"
  },
  {
    "objectID": "intro.html#resources",
    "href": "intro.html#resources",
    "title": "\n1  Getting started with remote sensing\n",
    "section": "\n1.2 Resources",
    "text": "1.2 Resources\n\n\n\n\n\n\nThis week\n\n\n\n\n\nTempfli, K., Huurneman, Gc., Bakker, Wh., Janssen, L.L., Feringa, W., Gieske, A., Grabmaier, K., Hecker, C., Horn, J., Kerle, N., others, 2009. Principles of remote sensing: an introductory textbook. International Institute for Geo-Information Science and Earth Observation\n\nElectromagnetic energy and remote sensing, Chapter 2\nOverview of popular spaceborne sensors, Section 4.6.\n\n\n\nJensen, J.R., 2015. Introductory digital image processing: a remote sensing perspective. Prentice-Hall Inc.\n\nRemote Sensing and Digital Image Processing, Chapter 1, pages 1-18 and from the bottom of page 23 (Remote Sensing Data Analysis) to the end of page 27.\n\n\nBrady, M., 2021. Remote Sensing for Dummies (accessed 1.4.23).\n\nButcher, G., 2016. Tour of the electromagnetic spectrum, Third edition. ed. National Aeronautics and Space Administration, Washington, DC.\n\nRadiation budget\nReflected near-infrared waves"
  },
  {
    "objectID": "intro.html#eo-dashboard",
    "href": "intro.html#eo-dashboard",
    "title": "\n1  Getting started with remote sensing\n",
    "section": "\n1.3 EO dashboard",
    "text": "1.3 EO dashboard\nBefore we start the practical, spend 5 minutes exploring the EOdashboard website. It contains a useful overview of some of the EO data that we can and will explore later in the module."
  },
  {
    "objectID": "intro.html#gitignore",
    "href": "intro.html#gitignore",
    "title": "\n1  Getting started with remote sensing\n",
    "section": "\n1.4 .gitignore",
    "text": "1.4 .gitignore\nIf you are using Git with your project and have large data it’s best to set up a .gitignore file. This file tells git to ignore certain files so they aren’t in your local repository that is then pushed to GitHub. If you try and push large files to GitHub you will get an error and could run into lots of problems, trust me, i’ve been there. Look at my .gitignore for this repository and you will notice:\n/prac_1/*\nThis means ignore all files within these folders, all is denoted by the *. The .gitignore has to be in your main project folder, if you have made a a git repo for your project then you should have a .gitignore so all you have to do is open it and add the folder or file paths you wish to ignore.\nGitHub has a maximum file upload size of 50mb….\n\n\n\n\nGitHub maximum file upload of 50mb. Source: reddit r/ProgrammerHumor\n\n\n\n\n\n1.4.1 Sentinel download\n\nGo to the Copernicus Open Access Hub\n\nYou will need to make a free account\n\nDraw around the study area\nSelect image filter criteria\nSelect search icon\nDownload the S2MSI2A data\n\n\n\n\n\n\n\n\n\n\n\nWithin this image we have:\n\nAllows you to either move around the global or draw a study area\nAn example of a study area\nHow to sort the image results\nSentinel mission selection :\n\n\nSentinel product types refer to the amount of processing that has been undertaken on the multi-spectral imagery. S2MSI2A = Bottom of Atmosphere (BOA) or otherwise known as surface reflectance. Consult the product specification for more details. We cover different products (e.g. BOA or Top of Atmosphere, TOA) in future sessions.\n\nYou can also use the Sentinel 2 toolbox to replicate the conversation from TOA to BOA\n\nThe platform refers to either sentinel 2A or 2B. Theses are the same sensors but they operate at 180 degrees from each other reducing the revisit time from 10 to 5 days. 2A was launched first in 2015 followed by 2B in 2017.\nCloud cover e.g. [0 TO 5]\nMy image was from the 15/5/2022\n\n\n1.4.1.1 Open\nOnce downloaded and unzipped you’ll be presented with a load of folders! Here we are interested in the 10m bands…which are:\n\n\n\n\n\n\n\n\nYou’ll find then in the GRANULE > sensor number > IMG_DATA > R10.\nNext open them up in QGIS for some exploration, if there is a TCI image this is a True Colour Image of B02 (Blue), B03 (Green), and B04 (Red) Bands - open this first, it’s just a single raster layer. See the Sentinel user guide definitions for any other acronyms you might need.\n\n\n\n\n\n\n\n\nUsing the Identify tool we can create a spectral signature by changing the view option to graph (look under the graph in the image above).\nHowever the TCI values are coded between 0 and 255 which will limit what we can do with it. As the the radiometric resolution of Sentinel-2 is 12-bit, meaning brightness levels from 0 - 4095 it’s not clear how this product has been made.\nSo we can make our own raster stack using the BOA bands, if you recall we did this in CASA0005 in R.\nBut in QGIS it’s easier to visualise the output\nFirst load the four 10M bands from the same folder e.g…T34HBH_20220515T081611_B02_10m\nFind the merge tool from the Raster miscellaneous tool box and select the following:\n\n\n\n\n\n\n\n\nNote:\n\nThe input layers are the four 10m optical bands (not the AOT band) that I loaded into QGIS. AOT is Aerosol Optical Thickness.\nTick the box to place each raster file will be it’s own band\nThe file being saved into a .tiff as opposed to memory\n\nOnce merged we can created a true colour composite using the BOA data…to do so:\n\nRight click on the merged layer in the attribute table\nSymbology > Render type > select multiband color\n\nIn remote sensing the Red, Green, Blue display options are often called colour guns that are used to display images on the screen. For a true colour composite B1=Blue, B2=Green, B3=Red.\n\n\n\n\n\n\n\n\nTry changing the contrast enhancement and see what happens, then consult GIS stack exchange to understand what is happening..\nOf course we have only used the 10m bands so far…there are two options that we can take to use the full range of spectral data:\n\nDownscale the other bands to 10m forming a super-resolution\nUpscale the 10m to 20m\n\nDown scaling is quite an intensive process and beyond the scope of this practical. However, it can be achieved using Sen2Res that is another plug in for the SNAP toolbox. Arguably SNAP is just a difference type of GIS software specific to Sentinel, but we will explore it later as it makes some of these concepts easier to understand.\nUpscaling aggregates the images to a more coarse resolution (e.g. 10m to 20m). The Sentinel user guide states that bands will be resampled and provided (e.g. within the 20m folder there are 10 bands). However, it’s not clear what method has been used, the documentation suggests nearest neighbour.\nThis can also be termed resampling, and nearest neighbour simply means taking the new raster resolution and then assigning the value from the closest value in the original data. You might recall from CASA0005 there are different ways to define this, for example is nearest neighbour based on the distance from the center of a cell or the majority of cells, the latter would be termed majority rule.\n\n\n\n\nGraphic representation of Nearest Neighbour (NN) and Majority Rule (MR). Source: David García Álvarez, 2021\n\n\n\n\nOthers approaches include bilinear or cubic convolution resampling. See stack exchange for a worked example.\n\n1.4.1.2 SNAP\nSNAP stands for Sentinels Application Platform it is a collection of toolboxes specifically for pre-processing and analysing remotely sensed data.\nDownload the all toolboxes version\nSNAP allows us to easily do undertake many GIS raster type analysis that we’ve seen / discussed in other modules (like CASA0005) and that we will come across within this module including:\n\nre sampling raster data\nre projecting\nmasking\nclassifying data\nprincipal component analysis\northorectification\nmany more methods!\n\nAside from these methods in a GUI the real benefit is that it’s made to use remotely sensed data. Let’s explore some features of SNAP.\n\n1.4.1.2.1 Load data\n\nFile > Open Product > select the .zip that was downloaded. Do not rename it or unzip it before hand!\nIn the side bar under Product Explorer there will be a lot on data that you can load. The remotely sensed data in under Bands and then each band is listed (e.g. B1 443nm). What does 443nm mean?\nDouble right click on a layer and it will appear in the viewer area\n\n1.4.1.2.2 SNAP layout\nThe layout of SNAP isn’t too different to QGIS, we have:\n\nProducts on the left side bar\nMap info in the bottom left\n\nadditional panes of info can be added through view >toolbar windows\nnote i have added pixel info (updates when moving the cursor) and world map\n\n\nLayer / mask managers in the right sidebar\nProcessing tools in the top tool bars\n\n\n\n\n\n\n\n\n\nHere, i have also the two link buttons selected (bottom left window) these mean that if i move to another band i will still be in the same position on the image.\n\n1.4.1.2.3 Colour composites\nTo re-created a true colour composite, right click on the data product and open RGB image\n\n\n\n\n\n\n\n\nThere are a variety of other band combinations that we can use to show certain aspects of Earth’s surface based on the absorption and reflection properties of the materials in the wavelengths of the bands we display “through the colour guns”, for example:\n\nThe false colour composite: B8, B4, B3. Plants reflect near-infrared and green light whilst absorbing red….\nAtmospheric penetration composite: B12, B11, B8A with no visible bands to penetrate atmospheric particles. Vegetation = blue, urban area = white, gray cyan or purple.\nThe short-wave infrared composite: B12, B8A and B4 shows vegetation in green, the darker the greener the denser it is and brown shows built up or bare soil\n\nFor other band colour combinations consult gisgeogrpahy\n\n1.4.1.2.4 Creating a project\nBefore we go any further we should create a project, that way all our processed outputs will be stored within the projected and loaded again when we open it in future - like projects in any other software such as QGIS or R. Go File > Project > Save project. A project tab next to the product explorer tab will appear, but not you can’t access the bands from within the project that must be done through the product explorer.\n\n1.4.1.2.5 Image statistics\n\n1.4.1.2.6 Image histogram\nWhen we open an RGB image in SNAP the histogram is clipped by 1% at the lower end and 4% at the upper end and then mapped in bins between 0 and 255 for display. A computer screen in full RGB displays colours between 0 and 255, hence why this is done.\nThis is similar to the contrast enhancement we saw in QGIS and we can manually change it here again through View > Tool Windows > Colour Manipulation.\nChanging the distribution displayed will impact the colour of the image - try using the sliders or selecting 95% and 100% of all pixels to display. To reset click the back arrow button.\n\n\n\n\n\n\n\n\n\n1.4.1.2.6.1 Scatterplots\nUnder the analysis button there are a variety of tools we can use to explore some image statistics….for example, here i have created a scatter plot of band 4 (x axis) and band 8 ( y axis). These bands are the red (vegetation absorbs) and Near-infrared (NIR, that vegetation strongly reflects)…so where we have high values of NIR and low values of red the plot represents dense vegetation whilst low values of both red and NIR are usually wet bare soil. you need to click the refresh button to generate the plot.\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Remote Sensing 4113\n\n\n\n\nIn remote sensing this can be called “spectral feature space”, more on this later in the term. You might see the software ENVI in the source above - ENVI is propriety software that is similar to SNAP.\n\n1.4.1.2.6.2 Tasseled Caps\nThe output should look somewhat like a “Tasseled Cap” (a wizards hat at an angle), although this is different to the tasseled cap transformation which was proposed by Kauth and Thomas (1976). The tasseled cap transformation was originally applied to Landsat data being composed of brightness, greeness, yellow stuff (yes!) and none-such. It was then modified in 1968 to brightness, greeness and wetness. This can can useful for identifying urban areas, they are usually bright (although Andy will have more to say on this), high biomass will show in the greeness and moisture in the wetness. The tasseled cap comes from the plot between brightness on the x axis and greenness on the y axis.\nTraditionally this is usually only applied to Landsat data, however if our bands in other sensors (like Sentinel) cover the same wavelengths we can apply it… \\[\n\\begin{split}\nBrightness = 0.3037(B2)+0.2793(B3)+\\\\0.4743(B4)+0.5585(B8)+\\\\\n0.5082(B11)+0.1863(B12)\n\\end{split}\n\\] \\[\n\\begin{split}\nGreeness = −0.2848(B2)−0.2435(B3)\\\\−0.5436(B4)+0.7243(B8)+\\\\\n0.0840(B11)−0.1800(B12)\\end{split}\n\\]\n\\[\n\\begin{split}\nWetness = 0.1509(B2)+0.1973(B3)\\\\+0.3279(B4)+0.3406(B8)\\\\−\n0.7112(B11)−0.4572(B12)\n\\end{split}\n\\]\nNow, this is fairly straight forward to do in SNAP (or R) using Band Maths (Raster > Band Maths) and then clicking edit expression, before we can apply this we have two problems:\n\nB11 and B12 are at a 20 meter resolution where as all the others at a 10 meter resolution, to fix this we must re-sample to 20m\nUsing the image provided will mean that the entire tile is computed when we really only care about our study area\n\nNote, that there are many spatial indexes that can be applied to remotely sensed data, the Index DataBase holds them all or Chapter 8, pages 325 from Jensen is a good place to start.\n\n1.4.1.2.7 Masking and resampling\nThis is sometimes also called clipping in CASA0005 we saw cropping (to the extent of a polygon) and then masking. Here I am only interested in a study area of the City of Cape Town District Municipality, which is in the gadm40_ZAF_2 from the GADM data, it also appears in the gadm40_ZAF_3 data too!\nExplore the GADM data in QGIS to work out what spatial aggregation you can use to mask your study area. For the sake of this example you might want to use the smallest spatial unit, GADM level 4, which for South Africa is wards.\nSNAP only permits ESRI shapefile to be loaded! To do so you must select the product on interest in the product explorer > Vector > Import > ESRI Shapefile\nWhen you load the vector file it will appear under the image product that you had selected. To show it on the map, move to the right of the screen > select layer manager > check the layer you want to show.\n\n\n\n\n\n\n\n\nWhen you open a Shapefile in SNAP each polygon within your current extent will be made into an individual feature, opening the feature (from the Vector Data folder) will open the attribute table for the row of that polygon. You also might notice that the shapefile may have a larger extent than the image, this means we’d need to find another image and “mosaic” (merge or join together) them but we will see this later in the module, for now just accept that some area might be missing.\nIn my case the City of Capetown has been called ZAF_1 and now i will clip the raster to the polygon. However, the problem here is that we can only mask bands on the same spatial resolution, so we need to re sample bands 2,3,4 and 8 to 20m…here, we can use the Sentinel 2 resampling toolbox to resample the image and then move to masking it.\nThere are two options to resample within SNAP, a traditional resample which just considers the neighbouring pixels or the Sentinel 2 products resample to account for the particularities of the angle (satellite viewing) bands. For the sake of time we will use the traditional resample to generate a 20m raster dataset…Raster > Geometric > Resampling…\n\n\n\n\n\n\n\n\nNote that there will be both upsampling and downsampling here as i’ve selected a 20m output.\nThis will create a new product in the product explorer so make sure you use that from now we, next we can mask out the all the bands we need:\nTo mask out our study area use the Land/Sea mask (Raster > Masks > Land/Sea mask) and select the vector to use as the mask. Here, i only take forward the bands i need (scroll up to check):\n\n\n\n\n\n\n\n\nAt this stage you may want to remove the shapefile that will still be within the new product created. Under the vector folder remove the relevant polygons.\nThen compute the tasseled cap transformation (Raster > Band Maths) with the equations above. Note that when using a subtraction (-) you might get invalid expression so use the inbuilt subtraction within the edit expression button. My brightness equation was:\n\n(0.3037* $3.B2) + (0.2793 * $3.B3) + (0.4743 * $3.B4) + (0.5585 * $3.B8) + (0.5082 * 0.5082) + (0.1863 * 0.1863)\n\nThe new layers we’ve created will be added to the bands folder of the product you had selected (remember that we can only do this with bands that have the same resolution, hence our up and down sampling). We can now display the bands through the colour guns, R: Brightness, G: Greeness, B: Wetness and also create a scatter plot between Greeness (y) and Brightness (x) like we did earlier.\nAt this stage it is useful to also have open a true colour composite to compare to our tasseled cap transformation RBG image…what do the values and colours show? The Tasseled Cap function from ArcPro will help explain this further.\nThe data we have created show:\n\nBrightness, associated with spectral bands that show bare soil, man made surfaces or bight materials\nGreenness, associated with green vegetation\nWetness, associated with moisture\n\nThe article states that the Tasseled Cap function is a form or Principal Component Analysis (PCA), covered more next week. This means that we have transformed our original data into a new data set that has reduced the dimensionality. PCA reduces the size of data (dimensions) whilst maximising the variance, there are specific tools for PCA so arguably this isn’t PCA as such. I would term this a spectral index which is a combination of two or more bands to highlight certain features of an image. There are many different spectral indexes that can be implemented - see the Index DataBase holds them all or Chapter 8, pages 325 from Jensen. There are specific urban spectral indexes such as the The Normalized Difference Built-up Index (NDBI).\n\n\n\n\n\n\n\n\n\n1.4.2 Landsat\nLandsat imagery is the longest free temporal image repository of consistent medium resolution data. It collects data at each point on Earth each every 16 days (temporal resolution) in a raster grid composed of 30 by 30 m cells (spatial resolution). Geographical analysis and concepts are becoming ever more entwined with remote sensing and Earth observation.\nTo access the Landsat data we will use in this practical you can need to sign up for a free account at: https://earthexplorer.usgs.gov/.\nTo download the data:\n\nEnter your city in the address/place box > select the country > click Show and then click on the word of your city in the box that appears.\n\n\n\n\n\n\n\n\n\n\nSet the search range to similar dates that you used for the Sentinel data, in my case 05/01/2022 to 05/31/2022\nOn the cloud cover tab set the filter to between 0 and 5% (or up to 10% if needed)\nClick Data Sets and select Landsat Collection 2 Level 2 > Landsat 8-9 OLI/TIRS C2 L2 (more on what this means next week)\nClick results, my image is: LC09_L2SP_175083_20220509_20220511_02_T1. Download the complete bundle.\n\nOnce downloaded unzip the file.\nIn SNAP go File > Open Product, navigate to the .MTL and open it. The product should appear. This is a real benefit of SNAP, having data from multiple sensors in the same software that can be explored together - when you move around on the Landsat image then the Sentinel image will also move to the same location.\nTo see the bands (or RGB images) side by side use the window dividers in the top right and also ensure the views are linked in the bottom left window.\n\n\n\n\n\n\n\n\nFor the next part of the practical we will compare the spectral signatures from both Sentinel and Landsat. To do so we need to generate a series of points of interest (called POIs) that are coincident in both images. First we need to do two things:\n\nClip all the spectral data from the Sentinel image to the vector outline\n\nwe will need to resample the Sentinel data to the same pixel size so consider what you want to do here\n\nupscale or downscale ?\nWhat bands over Sentinel and Landsat actually overlap, do you need all of them\n\nwhat resolution is useful\n\n\n\n\nClip the Landsat image to the same vector outline as the Sentinel image, during the mask you can select the bands to mask and output\n\nOnce you have completed these steps you output should look something like this, however, note that my Landsat tile is a different extent to my Sentinel tile, we cover this later in the module.\n\n\n\n\n\n\n\n\n\n1.4.2.1 Select POIs\n\nTo create points of interest we want select pixels representative of certain land cover types (e.g. Bare earth, water, grass, forest, urban) so we can compare them from both Landsat and Sentinel. Of course the pixels need to be present in both datasets.\nWe can select pixels through QGIS, R or SNAP. I’ll focus on the latter here.\nMake a new vector data container for a land cover type (Vector > New Vector Data Container). Note that it will be created within the data product that is selected. Next click the square icon with a plus to add vectors. Once finished repeat the process of adding a new vector data container for the next land cover type.\n\n\n\n\n\n\n\n\n\n\nNext use a drawing tool to draw around areas of land cover, when you select the drawing tool you will be asked which landcover the polygon is for, make sure the area is in both imagery - switch between them as we have done or display them side by side. Whilst doing this it is useful to have the spectrum view open (View > Tool Windows > Optical >Spectrum Viewer:\n\n\n\n\n\n\n\n\n\n\nOnce done create another vector data container and repeat for the next land cover type.\nConsider have a separate class for highly reflective urban (e.g. industrial areas)\n\n1.4.3 Spectral signatures\nTo compare out spectral signatures we have a few options\n\nright click on the image in SNAP and then export mask pixels as a .txt, but you will need to do this for each land cover class and for each image (Sentinel and Landsat).\n\nor\n\nexport the imagery (to Geotiff) and vector files (to shapefiles) and then subset in R\n\nI will do the second option here:\n\nRight clicking on the vector containers will allow export to shapefile - do this for all the land covers you have\nTo export the “product” which is what your image will be un SNAP. Select it (just click it) then go File > Export > GeoTIFF. When exporting the Landsat data click the subset button in the save box > band subset (select only spectral bands) and then metadata subset and remove all the selection.\nOpen the GeoTIFFS and shapefiles in QGIS to check everything.This is another good opportunity to understand how contrast enhancement works - in QGIS > Symbology > expand min / max and then change the cumulative count cut the value for each band will change. For Landsat B3 is red, B2 is green and B1 is blue. This does not change the values of the pixels, just how they are displayed on a colour screen.\n\n1.4.4 Using R\nRemember that before we deal with large data sets in R, we must sort out the .gitignore file, if we intend to use Git and GitHub at any stage (hint: you will for the assessment). Go back to CASA0005 to remember how if you need to\nRaster packages in R have and are still evolving rapidly, we have probably seen and used the raster package and may have seen the terra or stars packages. All can handle raster data and do the same analysis (pretty much).\nIn 2023 some key packages will retire like maptools, rgdal and rgeos as their maintainer, Roger Bivand will retire. raster uses sp objects for vector data and also the rgdal package - the Geospatial Data Abstraction Library (GDAL) for reading, writing and converting between spatial formats. sp (that we saw in CASA0005) also uses rgdal and suggests rgeos.\nThe terra package (2020) is somewhat mixed with raster, however the retiring packages are only needed by raster, with the terra package replacing raster. Terra is much faster than raster as datasets that can’t be loaded into RAM are stored on disc and when loaded doesn’t read the values. Then when computations occur the do so in chunks. Terra is very well documented and the functions are very similar to raster - https://rspatial.org/terra/pkg/index.html\nThe stars package (2018) works with sf!!, this means many of the functions we’ve seen and are familiar with it will work - e.g. st_transform() which is much easier than doing it in raster. The real benefit of stars is its ability to handle large data (like satellite data) that can’t fit into memory. It does this through not loading the pixel values but keeping a reference to them through a proxy and only loading / running computations when specifically needed (e.g. plotting, will only plot the pixels that can be seen)! The stars section from Spatial Data Science in Pebesma and Bivnad 2022 gives a good overview with examples\nStars is faster than terra, terra is faster than raster.\n\n1.4.4.1 Data loading\n\nlibrary(sf)\nlibrary(terra)\nlibrary(raster)\n\n\nbare_earth <- st_read(\"prac_1/Data/Bare_earth_Polygon.shp\")\ngrass <- st_read(\"prac_1/Data/Grass_Polygon.shp\")\nforest <- st_read(\"prac_1/Data/Forest_Polygon.shp\")\nurban <- st_read(\"prac_1/Data/Low_urban_Polygon.shp\")\nhigh_urban <- st_read(\"prac_1/Data/High_urban_Polygon.shp\")\n\n\n#Landsat equivalent\nbands <- c(\"1\",\"2\", \"3\", \"4\", \"5\", \"6\", \"7\")\n\nsentinel <- rast(\"prac_1/Data/S2A_LSAT_msk.tif\")\n\nnames(sentinel) <- bands\n  \nlandsat<-rast(\"prac_1/Data/LSAT_msk.tif\")\n\nnames(landsat) <- bands\n\nHere i have chosen to use terra, this is because i want to extract the pixel values from within the polygons and at the moment stars will only permit aggregation - e.g. the mean of the pixels in the polygons. This is fine unless you want to explore the variation in signatures! then you will need the values.\nTo use a vector layer in terra it needs to be in a SpatVector (in terra the raster is a SpatRaster). Now there are two ways we can do this…the first is with the vect() function from terra:\n\nurban <- vect(urban)\n\nIn the second we can actually convert the SpatRaster to a raster and then just use the sf object! I know, i know, how many formats do we need here! But this will mean we have a raster brick as we have all of our bands and a raster brick won’t work here.\n\nlandsat <- as(landsat, \"Raster\")\n\nOf course, we can just do this straight from loading the data with the pipe…\n\nbare_earth <- st_read(\"prac_1/Data/Bare_earth_Polygon.shp\") %>%\n  vect()\ngrass <- st_read(\"prac_1/Data/Grass_Polygon.shp\")%>%\n    vect()\nforest <- st_read(\"prac_1/Data/Forest_Polygon.shp\")%>%\n    vect()\nurban <- st_read(\"prac_1/Data/Low_urban_Polygon.shp\")%>%\n    vect()\nhigh_urban <- st_read(\"prac_1/Data/High_urban_Polygon.shp\")%>%\n    vect()\n\nBefore we can start the extraction you might have seen that my Landsat data is not in the same CRS as the rest of the data, it’s close, but different. We can also just use the raster to get the CRS info, like\n\ncrs(landsat)\ncrs(sentinel)\n\n# reproject landsat\nlandsat <- project(landsat, sentinel)\n\nNow let’s pull out those values, and get the mean and standard deviation, starting with urban from sentinel data…\n\nlibrary(tidyverse)\nsen_urban<- terra::extract(sentinel, urban, progress = F)%>%\n  as_tibble()%>%\n  pivot_longer(cols = 2:7, \n               names_to=\"bands\", \n               values_to=\"band_values\")%>%\n  add_column(sensor=\"sentinel\")%>%\n  add_column(land=\"urban\")\n\nNow, because this process will be the same for the other landcover types we can make a function…this is the exact same code as above but i’ve replaced two arguments with sensor and lancover that i can now change for each version (e.g. bare earth in Landsat or forest in Sentinel)\n\nband_fun <- function(sensor, landcover) {\n  col_sensor <- deparse(substitute(sensor))\n  col_land <- deparse(substitute(landcover))\n\n  sen_urban<- terra::extract(sensor, landcover, progress = F)%>%\n    as_tibble()%>%\n    pivot_longer(cols = 2:7, \n               names_to=\"bands\", \n               values_to=\"band_values\")%>%\n    add_column(sensor=col_sensor)%>%\n    add_column(land=col_land)\n                 \n}\n\nLater on we will also create a density plot, so let’s write a function for all the values…it’s very similar…we will call this one when we need it later…\n\nband_fun_all_values <- function(sensor, landcover) {\n  col_sensor <- deparse(substitute(sensor))\n  col_land <- deparse(substitute(landcover))\n\n  sen_urban<- terra::extract(sensor, landcover, progress = F)%>%\n    as_tibble()%>%\n    pivot_longer(cols = 2:7, \n               names_to=\"bands\", \n               values_to=\"band_values\")\n                 \n}\n\nThen it’s simply…\n\nsen_bare <- band_fun(sentinel, bare_earth)\nsen_grass<- band_fun(sentinel, grass) \nsen_forest<- band_fun(sentinel, forest) \nsen_high_urban <- band_fun(sentinel, high_urban) \n\nlsat_urban<- band_fun(landsat, urban)\nlsat_bare<- band_fun(landsat, bare_earth)\nlsat_grass<- band_fun(landsat, grass)\nlsat_forest<- band_fun(landsat, forest)\nlsat_high_urban <- band_fun(sentinel, high_urban) \n\nThink about what this has given us?\nThe next stage is to put then into a tibble…\n\nsen_lsat <- bind_rows(sen_urban, sen_bare, sen_grass,\n                      sen_forest, sen_high_urban,\n                      lsat_urban, lsat_bare, lsat_grass,\n                      lsat_forest, lsat_high_urban)\n\nThe next stage is to get the mean (and standard deviation) values for each band per sensor and land cover type:\n\nmeans<- sen_lsat%>%\n  group_by(bands, sensor, land)%>%\n  summarise(Mean=mean(band_values), Std=sd(band_values))\n\nPlot some spectral profiles, first for Sentinel…\n\np1 <- means %>%\n  filter(sensor==\"sentinel\") %>%\n  ggplot(., aes(x = bands, y = Mean,\n                col=land))+\n  geom_point()+\n  geom_line(aes(group = land)) +\n  geom_errorbar(aes(ymin = (Mean-Std), ymax = (Mean+Std), width = 0.2))\np1\n\n\n\n\nWe can also look at a density plot….ideally each land cover has a clear and separate historgram…\n\np2 <- sen_lsat %>%\n  filter(sensor==\"sentinel\") %>%\nggplot(., aes(x=band_values, group=land, fill=land)) + \n  geom_density(alpha = 0.6)+\n#Add a mean vertical line\n  geom_vline(data = . %>% group_by(land) %>% summarise(group_mean = mean(band_values)),\n             aes(xintercept=group_mean, color = land), linetype=\"dashed\", size=1)\n\np2\n\n\n\nThanks to Sydney Goldstein for some of this code\n\n\n\n\nRemember this mean vertical line is for all values per land cover, it’s not band specific.\nAndy will talk about these outputs.\nWe can arrange our grids nicely with the cowplot package…\n\nlibrary(cowplot)\n\noutput<-plot_grid(p1, p2, labels = c('A', 'B'), label_size = 12, ncol=1)\n\noutput\n\n\n\n\nWe can also export this if we wanted with ggsave() to,ggsave has a lot of options and can output to most formats, it also gives examples of not using ggsave\n\nggsave(\"spectral_reflectance.pdf\", width = 20, height = 20, units = \"cm\")\n\nHere, i have just shown the process for Sentinel data. Your could do similar analysis of the Landsat data / some comparisons…perhaps even a statistical test (e.g. a t-test) to compare the difference between the reflectance of Landsat and Sentinel data e.g..\n\nt1<- sen_lsat %>%\n           filter(sensor==\"sentinel\" & land ==\"urban\")%>%\n           select(band_values)%>%\n           pull()\n\nt2<- sen_lsat %>%\n           filter(sensor==\"landsat\" & land ==\"urban\")%>%\n           select(band_values)%>%\n           pull()\n\nt.test(t1, t2)\n\n\n    Welch Two Sample t-test\n\ndata:  t1 and t2\nt = -355.74, df = 5970.1, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -9300.307 -9198.368\nsample estimates:\nmean of x mean of y \n 2190.602 11439.939 \n\n\nHow do we interpret this?\nIf you make plots of reflectance be careful to use the same scale, you can do this by adding a new line under ggplot like:lims(y = c(0, 80000))"
  },
  {
    "objectID": "intro.html#learning-diary",
    "href": "intro.html#learning-diary",
    "title": "\n1  Getting started with remote sensing\n",
    "section": "\n1.5 Learning diary",
    "text": "1.5 Learning diary\nConsult the assignment requirements document and complete your learning diary entry. We cover the tools to make the online learning diary next week, so in this week use a word editor, or markdown document and then add your content to your online diary next week."
  },
  {
    "objectID": "intro.html#feedback",
    "href": "intro.html#feedback",
    "title": "\n1  Getting started with remote sensing\n",
    "section": "\n1.6 Feedback",
    "text": "1.6 Feedback\nWas anything that we explained unclear this week or was something really clear…let us know using the feedback form. It’s anonymous and we’ll use the responses to clear any issues up in the future / adapt the material."
  },
  {
    "objectID": "2_portfolio.html#learning-outcomes",
    "href": "2_portfolio.html#learning-outcomes",
    "title": "2  Portfolio",
    "section": "\n2.1 Learning outcomes",
    "text": "2.1 Learning outcomes\nBy the end of this week / the practicals you should be able to:\n\nDemonstrate appropriate syntax and file structure for Xaringan and Bookdown / Quarto\nCreate presentations and books appropriate to the module assignment requirements\nShare your resources online through Git and GitHub\n\nA lot of the concepts have been covered in the lecture (more like a tutorial this week), here some key notes are provided to support and demonstrate the code for creating the slides and book. From using RMarkdown in CASA0005, this should be a little familiar."
  },
  {
    "objectID": "2_portfolio.html#resources",
    "href": "2_portfolio.html#resources",
    "title": "2  Portfolio",
    "section": "\n2.2 Resources",
    "text": "2.2 Resources\n\n\n\n\n\n\nThis week\n\n\n\n\nQuarto Quick Start\nGrolemund, Y.X., J.J. Allaire, Garrett, 2022. R Markdown: The Definitive Guide.\n\nXie, Y., 2022. [With Quarto Coming, is R Markdown Going Away? No. - Yihui Xie | 谢益辉](https://yihui.org/en/2022/04/quarto-r-markdown/ (accessed 1.19.23).\n\nxaringan Presentations, Chapter 7\n\n\nKirenz, J.K., n.d. Build Presentations in R (accessed 1.5.23)."
  },
  {
    "objectID": "2_portfolio.html#xaringan",
    "href": "2_portfolio.html#xaringan",
    "title": "2  Portfolio",
    "section": "\n2.3 Xaringan",
    "text": "2.3 Xaringan\n🗣 shar-in-gen\n\nInstall / load the packages\n\n\nremotes::install_github(\"yihui/xaringan\")\nlibrary(xaringanExtra)\n\n\n2.3.1 Template:\nFile -> New File -> R Markdown -> From Template -> Ninja Presentation\nThe loaded template and lecture had a lot of info, but to get started you need to know the following tools:\n\n2.3.2 Themes\nTo change how the RMarkdown renders you can select a different theme…just change the css argument in the preamble…currently it will be…\ncss: [default, metropolis, metropolis-fonts]\nTo see other themes:\nnames(xaringan:::list_css())\nOr, use the package xaringanthemer. Simply load the package, have a code chunk at the start of the presentation like this…\nChange the preamble to * css: [\"xaringan-themer.css\"] * highlightStyle: solarized-dark or which ever style you selected.\nSee the Xaringan CSS Theme Generator site for more details.\n\n2.3.3 Slide controls:\n\nAdd a new slide = ---\n\nAdd a click to open the next part of a slide = --\n\nAdd a flipped slide (colours reversed and message in centre) = class: inverse, center, middle (after the ---)\n\n2.3.4 Headings / lists:\n\n\n#, ## and so on for sub headings\nLists use * or 1.First item and 1.Second item, it will know to list them 1 and then 2.\n\n2.3.5 Slide sides (e.g. like power point)\n.pull-left[To have things on the left]\n.pull-right[To have things on the right]\n\n2.3.6 Images\n\nknitr::include_graphics('img/Lena-river.jpg')`\n\nto control the figure you can specify options within the chunk e.g.\n{r  echo=FALSE, out.width='60%', fig.align='center'}\n\n2.3.7 Equations\nTo use an equation it’s just placing the values / text in an opening $$ and closing $$e.g.\n$$NDVI= \\frac{NIR-Red}{NIR+Red}$$\nYou may also want to split equations if they are rather long…with \\begin{split} and \\\\ where you want the splits and then \\end{split}\n\\[\\begin{split}\nWetness = 0.1509(B2)+0.1973(B3)\\\\+0.3279(B4)+0.3406(B8)\\\\−\n0.7112(B11)−0.4572(B12)\n\\end{split}\\]\n\n2.3.8 Preview\nLoad the add In “Infinite Moon Reader” from Yihui Xie generates the slides every time you save the .Rmd. Tools > Addins.\n\n2.3.9 xaringanExtra\nxaringanExtra, developed by Garrick Aden-Buie is “a playground of enhancements and extensions”..that are easy to add..for example, to add a search function in your slides just add..see xaringanExtra"
  },
  {
    "objectID": "2_portfolio.html#quarto",
    "href": "2_portfolio.html#quarto",
    "title": "2  Portfolio",
    "section": "\n2.4 Quarto",
    "text": "2.4 Quarto\nQuarto allows you to publish Python, R, Julia or Observable in a online book or presentation. To an extent it is an updated Bookdown package that the CASA0005 resources were made with, although Bookdown still exists Quarto makes it easier to incorporate different languages and from what i can tel publish to pdf.\nDownload Quarto: https://quarto.org/\nmake sure you have updated RStudio to at least v2022.02\nNext open a blank RStudio session > New Project > New Directory > Quarto Book\nTwo files will open:\n\n\nindex.qmd = the main landing page of the book\n\n_quarto.yml = configuration file\n\nIn the _quarto.yml file change the details to reflect your workbook, you will notice that the chapters are listed - these reference the other .qmds and need to be listed here in order to be rendered in the final book - each .qmd is a chapter.\nAt the bottom there is also the editor argument that can be changed to visual or source code, depending on how you want to edit the contents of each chapter.\nClick Render (in the tool bar at the top of the .qmd) to see what happens..\nThe syntax of Quarto, Xaringan and RMarkdown are the same, except you some features are specific to each package…for example..\nTo have figures side by side in Quarto you’d do:\n\n::: {#CHUNKNAME layout-ncol=2}\n\n![Fig name](FILE){#reference)\n\n![Fig name](FILE){#reference}\n\n:::\n\nWhere as in xaringan you might use .pull-left[]"
  },
  {
    "objectID": "2_portfolio.html#git-and-github",
    "href": "2_portfolio.html#git-and-github",
    "title": "2  Portfolio",
    "section": "\n2.5 Git and GitHub",
    "text": "2.5 Git and GitHub\nWe have seen the use of Git and GitHub in several other modules, so i won’t go into it here. For a refresher read over Git, GitHub and RMarkdown\nRemember don’t upload large files use .gitignore\nOnce you have got your project set up with Git and GitHub:\n\nRender the presentation / book (if using Infinite Moon Reader this will be done for you / if using Quarto you can select render on save)\nAdd > Commit > Push to GitHub\nOn GitHub > Settings > Pages the under source\n\nFor Xaringan select /root\n\nFor Quarto select /docs\n\n\n\n\nThe page you are on will then provide a URL"
  },
  {
    "objectID": "2_portfolio.html#learning-diary",
    "href": "2_portfolio.html#learning-diary",
    "title": "2  Portfolio",
    "section": "\n2.6 Learning diary",
    "text": "2.6 Learning diary\nThis week for your learning diary you need to create a Xaringan presentation and a your Quarto learning diary.\n\n2.6.1 Xaringan\nCreate a small 9 slide presentation (not including reference slide(s)) and host it on GitHub - place the link in your Quarto portfolio\n\nSelect a sensor of your choice (any)\nCreate a short (maximum 9 slide, not including reference slide) presentation on the sensor in xaringan\n\nThe presentation will be marked in the same manner set out in mark scheme for the learning diary. Specifically:\n\nThe summary criterion will refer to the summary of the sensor you have selected.\nThe application criterion will refer to examples of studies that have used the data from the sensor and their purpose\nThe reflection criterion will refer to what you have learnt in relation to the sensor, its use and how the data might be used in future work.\n\n2.6.2 Quarto learning diary\nThe production of the Quarto learning diary is not marked, but it is required for component 2. Once you have made your Quarto document update it with your learning diary entry from week 1, and the Xaringan presentation from week 2."
  },
  {
    "objectID": "2_portfolio.html#feedback",
    "href": "2_portfolio.html#feedback",
    "title": "2  Portfolio",
    "section": "\n2.7 Feedback",
    "text": "2.7 Feedback\nWas anything that we explained unclear this week or was something really clear…let us know using the feedback form. It’s anonymous and we’ll use the responses to clear any issues up in the future / adapt the material."
  },
  {
    "objectID": "3_corrections.html#resources",
    "href": "3_corrections.html#resources",
    "title": "\n3  Corrections\n",
    "section": "\n3.1 Resources",
    "text": "3.1 Resources\n\n\n\n\n\n\nThis week\n\n\n\n\nJoyce, K., 2013. Radiative transfer and atmospheric correction video\n\nJensen, J.R., 2015. Introductory digital image processing: a remote sensing perspective. Prentice-Hall Inc.\n\nAtmospheric correction, Chapter 6, p.208\nTypes of Geometric correction, Chapter 7, p.242\nMosaicking, Chapter 7, p.267\nImage enhancements, Chapter 8, p.273\n\n\n\nSchulte to Bühne, H., Pettorelli, N., 2018. Better together: Integrating and fusing multispectral and radar satellite imagery to inform biodiversity monitoring, ecological research and conservation science. Methods in Ecology and Evolution 9, 849–865.\n\n\nData fusion video created for the paper"
  },
  {
    "objectID": "3_corrections.html#atmosphereic-correction",
    "href": "3_corrections.html#atmosphereic-correction",
    "title": "\n3  Corrections\n",
    "section": "\n3.2 Atmosphereic correction",
    "text": "3.2 Atmosphereic correction\n\n3.2.1 DOS\nAs described in the lecture, a rather simple method to correct raw satellite (or any other) imagery is called Dark Object Subtraction (DOS). This uses the logic that the darkest pixel within the image should be 0 and therefore any value that it has can be attributed to the atmosphere.\nSo in order to remove the effect of the atmosphere we can the subtract the value from the rest of the pixels within the image.\nHere, we will need to download some raw satellite imagery, that is in Digital Number (DN).\nThen we will apply the formula to calculate the reluctance of the pixels by removing atmosphere effects.The formula for the cosine of the solar zenith angle correction (COST) is, this is the same as DOS but DOS omits TAUz. The following has made use of the documentation from GIS Ag Maps.com\n\\[\\rho_{\\lambda}= \\frac{(Lsat_{rad} - Lhaze1percent_{rad})\\pi * d^2}{EO_{\\lambda} * cos\\theta S * TUAv + TAUz}\\]\nWhere…\n\n\n\\(\\rho_{\\lambda}\\) is the corrected value (DOS applied)\n\nTop line of equation…\n\n\\(Lsat_{rad}\\) = at sensor radiance (recall DN goes to radiance through the regression equation we saw!)\n\n\\(Lhaze1percent_{rad}\\) = amount of radiance that is due to the atmosphere (atmospheric haze) from path or scatter radiance. Very few surfaces are completely black so it is assume that the darkest surface has a 1% reflectance. Various methods to caclcualte this…\n\nLook up tables\nSelecting the darkest pixels (shadow, water)\n\n\n\nWhen we have the haze amount then deduct 1% from that value per band as few targets are absolutely black.\nFor COST this this:\n\\[ 0.01 reflectance = 0.01 *\\frac{Eoλ * cosθs^2} {d² * pi}\\]\nFor DOS it’s\n\\[ 0.01 reflectance = 0.01 *\\frac{Eoλ * cosθs} {d² * pi}\\]\n\n\n\\(EO_{\\lambda}\\) or \\(ESUN_{\\lambda}\\) = mean exoatmospheric irradiance\n\nirradiance = power per unit area received from the Sun\nexoatmospheric = just outside the Earth’s atmosphere\nThese values are available from the Landsat user manual such as table 5.3 in the Landsat 7 user guide\n\n\n\n\\(cosθs\\) = cosine of the solar azimuth, remember from the lecture that this is 90 - solar elevation.\n\\(d\\) = the Earth-sun distance and is in the .MTL file\n\\(pi\\) = 3.14159265\n\nOnce we have all these values then we can do the following:\n\nCompute the haze value for each band (although not beyond NIR) - this is the amount of radiance that is due to the atmosphere (atmospheric haze), see above methods.\nConvert DN to radiance\nCompute the 1% reflectance value using the equations above\nSubtract the 1% reflectance value from the radiance. Here we are saying that we have a pixel (e.g. darkest pixel), we know what 1% of the total radiance is and we are subtracting that from the darkest pixel (which still has atmospheric effects) to account for most targets not being completely black.\n\nWe can now plug the values in:\n\\[\\rho_{\\lambda}= \\frac{(Lsat_{rad} - Lhaze1percent_{rad})\\pi * d^2}{EO_{\\lambda} * cos\\theta S * TUAv + TAUz}\\]\nWhere the \\(Lhaze1percent_{rad}\\) is the haze value (e.g. darkest pixel) minus 1% of the total radiance. This 1% was computed from the equations above.\nWithin this equation:\n\\(TAUv\\) = 1.0 for Landsat and \\(TAUz\\) = cosθs for COST method. DOS is the same, but without \\(TAUz\\)\nOf course we can do this in R (or any other software) with just one function! First we need to download some raw satellite data that comes in the Digital Number (DN) format. This is the exact same process as we saw in week 1, expect this time select the Collection 1, Level-1 bundle. At the moment this process won’t work with Landsat 9. However, as this involves a large amount of data and it’s unlikely you will need to do this in the module of data read through the following code and then move to the next section\n\nlibrary(terra)\nlibrary(raster)\nlibrary(RStoolbox)\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(rgdal)\n\n## Import meta-data and bands based on MTL file\nmtlFile  <- (\"prac_3/Lsatdata8/LC08_L1TP_175083_20211005_20211013_01_T1_MTL.txt\")\n                        \nmetaData <- readMeta(mtlFile)\n\nlsatMeta  <- stackMeta(metaData)\n\n# surface reflectance with DOS\n\nl8_boa_ref <- radCor(lsatMeta, metaData, method = \"dos\")\n\n#terra::writeRaster(l8_boa_ref, datatype=\"FLT4S\", filename = \"prac_3/Lsatdata8/l8_boa_ref.tif\", format = \"GTiff\", overwrite=TRUE)\n\n# Radiance \n\nlsat_rad <- radCor(lsatMeta, metaData = metaData, method = \"rad\")\n\n#terra::writeRaster(lsat_rad, datatype=\"FLT4S\", filename = \"prac_3/Lsatdata8/lsat_rad.tif\", format = \"GTiff\", overwrite=TRUE)\n\n\nhazeDN    <- RStoolbox::estimateHaze(lsat, hazeBands = 2:4, darkProp = 0.01, plot = TRUE)\n\nlsat_sref <- radCor(lsatMeta, metaData = metaData, method = \"dos\", \n                    hazeValues = hazeDN, hazeBands = 2:4)\n\nhttps://rpubs.com/delViento/atm_corr\n\n3.2.2 Radiance (or DN) to Reflectance\nAs noted in the lecture there are a wide range of more sophisticated methods (beyond Dark Object Subtraction) to convert raw Digital Numbers or radiance to surface reflectance.\nWhilst this is a bit beyond the scope of this module, if you look again at the Landsat 7 Data Users Handbook you will see a radiance to reflectance calculation that can be used…“For relatively clear Landsat scenes”:\n\\[\\rho_{\\rho}= \\frac{\\pi* L_{\\lambda} * d ^2}{ESUN_{\\lambda} * cos\\theta_S}\\] and…we’ve seen all these values in the DOS formula, except \\(\\rho_{\\rho}\\) which is Unitless planetary reflectance\n…this method is still used in current research too, for example it’s listed in this paper of Land Surface Temperature retrieval by Sekertekin and Bonafonu, 2020. See Appendix C for Landsat 5, 7 (that use the above equation) and Landsat 8, that uses this slight variation…\n\\[\\rho_{\\rho}= \\frac{M_{p}* Q_{CAL} + A_p}{sin\\theta_{SE}}\\] Where…\n\n\\(M_p\\) is the band-specific multiplicative rescaling factor from the metadata\n\\(A_p\\) is the band-specific additive rescaling factor from the metadata\n\\(QCAL\\) is the digital number\n\\(\\theta_{SE}\\) is the sun elevation angle from the metadata file.\n\nAlthough it’s worth noting that this is Top of Atmosphere reflectance (TOA).\nRadiance is how much light the sensor sees.\nReflectance is the ratio of light leaving the target to amount striking the target. Here will still have atmopsheric effects in the way of our true apparent reflectance. Confusingly all of these can be termed reflectance and indeed sometimes radiance is referred to as reflectance.\nTOA reflectance changes the data from what the sensor sees to the ratio of light leaving compared to striking it. BUT, the atmosphere is still present. If we remove the atmopshere we have apparent reflectance (sometimes called Bottom of Atmosphere reflectance). DOS gives us a version of apparent reflectance."
  },
  {
    "objectID": "3_corrections.html#accessing-data",
    "href": "3_corrections.html#accessing-data",
    "title": "\n3  Corrections\n",
    "section": "\n3.3 Accessing data",
    "text": "3.3 Accessing data\nOk, so we can deal with a single image, but what happens when an image doesn’t cover your entire study area. We must select two images are mosaic (or merge) them together. Landsat data (and most satellite data) is collected using some form of Worldwide Reference System. This splits the data based on PATH (columns) and ROWS.\n\n\n\n\nSource: Samuel Akande\n\n\n\n\nBefore we start with merging we need to download two satellite tiles to merge together. The problem is that Landsat tiles won’t align with administration boundraies (or they might if you are lucky). For my example city of Cape Town i need at least two, possible three Landsat tiles to be merged together. In USGS Earth Explorer you can upload a shapefile or KML so you can search for tiles to cover the area. However, this is rather slow and the shapefile must only contain one polygon (cape town includes an island too). You can, however, draw a boundary to search within:\n\n\n\n\n\n\n\n\nIn my example i am going to select two tiles, i know which two to select from looking at the GADM boundary for Cape Town.\nWhen doing this:\n\nSelect two images as temporally close to each other as possible\nTry and make sure there is no (or very little cloud cover)\nIn this case select Landsat Collection 2 level 2 to get surface reflectance.\n\nNotes on Landsat Collections. The different collections denote a major difference in processing of the data. Where as the levels denote a specific product. There is no clear guide online that explains this, so be careful when reading papers!\nFor example…\n\nA primary characteristic of Collection 2 is the substantial improvement in the absolute geolocation accuracy of the global ground reference dataset used in the Landsat Level-1 processing flow\n\nCollection 2 was released in 2020 and has some updates, see this summary of differences for collection 1 vs collection 2. The collection 2 auxiliary data can also be downloaded from Earth Explorer.\nWhereas levels will provide:\n\nLevel 1 is delivered as a Digital Number\nLevel 2 includes surface reflectance and surface temperature\nLevel 3 science products are specific products generated from the data such as Burned Area, surface water extent\nMost of the level datasets are tiered, this is denoted through the file name that might end with T1. Tiers are based on data quality and level of processing. Tier 1 datasets are the best quality, tier 2 are good but might have some cloud that affects radiometric calibration covering ground control points.\nThere is also a U.S. Analysis Read Dataset (ARD) that includes a bundle of data (Top of Atmosphere (TOA) reflectance, TOA Brightness Temperature, Surface Reflectance , Surface Temperature and Quality Assessment) in a specific US grid strucutre. This removes the need to process data between the difference stages for applications in the US.\n\nTo conclude, we have collections, followed by levels, followed by tiers.\n\n\n\n\nSource: USGS\n\n\n\n\nIn this case I want to download two tiles from Collection 2, Level 2. You can view the tiles and preview the data in Earth Explorer:\n\n\n\n\n\n\n\n\nLooking at the options below each tile icon in the search results you can either download the tiles individually or add them to a basked (it’s the box icon). After clicking the box on the products you want to download > click the basket (top right) > you will then be presented with this screen where you click start order and then follow the prompts to download multiple tiles at once."
  },
  {
    "objectID": "3_corrections.html#merging-imagery",
    "href": "3_corrections.html#merging-imagery",
    "title": "\n3  Corrections\n",
    "section": "\n3.4 Merging imagery",
    "text": "3.4 Merging imagery\nOpen our two tiles, in my case i have move my tiles into two separate folders. So i will do this twice, note that we could automate this if we had a large number of tiles. I have also downloaded one Landsat 8 tile and one Landsat 9 tile.\n\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(fs)\n\n# List your raster files excluding band 8 using the patter argument\nlistlandsat_8<-dir_info(here::here(\"prac_3\", \"Landsat\", \"Lsat8\"))%>%\n  dplyr::filter(str_detect(path, \"[B123456790].TIF\")) %>%\n  dplyr::select(path)%>%\n  pull()%>%\n  as.character()%>%\n  # Load our raster layers into a stack\n  terra::rast()\n\nFor Landsat 9\n\n# List your raster files excluding band 8 using the patter argument\nlistlandsat_9<-dir_info(here::here(\"prac_3\", \"Landsat\", \"Lsat9\"))%>%\n  dplyr::filter(str_detect(path, \"[1B23456790].TIF\")) %>%\n  dplyr::select(path)%>%\n  pull()%>%\n  as.character()%>%\n  # Load our raster layers into a stack\n  terra::rast()\n\nThis might take about 2 minutes…\n\nm1 <- terra::mosaic(listlandsat_8, listlandsat_9, fun=\"mean\")"
  },
  {
    "objectID": "3_corrections.html#enhancements",
    "href": "3_corrections.html#enhancements",
    "title": "\n3  Corrections\n",
    "section": "\n3.5 Enhancements",
    "text": "3.5 Enhancements\nUsing our merged Landsat data we can now undertake some basic enhancements to try and emphasize / exaggerate certain features or spectral traits.\n\n3.5.1 Ratio\nRatioing is the difference between two spectral bands that have a certain spectral response meaning it is easier to identify a certain landscape feature…for example…\n\nThe Normalised Difference Vegetation Index is based on the fact that healthy and green vegetation reflects more in the NIR but absorbs in the Red wavelength\n\n\n\n\n\nSource: PhysicsOpenLab\n\n\n\n\nHere, we can visually see the spectral trait:\n\n\n\n\nSource: PhysicsOpenLab\n\n\n\n\nWe can leverage the fact that healthy vegetation has this spectral trait and use the NDVI index should we wish to highlight areas with healthy vegetation.\n\\[NDVI= \\frac{NIR-Red}{NIR+Red}\\] In R this would be:\n\nm1_NDVI <- (m1$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B5 - m1$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B4 ) / (m1$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B5 + m1$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B4)\n\nm1_NDVI %>%\n  plot(.)\n\nWe can reclassify this to pull out certain areas, for example, only where NDVI is equal to or greater than 0.2\n\nveg <- m1_NDVI %>%\n  terra::classify(., cbind(-Inf, 0.2, NA))\n\nveg %>%\n  plot(.)\n\n\n\n\nThere are many other ratios, all of which are detailed on the Index Database and most follow the same formula. For example, the Normalized Difference Moisture Index (NDMI):\nFor Landsat sensors 4-7:\n\\[NDMI= \\frac{Band 4-Band 5}{Band 4 + Band 5}\\] For Landsat 8, bands are increased by 1.\n\n3.5.2 Filtering\nFiltering refers to any kind of moving window operation to our data which can be saved as a separate raster file. As we saw in the lecture this can include low or high pass filters. Here \\(w\\) means window. We can also set a weight matrix (as seen in the lecture):\nLaplacian filter: matrix(c(0,1,0,1,-4,1,0,1,0), nrow=3)\n\n\n\n\nSource: Introduction to Geographic Information Systems in Forest Resources\n\n\n\n\n\n# for a 3 by 3 filter on 1 band\nm1_filter <- terra::focal(m1$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B4, w=matrix(nrow=3,ncol=3))\n\n\n3.5.3 Texture\nThe basics of texture were covered in the lecture. To apply texture analysis to data in R, we can use the glcm package which has a selection of eight texture measures, and we can apply these per band…for example…Note, to use this you must have RTools installed as it makes use of the C++ language.\nBelow we can specify:\n\nthe size of the moving window\nthe shift for co-occurrency (or second order) as seen in the lecture. If multiple shifts are supplied, glcm will calculate each texture statistic using all the specified shifts and return the mean value of the texture for each pixel\nthe measures we want, for full equations see Texture Metrics Background\n\n\nNote, that we commonly don’t use the pancrhomatic band in landcover classification, however as it is 15m it can produce some useful outputs.\nCurrently the glcm package only accepts raster layers from the raster package so we first need to convert this to a raster layer…this will take 7-10 minutes…increasing the window size and selecting less statistics should speed this up.\n\nlibrary(glcm)\nlibrary(raster)\n\nband4_raster<-raster::raster(m1$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B4)\n\nglcm <- glcm(band4_raster,\n                   window = c(7, 7),\n                   #shift=list(c(0,1), c(1,1), c(1,0), c(1,-1)), \n                   statistics = c(\"homogeneity\"))\n\nglcm$glcm_homogeneity %>%\n  plot(.)\n\nDepending on your study area the texture measures might not show much, but in this example from Lu et al. 2012 what does it highlight or make more prominent.\n\n\n\n\nSource: Land use/cover classification in the Brazilian Amazon using satellite images\n\n\n\n\n\n3.5.4 Data fusion\nIn the simplest form data fusion is appending new raster data to the existing data or making a new raster dataset with different bands…here we can do this with the texture measure we have created (and the original spectral data if you wish). We are getting to the stage now where remote sensing is a merge of science and art. Specifically the science is how we correct and apply methods, the art is about how we select the right data / transform it / alter it to produce an output. There is never a completely right answer as we will see in future practicals.\nTo create decision level (or layer) fusion we append our new datasets to our existing data…\n\n# for the next step of PCA we need to keep this in a raster (and not terra) format...\nm1_raster <- stack(m1)\n\nFuse <- stack(m1_raster, glcm.red)\n\nRecall from the lecture there is also object fusion and image fusion.\n\n3.5.5 PCA\nPrincipal Component Analysis is designed to reduce the dimensionality of our data. In this case we might want to scale our data, meaning that we can compare data that isn’t measured in the same way (as we have spectral and texture data). To do so we can use the function scale, that by default standarize the data by subtracting the mean and dividing by the standard deviation. If we just wanted to subtract the mean we could set scale=FALSE as an argument.\nWhen we use rasterPCA() we can set the number of samples nsamples to be used for PCA which are then applied to the rest of the data to try and make this more efficient…\nAlso note that spca=TRUE which corresponds to centered and scaled input data\n\nlibrary(RStoolbox)\n\nFuse_3_bands <- stack(Fuse$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B4, Fuse$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B5, Fuse$glcm_homogeneity)\n\nscale_fuse<-scale(Fuse_3_bands)\n\npca <- rasterPCA(Fuse, \n                 nSamples =100,\n                 spca = TRUE)\n\nOnce the rasterPCA() has completed run the following code to get the proportion of variance explained by each PCA component and the cumulative proportion of the variance explained (from all the PCA layers compared to the original input data). Remember that PCA is trying to:\n\nTransform multi-spectral data into uncorrelated and smaller dataset\nKeep most of the original information\nThe first component will (should) capture most of the variance within the dataset\n\n\nsummary(pca$model)\n\nIn my case component 1 (of 10) explains 77.62% of the variance from the entire dataset…how might this be useful in future analysis?\nFinally, to plot one of the PCA bands….\n\nplot(pca$map$PC1)"
  },
  {
    "objectID": "3_corrections.html#learning-diary",
    "href": "3_corrections.html#learning-diary",
    "title": "\n3  Corrections\n",
    "section": "\n3.6 Learning diary",
    "text": "3.6 Learning diary\nConsult the assignment requirements document and complete your learning diary entry in your Quarto learning diary.\n\n3.6.1 Useful blogs\n\nTexture and PCA in R"
  },
  {
    "objectID": "3_corrections.html#feedback",
    "href": "3_corrections.html#feedback",
    "title": "\n3  Corrections\n",
    "section": "\n3.7 Feedback",
    "text": "3.7 Feedback\nWas anything that we explained unclear this week or was something really clear…let us know using the feedback form. It’s anonymous and we’ll use the responses to clear any issues up in the future / adapt the material."
  },
  {
    "objectID": "4_policy.html#resources",
    "href": "4_policy.html#resources",
    "title": "4  Policy",
    "section": "4.1 Resources",
    "text": "4.1 Resources\n\n\n\n\n\n\nThis week\n\n\n\nThe focus this week is on identifying an earth observation dataset that could be used to assist a policy.\nThe following papers will be useful:\n\nGerasopoulos, E., Bailey, J., Athanasopoulou, E., Speyer, O., Kocman, D., Raudner, A., Tsouni, A., Kontoes, H., Johansson, C., Georgiadis, C., Matthias, V., Kussul, N., Aquilino, M., Paasonen, P., 2022. Earth observation: An integral part of a smart and sustainable city. Environmental Science & Policy 132, 296–307.\nKadhim, N., Mourshed, M., Bray, M., 2016. Advances in remote sensing applications for urban sustainability. Euro-Mediterr J Environ Integr 1, 7.\nWellmann, T., Lausch, A., Andersson, E., Knapp, S., Cortinovis, C., Jache, J., Scheuer, S., Kremer, P., Mascarenhas, A., Kraemer, R., Haase, A., Schug, F., Haase, D., 2020. Remote sensing in urban planning: Contributions towards ecologically sound policies? Landscape and Urban Planning 204, 103921.\nSearching for urban or city in the International Journal of Applied Earth Observation and Geoinformation\nJensen, J.R., 2015. Introductory digital image processing: a remote sensing perspective. Prentice-Hall Inc.\n\nUrban-Suburban Phenological Cycles, Chapter 12, p.509\n\n\nA very nice example that combines EO data and analysis we saw in CASA0005! The spatial error model!. We will discuss the specific data in future weeks.\n\nLi, D., Newman, G.D., Wilson, B., Zhang, Y., Brown, R.D., 2022. Modeling the relationships between historical redlining, urban heat, and heat-related emergency department visits: An examination of 11 Texas cities. Environment and Planning B: Urban Analytics and City Science 49, 933–952.\nMartinez, A. de la I., Labib, S.M., 2023. Demystifying normalized difference vegetation index (NDVI) for greenness exposure assessments and policy interventions in urban greening. Environmental Research 220, 115155."
  },
  {
    "objectID": "4_policy.html#before-the-practical-session",
    "href": "4_policy.html#before-the-practical-session",
    "title": "4  Policy",
    "section": "4.2 Before the practical session",
    "text": "4.2 Before the practical session\nFor the practical session this week come prepared to talk about the following:\n\nExplore the policies below\nSearch for one metropolitan policy challenge (any city in the World) that could be solved by incorporating remotely sensed data\nIdentify and evaluate a remotely sensed data set that could be used to assist with contributing to the policy goal\nDemonstrate how this links to global agendas / goals\nExplain how it advances current local, national or global approaches.\n\nCities will have a diverse range of documentation available…"
  },
  {
    "objectID": "4_policy.html#learning-diary",
    "href": "4_policy.html#learning-diary",
    "title": "4  Policy",
    "section": "4.3 Learning diary",
    "text": "4.3 Learning diary\nFollowing the practical and subsequent discussion write up your case study city\nThe case study will be marked in the same manner set out in mark scheme for the learning diary. Specifically:\n\nThe summary criterion will refer to the summary of the policy and city you have selected.\nThe application criterion will refer to how the remotely sensed data you sourced could be used to assist with contributing to the policy goal. How could the data be applied to solve the policy challenge.\nThe reflection criterion will refer to what you have learnt in relation to the policy, city and the application of the data.\n\nShould you struggle to find current approaches within your city explore other cities discussed within the practical."
  },
  {
    "objectID": "4_policy.html#policies",
    "href": "4_policy.html#policies",
    "title": "4  Policy",
    "section": "4.4 Policies",
    "text": "4.4 Policies\nA few examples to get you started …you are not limited to this list. We explore some of these in more detail within future lectures.\n\n4.4.1 Metropolitan\n\nCape Town Municipal Spatial Development Framework\nAhmedabad 2016 Heat Action Plan\nSouth Asia’s First Heat-Health Action Plan in Ahmedabad (Gujarat, India)\nOneNYC 2050\nLondon Plan\nPerth and Peel @ 3.5 million\n\n\n\n4.4.2 National\n\nSingapore Master Plan\n\n\n\n4.4.3 International\n\nNEW!! World Cities Report 2022\nC40 Cities\nUnited Nations New Urban Agenda\nARUP City Resilience Framework\nUnited Nations International Strategy for Disaster Reduction Sendai Framework\nUniversal Sustainable Development Goals\n\nThe SDG targets\n\nBeat the Heat Handbook"
  },
  {
    "objectID": "4_policy.html#example",
    "href": "4_policy.html#example",
    "title": "4  Policy",
    "section": "4.5 Example",
    "text": "4.5 Example\nFor a written example read up to the study area section in my paper on temperature mitigation (first 2 pages). Take note of table 1."
  },
  {
    "objectID": "4_policy.html#feedback",
    "href": "4_policy.html#feedback",
    "title": "4  Policy",
    "section": "4.6 Feedback",
    "text": "4.6 Feedback\nWas anything that we explained unclear this week or was something really clear…let us know using the feedback form. It’s anonymous and we’ll use the responses to clear any issues up in the future / adapt the material."
  },
  {
    "objectID": "5_GEE_I.html#resources",
    "href": "5_GEE_I.html#resources",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.1 Resources",
    "text": "5.1 Resources\n\n\n\n\n\n\nThis week\n\n\n\nThe original GEE paper\n\nGorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., Moore, R., 2017. Google Earth Engine: Planetary-scale geospatial analysis for everyone. Remote Sensing of Environment, Big Remotely Sensed Data: tools, applications and experiences 202, 18–27.\n\n\nThis paper is very similar, but focus on SECTION VII.GEE Applications\n\nAmani, M., Ghorbanian, A., Ahmadi, S.A., Kakooei, M., Moghimi, A., Mirmazloumi, S.M., Moghaddam, S.H.A., Mahdavi, S., Ghahremanloo, M., Parsian, S., Wu, Q., Brisco, B., 2020. Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13, 5326–5350.\nGEE applications topical collection, search for urban related studies\n\nCloud-Based Remote Sensing with Google Earth Engine (accessed 1.5.23).\n\nSection F1 Programming and Remote Sensing Basics\nSection F2.0 Image Manipulation: Bands, Arithmetic, Thresholds, and Masks\nChapter F3.2 Neighborhood-Based Image Transformation\nChapter F3.1: Advanced Pixel-Based Image Transformation (PCA and tasselled cap)\nChapter A1.2 Urban Envrionments\nChapter A1.3: Built Environments\n\n\n\nKochenour, C., 2020. Introduction — Remote Sensing with Google Earth Engine.\n\nSimilar to the GEE book above, with some nice workflows.\n\n\n\n\n\nFor this week we are going to move away form R and focus on Google Earth Engine, which is still quite new for me too.\nGoogle Earth Engine (GEE) is a platform that lets us analyse data that is stored remotely, each time we write some code it is sent to a server to be evaluated and the results are returned to us. As we don’t compute locally it speeds all of our processes up!\nGEE uses Javascript…and according to the GEE team this is all of the Javascript you need to know:\n\n// All the javascript you need to know (almost)\n\nvar number = 1\n\nvar string = 'Hello, World!'\n\nvar list = [1.23, 8, -3]\nprint(list[2])\n\nvar dictionary = {\n  a: 'Hello',\n  b: 10,\n  c: 0.1343,\n  d: list\n}\n\nprint(dictionary.b)\nprint(number, string, list, dictionary)\n\nWhen you open GEE code editor you will see the following screen composed of these parts:\n\n\n\n\nGEE screen: GEE community Beginner’s Cookbook, TC25"
  },
  {
    "objectID": "5_GEE_I.html#examples",
    "href": "5_GEE_I.html#examples",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.2 Examples",
    "text": "5.2 Examples\nBefore we begin explore some of the examples in the script editor, start with the user interface ones.."
  },
  {
    "objectID": "5_GEE_I.html#points",
    "href": "5_GEE_I.html#points",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.3 Points",
    "text": "5.3 Points\nTo start with GEE i want to pull up the same image from a specific date for a city, i will use\nWe can make a new point in the code editor with:\n\n// Construct a point from coordinates.\nvar point = ee.Geometry.Point([77.216721, 28.644800]);\n\nIn this case my city is Dheli, when i enter this code you will get a message asking if you want to convert it to an imported record, click convert and it will appear in the imports…\nAlternatively we can click the point icon on the map and add a new point…> new layer > give an suitable name\n\n\n\n\n\n\n\n\nTo center the map on this point…where the second argument is the zoom level\n\nMap.centerObject(Dheli, 10)"
  },
  {
    "objectID": "5_GEE_I.html#landsat-data",
    "href": "5_GEE_I.html#landsat-data",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.4 Landsat data",
    "text": "5.4 Landsat data\nNow we need to load some surface reflectance data > browse data catalogue > click through to a Landsat surface reflectance dataset, Collection 2 and tier 1. You will see the code to import the data to your script and if you click the open window icon you can see an example…\n\n\n\n\n\n\n\n\nHere through let’s just load some data…\n\nvar dataset = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')\n    .filterDate('2022-01-01', '2022-02-01');\n\nThis doesn’t do anything until we actually add it to our map….the final Landsat 9 here is what the layer will be called on the map.\n\nMap.addLayer(dataset, {bands: [\"SR_B4\", \"SR_B3\", \"SR_B2\"]}, \"Landsat 9\")\n\nWhat has this actually done? Try zooming out and inspecting the image. On the right hand side click inspector and click around the image…\nAt the moment we aren’t sure exactly what images we are using or how much cloud cover is in them…to sort this…\nNote, that in GEE // comments out a line of code…\n\n// Load Landsat 9 data, filter by date, month, and bounds.\nvar dataset = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')\n  .filterDate('2020-01-01', '2022-10-10')\n // .filter(ee.Filter.calendarRange(1, 2, 'month'))  // Jan and Feb images\n  .filterBounds(Dheli)  // Intersecting ROI\n  .filter(ee.Filter.lt(\"CLOUD_COVER\", 0.1));\n  \n  print(dataset, \"datasetprint\")"
  },
  {
    "objectID": "5_GEE_I.html#problems",
    "href": "5_GEE_I.html#problems",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.5 Problems",
    "text": "5.5 Problems\nIn my case this provides three images…and if we wanted to export the metadata of these images we could using…\n\nExport.table.toDrive(dataset, 'exportAsCSV', 'GEE', 'CSVexport', 'CSV');\n\nNow I have two problems:\n\nThe collection has three images within it and when i display it Map.addLayer(filtered, {bands: [\"SR_B4\", \"SR_B3\", \"SR_B2\"]}) we get the top most image.\nThe image only covers my single point and not the whole of Dehli / the administration area.\n\n\n5.5.1 Polygons\nLet’s deal with problem two first…\n\nI could change my point to a shape that i can draw. Next to the point icon (in the map/layer window) there is a polygon icon.\nI could also add another point to my current single point where i want the next tile to be\nI could filter based on tiles from the image collection\nI can upload a file, such as one from GADM. To do so, download the appropraite GADM boundary for your city, query it in QGIS to get a city outline. For me this was the shapefile gadm41_IND_2 and I need to filter (in GEE or other software) the GID_1 column for the row IND.25_1.\n\nOn GEE there is an assets button where you can upload data, upload your shapefile. Next we load it and filter what you need…\n\nvar india = ee.FeatureCollection('users/andrewmaclachlan/india')\n    .filter('GID_1 == \"IND.25_1\"');\n\nTo now filter based on this we include it as our filter bounds. At the time of writing there weren’t enough cloud free Landsat 9 scenes over Delhi to create a complete image, so I have changed to Landsat 8…\n\nvar oneimage_study_area_cloud = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n  .filterDate('2021-06-01', '2022-10-10')\n  .filterBounds(india)  // Intersecting ROI\n  .filter(ee.Filter.lt(\"CLOUD_COVER\", 0.1));\n\nWithin the Landsat 8 filename we can see which path and rows our tiles are on…LANDSAT/LC08/C02/T1_L2/LC08_146040_20211127\nUsing these dates I have 1 image from path 146, row 40, 1 image from path 146, row 41 and 4 images from path 147, row 40. See the Landsat Acquisition Tool to check your path and row\n\n5.5.2 Single images\nNow problem one…\n\nI could just select a single image from the collection by filtering using the specific date, which you can get from the console window when printing the images within the collection. Landsat (and most other EO datasets) have the date within the file path name, this is….LC09_147040_20220403\n\n\n\nvar oneimage = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')\n  .filterDate('2022-04-03', '2022-04-04')\n  .filterBounds(india);  // Intersecting ROI\n\n\nI can load the specific image(s) i want…\n\n\nvar image_146_40 = ee.Image('LANDSAT/LC08/C02/T1_L2/LC08_146040_20211127')\n\n\nI can reduce the collection to a single image through taking the mean, median, max or min using the imageCollection.reduce() function…\n\n\nvar median = oneimage_study_area_cloud.reduce(ee.Reducer.median());\n// print the image info\nprint(median, \"median\")\n\nIs the median the best image to make though ? watch from 27:28 to 31:46"
  },
  {
    "objectID": "5_GEE_I.html#better-images",
    "href": "5_GEE_I.html#better-images",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.6 Better images",
    "text": "5.6 Better images\nUntil now we haven’t really dealt with the scaling factors from the Landsat surface reflectance product…\n\nLandsat Collection 2 surface reflectance has a scale factor of 0.0000275 and an additional offset of -0.2 per pixel.\n\nWe do this through making a function and then calling our collection to the function\n\n// Applies scaling factors in a function\nfunction applyScaleFactors(image) {\n  var opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n  var thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0);\n  return image.addBands(opticalBands, null, true)\n              .addBands(thermalBands, null, true);\n}\n\n// call our collection to the function and assign it to a new variable \noneimage_study_area_cloud_scale = oneimage_study_area_cloud.map(applyScaleFactors);\n\n// apply the median reducer\nvar oneimage_study_area_cloud_scale_median = oneimage_study_area_cloud_scale.reduce(ee.Reducer.median());\n\nWe can then map this…\n\n// set up some of the visualisation paramters \nvar vis_params = {\n  bands: ['SR_B4_median', 'SR_B3_median', 'SR_B2_median'],\n  min: 0.0,\n  max: 0.3,\n};\n\n// add a layer to the map\nMap.addLayer(oneimage_study_area_cloud_scale_median, visualization, 'True Color (432)');\n\nYou should have produced something like this…\n\n\n\n\n\n\n\n\nNote here that we give this layer a name 'True Color (432)' this means on the map under the layer button it will have that same name. Here, you can also set the transparency of each layer and even change the bands that display (although this doesn’t change the code)"
  },
  {
    "objectID": "5_GEE_I.html#mosaic-images",
    "href": "5_GEE_I.html#mosaic-images",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.7 Mosaic images",
    "text": "5.7 Mosaic images\nWhen we look at these images there might be very apparent differences between the tiles - this is probably due to the date of collection and the atmospheric correction applied (remember that it’s a model of the atmosphere).\n\nvar mosaic = oneimage_study_area_cloud_scale.mosaic();\n\nvar vis_params2 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0.0,\n  max: 0.3,\n};\n\nMap.addLayer(mosaic, vis_params2, 'spatial mosaic');\n\nIn this example i’ve mosaiced the image collection (before taking the median values). This has taken images according to their order in the collection (last on top).You’ll notice there isn’t much difference…but you will see the effect of the last on top rule with clear demarcations across where images overlap…making the problem we wanted to resolve worse!\n\n\n\n\n\n\n\n\nThe best easy solution we have here is to take a mean of all overlapping pixels…\n\nvar meanImage = oneimage_study_area_cloud_scale.mean();\n\nMap.addLayer(meanImage, vis_params2, 'mean');\n\nHowever, as noted in the lectures we could also match the histograms of the images, this is beyond the scope of the practical but more details are:\n\nIn Nocel Gorelick’s Histogram Matching Earth Engine by Example article and code"
  },
  {
    "objectID": "5_GEE_I.html#clip-images",
    "href": "5_GEE_I.html#clip-images",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.8 Clip images",
    "text": "5.8 Clip images\nCurrently our analysis has spanned the tiles that cover the study area - in my case this is three tiles. So we should clip them to our current study area. After the clip i’m also just selecting the bands i want.\nWe could also visualise this with, where have used the interactive visulisation box to select my min and max values…\n\nvar clip = meanImage.clip(india)\n  .select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7']);\n\nvar vis_params3 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0,\n  max: 0.3,\n};\n\n// map the layer\nMap.addLayer(clip, vis_params3, 'clip');"
  },
  {
    "objectID": "5_GEE_I.html#texture-measures",
    "href": "5_GEE_I.html#texture-measures",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.9 Texture measures",
    "text": "5.9 Texture measures\nAs we saw earlier in the module we can compute texture…however, the glcmTexture() function requires an interger value and as our values are in surface reflectance this will mean they will be converted to 1 and 0s. To explore this, add the clipped surface reflectance layer to the map (using the code above)…Then use the inspector tab and the list view to query individual pixels….\n\n\n\n\n\n\n\n\nTo resolve this we need to multiple our surface reflectance so we can compute texture. If we didn’t do this our texture would just be 1s and 0s. ….\nNote, that we are selecting a lot of data here…if GEE shows as unresponsive click wait and it should process, otherwise reduce the number of bands.\n\nvar glcm = clip.select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7'])\n  .multiply(1000)\n  .toUint16()\n  .glcmTexture({size: 1})\n  .select('SR_.._contrast|SR_.._diss')\n  .addBands(clip);\n  \n// add to the map, but change the range values  \nMap.addLayer(glcm, {min:14, max: 650}, 'glcm');\n\nHere note:\n\nthe function .toUint16() as.glcmTexture won’t work with 32 bit data\nsize 1, is the size of the neighborhood to include in each GLCM.The value refers to the number of pixels to move from the central pixel, so a value of 1 creates a surrounding 3 by 3 grid.\nThe selection function uses a regular expression to pull out just contrast and dissmilarity\nAs the texture function just outputs the texture measures, i’ve added back in the original bands to the image collection."
  },
  {
    "objectID": "5_GEE_I.html#pca",
    "href": "5_GEE_I.html#pca",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.10 PCA",
    "text": "5.10 PCA\nPCA in GEE is somewhat more complex that it was in R, by just using the one function. Most of this code is take from the GEE Eigen Analysis guide ….to start with set the:\nScale: 30 meters Band names: from the texture and original bands collection Region: the geometry points (x,y coordinates) of the study area\nWe then center the data to the mean (in the code below) and divide by the standard deviation that happens in the next section\n\n// scale and band names\nvar scale = 30;\nvar bandNames = glcm.bandNames();\n\nvar region = india.geometry();\nMap.centerObject(region, 10);\nMap.addLayer(ee.Image().paint(region, 0, 2), {}, 'Region');\n\nprint(region, \"india_geometry\")\n\n\n// mean center the data and SD strech the princapal components \n// and an SD stretch of the principal components.\nvar meanDict = clip.reduceRegion({\n    reducer: ee.Reducer.mean(),\n    geometry: region,\n    scale: scale,\n    maxPixels: 1e9\n});\nvar means = ee.Image.constant(meanDict.values(bandNames));\nvar centered = clip.subtract(means);\n\nWe can use this function to sort out our bandnames\n\n// This helper function returns a list of new band names.\nvar getNewBandNames = function(prefix) {\n  var seq = ee.List.sequence(1, bandNames.length());\n  return seq.map(function(b) {\n    return ee.String(prefix).cat(ee.Number(b).int());\n  });\n};\n\nThen we can pass our centered data to the PCA function, here i have added some code from Guo Qiqu that will display the % variance explained by each component. Note i have just used this function to get what i need here, and that is the PCA output.\n\n// This function accepts mean centered imagery, a scale and\n// a region in which to perform the analysis.  It returns the\n// Principal Components (PC) in the region as a new image.\nvar getPrincipalComponents = function(centered, scale, region) {\n  // Collapse the bands of the image into a 1D array per pixel.\n  var arrays = centered.toArray();\n\n  // Compute the covariance of the bands within the region.\n  var covar = arrays.reduceRegion({\n    reducer: ee.Reducer.centeredCovariance(),\n    geometry: region,\n    scale: scale,\n    maxPixels: 1e9\n  });\n\n  // Get the 'array' covariance result and cast to an array.\n  // This represents the band-to-band covariance within the region.\n  var covarArray = ee.Array(covar.get('array'));\n\n  // Perform an eigen analysis and slice apart the values and vectors.\n  var eigens = covarArray.eigen();\n\n  // This is a P-length vector of Eigenvalues.\n  var eigenValues = eigens.slice(1, 0, 1);\n  // This is a PxP matrix with eigenvectors in rows.\n  \n  var eigenValuesList = eigenValues.toList().flatten()\n  var total = eigenValuesList.reduce(ee.Reducer.sum())\n  var percentageVariance = eigenValuesList.map(function(item) {\n  return (ee.Number(item).divide(total)).multiply(100).format('%.2f')\n    })\n  \n  print(\"percentageVariance\", percentageVariance)  \n\n  var eigenVectors = eigens.slice(1, 1);\n\n  // Convert the array image to 2D arrays for matrix computations.\n  var arrayImage = arrays.toArray(1);\n\n  // Left multiply the image array by the matrix of eigenvectors.\n  var principalComponents = ee.Image(eigenVectors).matrixMultiply(arrayImage);\n\n  // Turn the square roots of the Eigenvalues into a P-band image.\n  var sdImage = ee.Image(eigenValues.sqrt())\n    .arrayProject([0]).arrayFlatten([getNewBandNames('sd')]);\n\n  // Turn the PCs into a P-band image, normalized by SD.\n  return principalComponents\n    // Throw out an an unneeded dimension, [[]] -> [].\n    .arrayProject([0])\n    // Make the one band array image a multi-band image, [] -> image.\n    .arrayFlatten([getNewBandNames('pc')])\n    // Normalize the PCs by their SDs.\n    .divide(sdImage);\n};\n\n// Get the PCs at the specified scale and in the specified region\nvar pcImage = getPrincipalComponents(centered, scale, region);\n\n// Plot each PC as a new layer\nfor (var i = 0; i < bandNames.length().getInfo(); i++) {\n  var band = pcImage.bandNames().get(i).getInfo();\n  Map.addLayer(pcImage.select([band]), {min: -2, max: 2}, band);\n}\n\nWe can then plot each component with a for loop:\n\n// Plot each PC as a new layer\nfor (var i = 0; i < bandNames.length().getInfo(); i++) {\n  var band = pcImage.bandNames().get(i).getInfo();\n  Map.addLayer(pcImage.select([band]), {min: -2, max: 2}, band);\n}\n\nIn my case i can see that the first component explains 81% of the variance within the collection and the second component 12%….we can just add these bands instead of the entire image.\n\nMap.addLayer(pcImage, {bands: ['pc2', 'pc1'], min: -2, max: 2}, 'PCA bands 1 and 2');"
  },
  {
    "objectID": "5_GEE_I.html#exporting",
    "href": "5_GEE_I.html#exporting",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.11 Exporting",
    "text": "5.11 Exporting\nThere are a number of ways you can export data from GEE, to your Google Drive, Cloud storage or as an Asset for future use.\nTo export it to your drive…\n\n// select the bands you want\nvar PCA_out = pcImage.select(['pc1', 'pc2'])\n\nvar projection = PCA_out.select('pc1').projection().getInfo();\nprint(projection, \"output_projection\")\n\nvar bounds = india.geometry();\n\n// Export the image, specifying the CRS, transform, and region.\nExport.image.toDrive({\n  image: PCA_out,\n  description: 'PCA_india',\n  scale:30,\n  crs: projection.crs,\n  maxPixels: 100E10,\n  region: bounds\n\n});\n\nNow when you run the script a task will appear in the task menu, to export your file click Run and then it will save into your Google Drive so you can download it locally.\nNote that setting the region here will mean that if you haven’t performed a clip it will just export the region you have set."
  },
  {
    "objectID": "5_GEE_I.html#band-math",
    "href": "5_GEE_I.html#band-math",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.12 Band math",
    "text": "5.12 Band math\nIt is also possible to quickly calculate indices in GEE…\n\n//NDVI\nvar NDVI_1 = clip.select('SR_B5').subtract(clip.select('SR_B4'))\n  .divide(clip.select('SR_5').add(clip.select('SR_B4')));\n\nMap.addLayer(NDVI_1, { min: -1, max: 1, palette: ['blue', 'white', 'green']}, 'NDVI');\n\nNDVI/ the formula to compute it even has it’s own function…\n\nvar NDVI_2 = clip.normalizedDifference([SR_B5, SR_B4]);"
  },
  {
    "objectID": "5_GEE_I.html#gee-apps",
    "href": "5_GEE_I.html#gee-apps",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.13 GEE apps",
    "text": "5.13 GEE apps\nA real feature of GEE is the ability to turn code into responsive applications. Whilst this is beyond the scope of this module, exploring some of these apps might give you some ideas for your project proposals:\n\nList of awesome GEE apps\nAir quality example\nNDVI slider example"
  },
  {
    "objectID": "5_GEE_I.html#gee-data",
    "href": "5_GEE_I.html#gee-data",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.14 GEE Data",
    "text": "5.14 GEE Data\n\n5.14.1 GEE catlog\nGEE has a massive catalog of data that we can search and just load directly into our script…over reading week explore the data catalog and see what might be useful for your project proposals..datasets of note:\n\nOpen Buildings V1 Polygons\nSentinel-5P pollution products from the Tropomi instrument\nWorld Settlement Footprint 2015\nNight time light data\nGlobal flood database\nTemperature (daily) from MODIS or on image basis from Landsat\nMODIS thermal anomalies and fire\nFamine early warning system\nUS census data and roads\nSome examples of high resolution data e.g. Planet SkySat Public Ortho Imagery\n\n5.14.2 Community catlog\nIn addition to the GEE catalog there is also now a community GEE catalog as it can be difficult to use GEE if the data you want isn’t available…datasets of note:\n\nGlobal urban extents from 1870 to 2100\nGlobal utilities\nGlobal fire / burned area\nAir pollutants (Long-term Gap-free High-resolution Air Pollutants (LGHAP))"
  },
  {
    "objectID": "5_GEE_I.html#learning-diary",
    "href": "5_GEE_I.html#learning-diary",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.15 Learning diary",
    "text": "5.15 Learning diary\nConsult the assignment requirements document and complete your learning diary entry in your Quarto learning diary."
  },
  {
    "objectID": "5_GEE_I.html#feedback",
    "href": "5_GEE_I.html#feedback",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.16 Feedback",
    "text": "5.16 Feedback\nWas anything that we explained unclear this week or was something really clear…let us know using the feedback form. It’s anonymous and we’ll use the responses to clear any issues up in the future / adapt the material."
  },
  {
    "objectID": "6_classification_I.html#resources",
    "href": "6_classification_I.html#resources",
    "title": "\n6  Classification I\n",
    "section": "\n6.1 Resources",
    "text": "6.1 Resources\n\n\n\n\n\n\nThis week\n\n\n\n\n\nJensen, J.R., 2015. Introductory digital image processing: a remote sensing perspective. Prentice-Hall Inc.\n\nThematic Information Extraction: Pattern Recognition, Chapter 9\n\nsupervised classification\nunsupervised classification\n\n\nInformation Extraction Using AI, Chapter 10\n\nexpert systems\nsupport vector machines\nrandom forest classifier\n\n\nRemote Sensing-Derived Thematic Map Accuracy Assessment, Chapter 13\n\nthe error matrix\n\n\n\n\n\nCloud-Based Remote Sensing with Google Earth Engine (accessed 1.5.23).\n\nChapter F2.1: Interpreting an Image: Classification\n\n\nPal, M., Mather, P.M., 2005. Support vector machines for classification in remote sensing. International Journal of Remote Sensing 26, 1007–1011.\nGISGeography, 2014. Image Classification Techniques in Remote Sensing (accessed 1.5.23).\nBarsi, Á., Kugler, Z., László, I., Szabó, G., Abdulmutalib, H., 2018. Accuracy dimensions in remote sensing. International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences 42."
  },
  {
    "objectID": "6_classification_I.html#data",
    "href": "6_classification_I.html#data",
    "title": "\n6  Classification I\n",
    "section": "\n6.2 Data",
    "text": "6.2 Data\nIn the last practical we uploaded our own admin boundary dataset from GADM. GEE has a massive catalog of data that we can search and just load directly into our script…\nSearch the GEE data catalogue for admin. You should see the FAO GAUL global admin layers and level 2 is equivalent to our unit of study from last week (although this varies based on the country)."
  },
  {
    "objectID": "6_classification_I.html#vector-data",
    "href": "6_classification_I.html#vector-data",
    "title": "\n6  Classification I\n",
    "section": "\n6.3 Vector data",
    "text": "6.3 Vector data\nTo load in the level 2 Global Administrative Unit Layers (GAUL) data we can use the example code….\n\nvar dataset = ee.FeatureCollection(\"FAO/GAUL/2015/level2\");\n\nMap.setCenter(12.876, 42.682, 5);\n\nvar styleParams = {\n  fillColor: 'b5ffb4',\n  color: '00909F',\n  width: 1.0,\n};\n\n// comment this out otherwise it will just display as RBG data\n//dataset = dataset.style(styleParams);\n\nMap.addLayer(dataset, {}, 'Second Level Administrative Units');\n\nMake a new script and copy this code in, then use the inspector to select a study area and filter the vector data to just that area. Note, that level 2 is different between different countries.\nI am going to focus on Shenzhen, but select any city. My inspector shows….\n\n\n\n\n\n\n\n\nTo filter out my study area….\n\nvar shenzhen = dataset.filter('ADM2_CODE == 13030');\n\nBe careful here as the column is numeric not string, so the value 13030 is not in “” (like it was last week with the GADM data)"
  },
  {
    "objectID": "6_classification_I.html#eo-data",
    "href": "6_classification_I.html#eo-data",
    "title": "\n6  Classification I\n",
    "section": "\n6.4 EO data",
    "text": "6.4 EO data\nHere i’m going to use Sentinel data. If you search the data catalog for sentinel there will be a wide variety to select from, here i have gone for surface reflectance and the harmonized product that shifts newer images to be the same as that in older images, creating a consistent collection!\nThere is code provided for us to load us on the Harmonized Sentinel-2 MSI: MultiSpectral Instrument Level-2A catalog page\nLooking at this code you will see there is a cloud mask. There are two options we have when dealing with clouds.\n\nTo filter the entire collection based on a low cloud coverage percentage per tile, like we did last week\nTo set a higher percentage per tile (e.g. 20) but then to use the quality assurance (QA) bands to filter the clouds per pixel. Scrolling down the bands list on the catalog page there is a description of the bitmask for the relevant QA band (QA60). Here we can see the value for each pixel. This is the same layer that we get when downloading Sentinel data, in practical one we could see this in the masks folder….\n\n\n\n\n\n\n\n\n\nLet’s now try both approaches…\nThe first thing to note is that the Sentinel data is scaled by 10000 so in order to display this between 0 and 1 we need to divide collection using a function…\n\n//create the function\nvar divide10000 = function(image) {\n  return image.divide(10000);\n};\n\n\nLet’s load the imagery with minimal cloud cover, now depending on your study area and the propesnity for it to be cloudy (like the UK) you will need to change the dates and compromise on the less than % of cloud cover per tile…i wanted just to get 2022 data for this composite, with less than .1% cloud cover…the latter limited the images i could select from so i changed it to 1%\n\n\nvar wayone = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n                  .filterDate('2022-01-01', '2022-10-31')\n                  .filterBounds(shenzhen)  // Intersecting ROI\n                  // Pre-filter to get less cloudy granules.\n                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',1));\n\nvar wayone_divide = wayone.map(divide10000);\n\nvar visualization = {\n  min: 0.0,\n  max: 0.3,\n  bands: ['B4', 'B3', 'B2'],\n};\n\nMap.centerObject(shenzhen, 10);\n\nMap.addLayer(wayone_divide, visualization, 'wayoneRGB');\n\nRemember that if we wanted to see all the files we have here, we can export it to a .csv\n\nExport.table.toDrive(wayone_divide, 'exportAsCSV_wayone', 'GEE', 'CSVexport', 'CSV');\n\nFor example…i have a tile with an index 20220404T025539_20220404T030757_T49QGE\nHere….\n\nThe first part is the sensing data and time (the T) - 20220404T025539\n\nThe second part is the product generation date and time - 20220404T030757\n\nThe third part is the UTM grid reference in the Military Grid Reference System (MGRS). Where T49 is the grid zone designator\n\nIn my case this gives me 13 images…now what about increasing the cloud coverage and masking the cloud pixels\n\nFirst we need another function to mask the cloudy pixels and do the division…\n\n\nfunction maskS2clouds(image) {\n  var qa = image.select('QA60');\n\n  // Bits 10 and 11 are clouds and cirrus, respectively.\n  var cloudBitMask = 1 << 10;\n  var cirrusBitMask = 1 << 11;\n\n  // Both flags should be set to zero, indicating clear conditions.\n  var mask = qa.bitwiseAnd(cloudBitMask).eq(0)\n      .and(qa.bitwiseAnd(cirrusBitMask).eq(0));\n\n  return image.updateMask(mask).divide(10000);\n}\n\nThen load and filter the collection with a cloud mask.Note a very similar method can be used for Landsat data\nOr https://developers.google.com/earth-engine/apidocs/ee-algorithms-landsat-simplecomposite\n\nvar waytwo = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n                  .filterDate('2022-01-01', '2022-10-31')\n                  .filterBounds(shenzhen)  // Intersecting ROI\n                  // Pre-filter to get less cloudy granules.\n                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',20))\n                  .map(maskS2clouds);\n                  \nMap.addLayer(waytwo, visualization, 'waytwoRGB');\n\nIn my case this produces a mixed result. I now have 62 images, but evidently the cloud mask hasn’t worked perfectly…\n\n\n\n\n\n\n\n\nNow, there are two ways we can deal with this…\n\nTake the median image from all images in our collection - meaning the middle value of each pixel that will result in the clouds being removed (as they are very reflective and will have high values)…adding\nUse something more advanced for cloud detection such as s2cloudless\n\nTaking the median returns…."
  },
  {
    "objectID": "6_classification_I.html#classification",
    "href": "6_classification_I.html#classification",
    "title": "\n6  Classification I\n",
    "section": "\n6.5 Classification",
    "text": "6.5 Classification\nThe main focus here is classification, this is where we take some samples, train a model and then apply the model to the rest of the image.\nIn classification we are using feature vectors, which are tables. For example pixel x has band values of 200, 300, 400 and a landcover class of grass. The row of the table is a feature, the pixel values are the properties and combined they are feature vectors.\nIn GEE the supervised classifiers include: CART, RandomForest, NaiveBayes and SVM. GEE only allows 1 million points for sampling (to train the classifier). If you have more than this you would need to take a sample.\nTo start we need some training data within our study area. I’m going to clip my median imagery to my admin boundary and then take some samples. To take samples either use QGIS (and upload the .shp), Google Earth (the desktop version that has high resolution imagery) or GEE itself to generate some samples….I will use GEE.\n\n6.5.1 Clip\n\nvar waytwo_clip = waytwo.clip(shenzhen)\n\nMap.addLayer(waytwo_clip, visualization, 'waytwoRGB_clip');\n\n\n6.5.2 Training data\nTo select training data in GEE, click the polygon symbol and add polygons for the landcovers you want to classify. Add a new layer for each different landcover.\n\n\n\n\n\n\n\n\nAs you add these multi-polygons they will appear as variables at the top of your script. Next we need to merge the polygons into a feature collection…\n\n// Make a FeatureCollection from the polygons\nvar polygons = ee.FeatureCollection([\n  ee.Feature(urban_low, {'class': 1}),\n  ee.Feature(water, {'class': 2}),\n  ee.Feature(urban_high, {'class': 3}),\n  ee.Feature(grass, {'class': 4}),\n  ee.Feature(bare_earth, {'class': 5}),\n  ee.Feature(forest, {'class': 6}),\n\n]);\n\nNext, we need to set some context for the classifier - which bands are we going to use and what is the classification property - we just set it as class….\n\n// Use these bands for classification.\nvar bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7'];\n// The name of the property on the points storing the class label.\nvar classProperty = 'class';\n\nNow, we need to pull out the data from our training areas…\n\n// Sample the composite to generate training data.  Note that the\n// class label is stored in the 'landcover' property.\nvar training = waytwo_clip.select(bands).sampleRegions({\n  collection: polygons,\n  properties: [classProperty],\n  scale: 10\n});\n\nWhen i print this variable you can see the values that have been pullout out of the data\n\nprint(training, \"training\")"
  },
  {
    "objectID": "6_classification_I.html#classify",
    "href": "6_classification_I.html#classify",
    "title": "\n6  Classification I\n",
    "section": "\n6.6 Classify",
    "text": "6.6 Classify\nTo train our classifier…\n\n// Train a CART classifier.\nvar classifier = ee.Classifier.smileCart().train({\n  features: training,\n  classProperty: classProperty,\n});\n// Print some info about the classifier (specific to CART).\nprint('CART, explained', classifier.explain());\n\nWhen you print out the classifier you will see:\n\nmax depth = max number of splits\nvariable importance\nnumber of nodes = leaf nodes, use to make a prediction.\n\n\n\n\n\nDecision Trees Explained With a Practical Example. Source: Towards AI\n\n\n\n\nThen we can classify the image…\n\n// Classify the image.\nvar classified = waytwo_clip.classify(classifier);\n\nAnd plot the output…\n\n// add output\nMap.centerObject(shenzhen);\nMap.addLayer(classified, {min: 1, max: 5, palette: ['d99282', '466b9f', 'ab0000', 'dfdfc2', 'b3ac9f', '1c5f2c']}, \"classified\");\n\n// colours from: https://gis.stackexchange.com/questions/336043/why-do-original-colors-in-gee-images-appear-in-different-colors\n\n\n\n\n\n\n\n\n\nWe can then query the classified layer using the inspector, the classification value will correspond to the one you assigned….\nIn my case:\n\nLow urban albedo\nHigh urban albedo\nGrass\nBare earth\nForest"
  },
  {
    "objectID": "6_classification_I.html#train-test-split",
    "href": "6_classification_I.html#train-test-split",
    "title": "\n6  Classification I\n",
    "section": "\n6.7 Train test split",
    "text": "6.7 Train test split\nNot the problem with this is that we didn’t retain a % of the data to test the model…we can fix this…this time i am going to use a Random Forest classifier, which is just many (or in this case 100) decison trees….read the code and comment after before you run this…\n\n// Optionally, do some accuracy assessment.  Fist, add a column of\n// random uniforms to the training dataset.\nvar withRandom = polygons.randomColumn('random');\n\n\n// We want to reserve some of the data for testing, to avoid overfitting the model.\nvar split = 0.5;  // Roughly 70% training, 30% testing.\nvar trainingPartition = withRandom.filter(ee.Filter.lt('random', split));\nvar testingPartition = withRandom.filter(ee.Filter.gte('random', split));\n\nprint(trainingPartition, \"train\")\nprint(testingPartition, \"test\")\n\n\n// take samples from image for training and validation  \nvar training = waytwo_clip.select(bands).sampleRegions({\n  collection: trainingPartition,\n  properties: ['class'],\n  scale: 10,\n});\n\n\nvar validation = waytwo_clip.select(bands).sampleRegions({\n  collection: testingPartition,\n  properties: ['class'],\n  scale: 10\n});\n\n// Random Forest Classification\n\nvar rf1 = ee.Classifier.smileRandomForest(100)\n    .train(training, 'class');\n    \n\n////classify//////\n    \nvar rf2 = waytwo_clip.classify(rf1);\n\n// Classify the test FeatureCollection.\nvar test = validation.classify(rf1);\n\nvar testAccuracy = test.errorMatrix('class', 'classification');\nvar consumers=testAccuracy.consumersAccuracy()\n\nprint('Validation error matrix: ', testAccuracy);\nprint('Validation overall accuracy: ', testAccuracy.accuracy())\nprint('Validation consumer accuracy: ', consumers);\n\nMap.addLayer(rf2, {min: 1, max: 5, palette: ['d99282', '466b9f', 'ab0000', 'dfdfc2', 'b3ac9f', '1c5f2c']}, \"RF\");\n\nHowever, when i plot this map it gives a worse looking result and the accuracy doesn’t make sense. Can you think why this is in this case? If not keep reading to find out…"
  },
  {
    "objectID": "6_classification_I.html#train-test-split-pixel",
    "href": "6_classification_I.html#train-test-split-pixel",
    "title": "\n6  Classification I\n",
    "section": "\n6.8 Train test split pixel",
    "text": "6.8 Train test split pixel\nThe problem above was that the train test split was done on the polygons and i didn’t draw that many in my original training data….So the process went…. draw polygons > make a feature collection (where i assigned the land cover classes values) > select a test train split of the polygons using random numbers > extract the values > train the model….\nNow i can get around this by using a pixel approach….draw polygons > select a number of points from each class (e.g. 500) > assign the pixels random numbers > generate a test train split > extract the values > train the model….\nThanks to Ollie Ballinger for some of this code…\n\n////Pixel approach/////////////\n\nvar pixel_number= 1000;\n\nvar urban_low_points=ee.FeatureCollection.randomPoints(urban_low, pixel_number).map(function(i){\n  return i.set({'class': 1})})\n  \nvar water_points=ee.FeatureCollection.randomPoints(water, pixel_number).map(function(i){\n  return i.set({'class': 2})})\n  \nvar urban_high_points=ee.FeatureCollection.randomPoints(urban_high, pixel_number).map(function(i){\n  return i.set({'class': 3})})\n\nvar grass_points=ee.FeatureCollection.randomPoints(grass, pixel_number).map(function(i){\n  return i.set({'class': 4})})\n\nvar bare_earth_points=ee.FeatureCollection.randomPoints(bare_earth, pixel_number).map(function(i){\n  return i.set({'class': 5})})\n\nvar forest_points=ee.FeatureCollection.randomPoints(forest, pixel_number).map(function(i){\n  return i.set({'class': 6})})\n\nvar point_sample=ee.FeatureCollection([urban_low_points,\n                                  water_points,\n                                  urban_high_points,\n                                  grass_points,\n                                  bare_earth_points,\n                                  forest_points])\n                                  .flatten()\n                                  .randomColumn();\n\n// assign 70% of training points to validation \nvar split=0.7\nvar training_sample = point_sample.filter(ee.Filter.lt('random', split));\nvar validation_sample = point_sample.filter(ee.Filter.gte('random', split));\n\n// take samples from image for training and validation  \nvar training = waytwo_clip.select(bands).sampleRegions({\n  collection: training_sample,\n  properties: ['class'],\n  scale: 10,\n});\n\nvar validation = waytwo_clip.select(bands).sampleRegions({\n  collection: validation_sample,\n  properties: ['class'],\n  scale: 10\n});\n\n// Random Forest Classification\n\nvar rf1_pixel = ee.Classifier.smileRandomForest(100)\n    .train(training, 'class');\n\n// Get information about the trained classifier.\nprint('Results of RF trained classifier', rf1_pixel.explain());\n\n\n// // --------------------- Step 3: Conduct classification --------------------------------\n    \nvar rf2_pixel = waytwo_clip.classify(rf1_pixel);\n\nMap.addLayer(rf2_pixel, {min: 1, max: 5, \n  palette: ['d99282', '466b9f', 'ab0000', 'dfdfc2', 'b3ac9f', '1c5f2c']},\n  \"RF_pixel\");\n// // --------------------- Step 4: Assess Accuracy --------------------------------\n\nvar trainAccuracy = rf1_pixel.confusionMatrix();\nprint('Resubstitution error matrix: ', trainAccuracy);\nprint('Training overall accuracy: ', trainAccuracy.accuracy());\n\nvar validated = validation.classify(rf1_pixel);\n\nvar testAccuracy = validated.errorMatrix('class', 'classification');\nvar consumers=testAccuracy.consumersAccuracy()\n\nprint('Validation error matrix: ', testAccuracy);\nprint('Validation overall accuracy: ', testAccuracy.accuracy())\nprint('Validation consumer accuracy: ', consumers);\n\nHere:\n\nRandom Forest Out of Bag Error Estimate. Remember that a RF is a collection of decision trees, in each tree not all the data is used and the left over data is termed the out of bag sample (OOB).After the DTs have been trained the OOB data is then applied to all the DTs that did not use it within the training data. Each DT predicts the outcome using the unseen data and the score is created through the correctly precited rows from the OOB sample across the trees. Here my error is about 1.2% which matches my accuracy of 98%.\nResubstituion accuracy is where we take the original training data and compare it to the modeled output. Mine is 99%.\nThe confusion matrix is where we held back our % of the original data calling it our validation or test data. We then trained with the model with our training data, took the model and applied it to our testing data. This matrix compares the output of the model and our validation data (which wasn’t used to train it). Mine is 98%.\n\nFinal output:"
  },
  {
    "objectID": "6_classification_I.html#learning-diary",
    "href": "6_classification_I.html#learning-diary",
    "title": "\n6  Classification I\n",
    "section": "\n6.9 Learning diary",
    "text": "6.9 Learning diary\nConsult the assignment requirements document and complete your learning diary entry in your Quarto learning diary."
  },
  {
    "objectID": "6_classification_I.html#feedback",
    "href": "6_classification_I.html#feedback",
    "title": "\n6  Classification I\n",
    "section": "\n6.10 Feedback",
    "text": "6.10 Feedback\nWas anything that we explained unclear this week or was something really clear…let us know using the feedback form. It’s anonymous and we’ll use the responses to clear any issues up in the future / adapt the material."
  },
  {
    "objectID": "7_classification_II.html#resources",
    "href": "7_classification_II.html#resources",
    "title": "\n7  Classification II\n",
    "section": "\n7.1 Resources",
    "text": "7.1 Resources\n\n\n\n\n\n\nThis week\n\n\n\n\nFoody, G.M., 2004. Sub-Pixel Methods in Remote Sensing, in: Jong, S.M.D., Meer, F.D.V. der (Eds.), Remote Sensing Image Analysis: Including The Spatial Domain, Remote Sensing and Digital Image Processing. Springer Netherlands, Dordrecht, pp. 37–49.\n\nJensen, J.R., 2015. Introductory digital image processing: a remote sensing perspective. Prentice-Hall Inc.\n\nThematic Information Extraction: Pattern Recognition, Chapter 9\n\nobject based image analysis classification\n\n\nInformation Extraction Using Image Spectroscopy, Chapter 11\n\nsubpixel classification, linear spectral unmixing or spectral mixture analysis\n\n\n\n\n\nCloud-Based Remote Sensing with Google Earth Engine (accessed 1.5.23).\n\nChapter F3.3: Object-Based Image Analysis\n\n\nBarsi, Á., Kugler, Z., László, I., Szabó, G., Abdulmutalib, H., 2018. Accuracy dimensions in remote sensing. International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences 42.\nKarasiak, N., Dejoux, J.-F., Monteil, C., Sheeren, D., 2022. Spatial dependence between training and test sets: another pitfall of classification accuracy assessment in remote sensing. Mach Learn 111, 2715–2740.\n\n\n\nIn this practical we will look at some more advanced classification methods and how to do change detection…"
  },
  {
    "objectID": "7_classification_II.html#data",
    "href": "7_classification_II.html#data",
    "title": "\n7  Classification II\n",
    "section": "\n7.2 Data",
    "text": "7.2 Data\n\n7.2.1 Vector data\nHere i will load the level 2 Global Administrative Unit Layers (GAUL), but style it on the map\n\n//--------------------------Vector data---------------------------\n\nvar dataset = ee.FeatureCollection(\"FAO/GAUL/2015/level2\");\n\nvar dataset_style = dataset.style({\n  color: '1e90ff',\n  width: 2,\n  fillColor: '00000000',  // with alpha set for partial transparency\n//  lineType: 'dotted',\n//  pointSize: 10,\n//  pointShape: 'circle'\n});\n\nMap.addLayer(dataset_style, null, 'Second Level Administrative Units');\n\nNote that the fill colour is set to 0, making it transparent. If i wanted to set this to a colour i could use any hex colour code and then adding a value (alpha) for transparency…e.g. e32636 and then e3263675 for 75% transparency. However this means that the attributes of the data are lost…so this might be more appropriate:\n\nMap.addLayer(dataset, {}, 'Second Level Administrative Units_2');\n\nNext, i will filter my study area…\n\nvar Daressalaam = dataset.filter('ADM1_NAME == \"Dar-es-salaam\"');\n\nMap.addLayer(Daressalaam, {}, 'Dar-es-salaam');\n\n\n7.2.2 EO data\nThis week i am going to switch back to Landsat…we need to…\n\nset the scale function\nload the imagery\n\nfiler based on dates\nfilter based on the study area\nfilter on cloud coverage\napply the scale function\nextract the median\n\n\nmap the output\n\n\n//--------------------------Landsat data---------------------------\n\nfunction applyScaleFactors(image) {\n  var opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n  var thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0);\n  return image.addBands(opticalBands, null, true)\n              .addBands(thermalBands, null, true);\n}\n\nvar oneimage_study_area_cloud = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n  .filterDate('2022-01-01', '2022-10-10')\n  .filterBounds(Daressalaam)  // Intersecting ROI\n  .filter(ee.Filter.lt(\"CLOUD_COVER\", 1))\n  .map(applyScaleFactors)\n  .reduce(ee.Reducer.median());\n\n// set up some of the visualisation paramters \nvar vis_params = {\n  bands: ['SR_B4_median', 'SR_B3_median', 'SR_B2_median'],\n  min: 0.0,\n  max: 0.3,\n};\n\nMap.addLayer(oneimage_study_area_cloud, vis_params, 'True Color (432)');\n\nNow in Landsat, just like Sentinel we also have a quality assessment band that we can use to mask out pixels such as clouds or cloud shadows…to use this we can make another function and then call it within the image loading code, just like the scaling above….\nThe function takes the values from the QA band — see QA Pixel in the GEE catalog and makes a mask. This code originated from stack exchange with confirmation coming from Niek Gorelick, one of the GEE founders.\n\nvar cloudMaskC2L7 = function(image) {\n  var dilatedCloud = (1 << 1)\n  var cloud = (1 << 3)\n  var cloudShadow = (1 << 4)\n  var qa = image.select('QA_PIXEL');\n  var mask = qa.bitwiseAnd(dilatedCloud)\n    .and(qa.bitwiseAnd(cloud))\n    .or(qa.bitwiseAnd(cloudShadow))\n  return image.updateMask(mask.not());\n}\n\nTo add this into the code that loads the image collection….\n\nvar oneimage_study_area_cloud = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n  .filterDate('2022-01-01', '2022-10-10')\n  .filterBounds(Daressalaam)  // Intersecting ROI\n  .filter(ee.Filter.lt(\"CLOUD_COVER\", 1))\n  .map(cloudMaskC2L7) /// this is the new function\n  .map(applyScaleFactors)\n  .reduce(ee.Reducer.median());\n\nNow when i look at my image i can see the masked areas…\n\n\n\n\n\n\n\n\nHowever, are these areas actually clouds? No, it’s sand or suspended sediment….\n\n\n\n\n\n\n\n\nTo use the QA band or not, generally with SR data i don’t. Before we finish loading our data I am going to select the bands I want and clip the image to the study area…all in one nice chunk of code..\n\nvar oneimage_study_area_cloud = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n  .filterDate('2022-01-01', '2022-10-10')\n  .filterBounds(Daressalaam)  // Intersecting ROI\n  .filter(ee.Filter.lt(\"CLOUD_COVER\", 1))\n  //.map(cloudMaskC2L7)\n  .map(applyScaleFactors)\n  .reduce(ee.Reducer.median())\n  .select(['SR_B1_median', 'SR_B2_median', 'SR_B3_median', 'SR_B4_median', 'SR_B5_median', 'SR_B6_median', 'SR_B7_median'])\n  .clip(Daressalaam);\n  \nMap.centerObject(Daressalaam, 10)\n\n\n\n\n\n\n\n\n\nNow let’s classify…"
  },
  {
    "objectID": "7_classification_II.html#classify",
    "href": "7_classification_II.html#classify",
    "title": "\n7  Classification II\n",
    "section": "\n7.3 Classify",
    "text": "7.3 Classify\n\n7.3.1 Sub-pixel\nSpectral unfixing is where we provide an end member and compute the fractions of each pixel from the often “spectrally pure” spectral signature. However, no pixel is ever (or rarely) on landcover type. There are two main ways to do this in GEE.\n\nDefine a variable that is the end member (this could be a point)\nSelect some training data of landcover and then take a average that becomes the endmember\n\nOnce we have the data we then pass this to the unmix() function that will generate and image.\nIn the first example, this could involve querying the image to generate a list of values…such as that from the manual\n\nvar urban = [88, 42, 48, 38, 86, 115, 59]\nvar veg = [50, 21, 20, 35, 50, 110, 23];\nvar water = [51, 20, 14, 9, 7, 116, 4];\n\nHowever, we can also use the image itself to generate these lists in way 2.\nIn this case i have created a point (or it could be a polygon) and extracted the (mean) value….\n\n//------------------urban-------------------\n\nvar urban_val = oneimage_study_area_cloud.reduceRegion({reducer: \n  ee.Reducer.mean(),\n  geometry:urban, \n  scale: 30, \n  maxPixels: 1e8});\n\nvar urban_val2 = urban_val.values()\n\nOnce we have got this working for one example, we can make it into a function…\n\n//------------------function-------------------\n\nfunction data_extract (image, vector) {\n\nvar data =  image.reduceRegion({reducer: \n  ee.Reducer.mean(),\n  geometry:vector, \n  scale: 30, \n  maxPixels: 1e8});\n  return data.values(); // here we use return\n}\n\nThen we can call it with the different landcover types we have…\n\nvar urban_data = data_extract(oneimage_study_area_cloud, urban);\nvar grass_data = data_extract(oneimage_study_area_cloud, grass);\nvar bare_earth_data = data_extract(oneimage_study_area_cloud, bare_earth);\nvar forest_data = data_extract(oneimage_study_area_cloud, forest);\n\nUnmix the image and add it to the map…recall what this is from the lecture\n\n//------------------fractions-------------------\nvar fractions = oneimage_study_area_cloud.unmix([urban_data, grass_data, bare_earth_data, forest_data]);\nMap.addLayer(fractions, {}, 'unmixed');\n\nNow at the moment these are unconstrained, meaning they don’t sum to one and we have negative values…to change this add the following from the documentation.\n\nvar fractions_constrained = oneimage_study_area_cloud.unmix([urban_data, grass_data, bare_earth_data, forest_data], true, true);\nMap.addLayer(fractions_constrained, {}, 'constrained fractions');\n\nNow we have the fraction (or percent) of each endmember per pixel. But how is this helpful and What can we do with this ?\nCredit the Earth Engine Intermediate Curriculum for some code here.\n\n7.3.1.1 Accuracy\nA problem with using sub pixel analysis is that it is harder to calculate an error matrix. There are two main ways to deal with this.\n\nHarden the sub-pixel image and classify each pixel that has the largest proportion of land cover (e.g. 50% of more)\nTake a very high resolution image > overlay the Landsat pixel (through a fishnet grid) > select a pixel > digitised the high resolution data within the pixel > compare them. This is a lot more involved, but possible with Google Earth (the desktop app that has high resoltion imagery)\n\nWe will focus on way 1.\nNow, an approach to do this is use a mask (or filter). For example, when a pixel is greater than 0.5 then assign it a value of x, which will be the land cover…in the below code, urban is 1 and so on.\n\nvar reclassified_urban = fractions_constrained.expression('b(0) >.5 ? 1 : 0');\nvar reclassified_grass = fractions_constrained.expression('b(1) >.5 ? 2 : 0');\nvar reclassified_bare_earth = fractions_constrained.expression('b(2) >.5 ? 2 : 0');\nvar reclassified_forest = fractions_constrained.expression('b(3) >.5 ? 3 : 0');\n\nHere, the band is represented by b(0) then we are querying it to find the pixels greater than .5, if that is true for the pixel (denoted by the ?) then assign a value of 1, if false (denoted by the : then assign a value of 0). We could also add another condition for example, greater than .5 but less than .9…\nWe then need to add all these images together, note they are just plain images not a collection….\n\nvar reclassified_all = reclassified_urban.add(reclassified_grass)\n                                .add(reclassified_bare_earth)\n                                .add(reclassified_forest)\n                                // the output is made at a larger extent so clip\n                                .clip(Daressalaam);\n\nAnd map the output…\n\nMap.addLayer(reclassified_all, {min: 1, max: 3, palette: ['d99282', 'dfdfc2', 'b3ac9f', '1c5f2c']}, \"reclassified_all\");\n\nHow could we run some accuracy assessment from this point, note it would be easier if we collected lots of training data at the start and held a % back before making an average endmember.\n\n7.3.2 Object based\n\n7.3.3 Image\nOtherwise know as object based image analysis (OBIA) as opposed to pixel analysis, pixels are grouped together based on a set of rules…\nThe first port of call here is image gradient, this is the directional change in intensity or colour in an image. To do this…\n\nvar gradient = oneimage_study_area_cloud.gradient();\n\nThe output gives us an x and y image for horizontal and vertical changes but this isn’t that useful…\nWe could change this to spectral gradient this is a per-pixel difference between erosion and dilation. Erosion is the pixel closet to the spectra and the dilation is the furthest.\n\nvar spectralgradient = oneimage_study_area_cloud.spectralGradient();\n\nMap.addLayer(spectralgradient,{}, \"spectral_gradient\");\n\nWithin this function we can also set the metric for the spectral gradient (e.g. spectral angle mapper is ‘sam’) and the kernel size, the default is a square radius of 1.\nTo make either of these into objects we’d need to vectorise the raster data - which might not be that useful, if you have high resolution data it will work better…\n\n7.3.4 Super pixels\nThe basic idea here is the same as the lecture, take a grid of points and get the pixels around them. We want to reduce the pixels to fewer objects…all pixels must go somewhere and then we can classify the objects….\nTo start with, let’s try k-means clustering of pixels….\n\nvar kmeans = ee.Algorithms.Image.Segmentation.KMeans({\n  image: oneimage_study_area_cloud,\n  numClusters: 1000,\n  numIterations: 100,\n // neighborhoodSize: 2,\n  //gridSize: 2,\n  forceConvergence: false,\n  uniqueLabels: true\n})\n\nMap.addLayer(kmeans, {}, 'kmeans')\n\nThis will produce a large grid of pixels (squares basically) of similar kinds of pixels, we could then take the average spectral reflectance and classify them. Note that the output is just a 1 band image with a cluster ID…\nAnother approach we can try is Simple non-iterative clustering (SNIC), this makes clusters without k-means. It uses a regular grid of points (like k-means) but then assigns pixels to points through distance color and co-ordinates - it represents normalised spatial and color distances.\nLet’s make a seed gird….the number denotes the spacing in pixels (i assume this is GEE pixels not our Landsat pixels), we can also set a square or hex grid\n\nvar seeds = ee.Algorithms.Image.Segmentation.seedGrid(40, \"hex\");\n\nMap.addLayer(seeds, {}, 'seeds')\n\nTo create the objects it’s then…\n\n// Run SNIC on the regular square grid.\nvar snic = ee.Algorithms.Image.Segmentation.SNIC({\n  image: oneimage_study_area_cloud, \n  //size: 50,  // don't need it seeds given\n  compactness: 1,\n  connectivity: 8,\n  neighborhoodSize:50,\n  seeds: seeds\n});\n\nMap.addLayer(snic, {}, 'means', true ,0.6)\n\nHere we have:\n\nimage = our image\nsize = seed locations, not needed as we have a seed layer\ncompactness = represents how compact the super pixels are. A higher value means compact superpixels which results in regular hexes or squares as opposed to different shapes and sizes….\nconnectivity = distances (as above) to either 4 or 8 connected pixels to current cluster.generally the higher the value the smoother the objects will appear\nneighborhoodSize = I believe this sets a limit for the clusters and defaults to 2 * the seed size, if you comment it out (or set it to 1) you will see large squares around some of the clusters. Probably not overly useful here, so i will set this quite high.\nseeds = our seed layer\n\nPrinting the snic variable we can see the mean values across each object and the cluster (or pixel) ID\n\nprint(snic, \"snic\")\n\nOnce we have the clusters we can extract some more info that might be useful to us…such as…would the area / perimeter / standard deviation of the objects be similar across the image and can we use that as a band in classification? Or perhaps NDVI per object that we could use to threshold out green objects such as parks and forests…\nHere is the standard deviation for each band within the object. We must append the cluster ID back to the original image data then reduce the image based on the clusters and finally calculate the standard deviation. The final value here is the maxSize that is the size of the neigbhourhood for aggregating the values - any object larger than this will be masked - we should set it to the same as our neighborhoodSize\n\n//------------------OBIA stats-------------------\n\nvar clusters = snic.select('clusters')\nvar stdDev = oneimage_study_area_cloud.addBands(clusters).reduceConnectedComponents(ee.Reducer.stdDev(), 'clusters', 100)\n\nMap.addLayer(stdDev, {min:0, max:0.1}, 'StdDev', true)\n\nFor NDVI of each object we can just use the mean SR per object…\n\nvar NDVI = snic.normalizedDifference(['SR_B5_median_mean', 'SR_B4_median_mean']);\nMap.addLayer(NDVI, {min:-1, max:1}, 'NDVI', false)\n\nOr we could add NDVI as a band to the original image and then compute the standard deviation. This is useful as it shows the dispersion of values across each object….e.g. a park might have a low standard deviation in the object, but an urban area might have a higher standard deviation….\nOnce we have what we want, create an image….\n\n//------------------OBIA merge-------------------\n\nvar bands = ['SR_B1_median_mean', 'SR_B2_median_mean', 'SR_B3_median_mean', 'SR_B4_median_mean',\n'SR_B5_median_mean', 'SR_B6_median_mean', 'SR_B7_median_mean']\n\nvar objectPropertiesImage = ee.Image.cat([\n  snic.select(bands),\n  stdDev,\n  NDVI\n]).float();\n\nTrain and classify….we need to get some training data….so select some points or reuse / add to your points from the mixture classification. Note, that if you only have 1 point per landcover class the classification will be very poor! Note, that some publications / workflows actually use classified data products (e.g. Dynamic World) to produce the training and test data.\nNote this code is adapted from the first classification we did last week, you might consider adding in the Train test split code as well to assess the accuracy of the image….\n\n//------------------ Classification -------------------\n\n// Make a FeatureCollection from the polygons\nvar points = ee.FeatureCollection([\n  ee.Feature(urban, {'class': 1}),\n  ee.Feature(grass, {'class': 2}),\n  ee.Feature(bare_earth, {'class': 5}),\n  ee.Feature(forest, {'class': 6}),\n]);\n\n// The name of the property on the points storing the class label.\nvar classProperty = 'class';\n\n// Sample the composite to generate training data.  Note that the\n// class label is stored in the 'landcover' property.\nvar training = objectPropertiesImage.sampleRegions({\n  collection: points,\n  properties: [classProperty],\n  scale: 30\n});\n\n// Train a CART classifier.\nvar classifier = ee.Classifier.smileCart().train({\n  features: training,\n  classProperty: classProperty,\n});\n\nvar classified = objectPropertiesImage.classify(classifier);\n\nMap.addLayer(classified, {min: 1, max: 5, palette: ['d99282', 'dfdfc2', 'b3ac9f', '1c5f2c']}, \"classified\");"
  },
  {
    "objectID": "7_classification_II.html#learning-diary",
    "href": "7_classification_II.html#learning-diary",
    "title": "\n7  Classification II\n",
    "section": "\n7.4 Learning diary",
    "text": "7.4 Learning diary\nConsult the assignment requirements document and complete your learning diary entry in your Quarto learning diary.\n\n7.4.1 Extra resources\nImage Segmentation and OBIA by Noel Gorelick:\n\npresentation\nvideo\nThe paper: Superpixels and Polygons using Simple Non-Iterative Clustering\nThe original GEE SNIC example"
  },
  {
    "objectID": "7_classification_II.html#feedback",
    "href": "7_classification_II.html#feedback",
    "title": "\n7  Classification II\n",
    "section": "\n7.5 Feedback",
    "text": "7.5 Feedback\nWas anything that we explained unclear this week or was something really clear…let us know using the feedback form. It’s anonymous and we’ll use the responses to clear any issues up in the future / adapt the material."
  },
  {
    "objectID": "8_temperature.html#resources",
    "href": "8_temperature.html#resources",
    "title": "\n8  Temperature\n",
    "section": "\n8.1 Resources",
    "text": "8.1 Resources\n\n\n\n\n\n\nThis week\n\n\n\n\nWilson, B., 2020. Urban Heat Management and the Legacy of Redlining. Journal of the American Planning Association 86, 443–457\nKlinenberg, E., 1999. Denaturalizing Disaster: A Social Autopsy of the 1995 Chicago Heat Wave. Theory and Society 28, 239–295.\nLi, D., Newman, G.D., Wilson, B., Zhang, Y., Brown, R.D., 2022. Modeling the relationships between historical redlining, urban heat, and heat-related emergency department visits: An examination of 11 Texas cities. Environment and Planning B: Urban Analytics and City Science 49, 933–952.\n\nCloud-Based Remote Sensing with Google Earth Engine (accessed 1.5.23).\n\nChapter A1.5 Heat Islands\n\n\nMacLachlan, A., Biggs, E., Roberts, G., Boruff, B., 2021. Sustainable City Planning: A Data-Driven Approach for Mitigating Urban Heat. Frontiers in Built Environment 6.\n\n\n\nWithin this practical we are going to explore temperature across urban areas using two different data products: Landsat and MODIS. We’ll then look at which admin areas are hottest over time.\nThe first stage of this practical is to load up a level 2 admin area to extract temperature for…."
  },
  {
    "objectID": "8_temperature.html#vector-data",
    "href": "8_temperature.html#vector-data",
    "title": "\n8  Temperature\n",
    "section": "\n8.2 Vector data",
    "text": "8.2 Vector data\n\n//--------------------------Vector data---------------------------\n\nvar dataset = ee.FeatureCollection(\"FAO/GAUL/2015/level1\");\n\nvar dataset_style = dataset.style({\n  color: '1e90ff',\n  width: 2,\n  fillColor: '00000000',  // with alpha set for partial transparency\n//  lineType: 'dotted',\n//  pointSize: 10,\n//  pointShape: 'circle'\n});\n\nMap.addLayer(dataset, {}, 'Second Level Administrative Units_1');\n\nvar Beijing = dataset.filter('ADM1_CODE == 899');\n\n\nMap.addLayer(Beijing, {}, 'Beijing');"
  },
  {
    "objectID": "8_temperature.html#landsat",
    "href": "8_temperature.html#landsat",
    "title": "\n8  Temperature\n",
    "section": "\n8.3 Landsat",
    "text": "8.3 Landsat\nNext, the Landsat data…note below that i have set a month range as i want to consider the summer and not the whole year…\n\n//--------------------------Landsat data---------------------------\n\nfunction applyScaleFactors(image) {\n  var opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n  var thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0);\n  return image.addBands(opticalBands, null, true)\n              .addBands(thermalBands, null, true);\n}\n\nvar landsat = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n  .filterDate('2022-01-01', '2022-10-10')\n  .filter(ee.Filter.calendarRange(5, 9,'month'))\n  .filterBounds(Beijing)  // Intersecting ROI\n  .filter(ee.Filter.lt(\"CLOUD_COVER\", 1))\n  .map(applyScaleFactors);\n\nprint(landsat)\n\nThe temperature band in Landsat is B10, however it comes in Kelvin and not Celsius, to cover we just subtract 273.1. But i want to do this per image…at the moment .subtract won’t work over an image collection so we can subtract the value from each individual image in a collection…we also want to mask any pixels that have a below 0 value (e.g. probably no data ones). I’ve done this within this function as well..\n\nvar subtracted = landsat.select('ST_B10').map(function (image) {\n  var subtract = image.subtract(273.1); // subtract\n  var mask = subtract.gt(0); //set mask up\n  var mask_0 = subtract.updateMask(mask); //Apply this in a mask\nreturn mask_0\n})  \n\nIf we were to plot this now, we would still show all of the images we have in the collection, but, we can take the mean (“reduce”) and clip to our study area…\n\nvar subtracted_mean = subtracted.reduce(ee.Reducer.mean())\n  .clip(Beijing)\n\nThen finally plot the data…\n\n// set up some of the visualisation paramters \n// the palette is taken from the MODIS example (which we will see later on)\nvar vis_params = {\n  min: 20,\n  max: 55,\n    palette: [\n    '040274', '040281', '0502a3', '0502b8', '0502ce', '0502e6',\n    '0602ff', '235cb1', '307ef3', '269db1', '30c8e2', '32d3ef',\n    '3be285', '3ff38f', '86e26f', '3ae237', 'b5e22e', 'd6e21f',\n    'fff705', 'ffd611', 'ffb613', 'ff8b13', 'ff6e08', 'ff500d',\n    'ff0000', 'de0101', 'c21301', 'a71001', '911003'\n  ]\n};\n\nMap.addLayer(subtracted_mean, vis_params, 'Landsat Temp');"
  },
  {
    "objectID": "8_temperature.html#modis",
    "href": "8_temperature.html#modis",
    "title": "\n8  Temperature\n",
    "section": "\n8.4 MODIS",
    "text": "8.4 MODIS\nNow, we can also do the same process with the Moderate Resolution Imaging Spectroradiometer (MODIS), MODIS is an instrument aboard both the Terra and Aqua satellites. Terra crosses the equator from N to S in the morning and Aqua S to N in the afternoon meaning the entire earth is sampled every 1-2 days, with most places getting 2 images a day.\n\n8.4.1 Aqua\n\nvar MODIS_Aqua_day = ee.ImageCollection('MODIS/061/MYD11A1')\n  .filterDate('2022-01-01', '2022-10-10')\n  .filter(ee.Filter.calendarRange(5, 9,'month'))\n  .filterBounds(Beijing)  // Intersecting ROI;\n  .select('LST_Day_1km')\n\nprint(MODIS_Aqua_day, \"MODIS_AQUA\")  \n\n\n8.4.2 Terra\n\nvar MODIS_Terra_day = ee.ImageCollection('MODIS/061/MOD11A1')\n  .filterDate('2022-01-01', '2022-10-10')\n  .filter(ee.Filter.calendarRange(5, 9,'month'))\n  .filterBounds(Beijing)  // Intersecting ROI;\n  .select('LST_Day_1km')\n\nprint(MODIS_Terra_day, \"MODIS_Terra\")  \n\n\n8.4.3 Scaling\nFor the Landsat data in this period we had 5 images, compared to 108 for Aqua and 107 for Terra!But! MODIS data is at 1km resolution…and if you look at the values (through adding one of the collections to the map) we haven’t addressed the scaling….\nLike Landsat data MODIS data needs to have the scale factor applied, looking at the GEE documentation or the MODIS documentation we can see that the value is 0.02 and the values are in Kelvin, when we want Celcius….\n\nfunction MODISscale(image) {\n  var temp = image.select('LST_.*').multiply(0.02).subtract(273.1);\n  return image.addBands(temp, null, true)\n}\n\n\nvar MODIS_Aqua_day = ee.ImageCollection('MODIS/061/MYD11A1')\n  .filterDate('2022-01-01', '2022-10-10')\n  .filter(ee.Filter.calendarRange(5, 9,'month'))\n  .select('LST_Day_1km')\n  .map(MODISscale)\n  .filterBounds(Beijing);  // Intersecting ROI;\n\nprint(MODIS_Aqua_day, \"MODIS_AQUA\")  \n\nvar MODIS_Terra_day = ee.ImageCollection('MODIS/061/MOD11A1')\n  .filterDate('2022-01-01', '2022-10-10')\n  .filter(ee.Filter.calendarRange(5, 9,'month'))\n  .filterBounds(Beijing)  // Intersecting ROI;\n  .select('LST_Day_1km')\n  .map(MODISscale);\n\n\n8.4.4 Collection merge\nMerge the collections and plot a mean summer temperature image…\n\nvar mean_aqua_terra = MODIS_Aqua_day.merge(MODIS_Terra_day)\n  .reduce(ee.Reducer.mean())\n  .clip(Beijing)\n\nMap.addLayer(mean_aqua_terra, landSurfaceTemperatureVis,\n    'MODIS Land Surface Temperature');\n\n\n8.4.5 Display the results\n\nvar landSurfaceTemperatureVis = {\n  min: 15,\n  max: 45,\n  palette: [\n    '040274', '040281', '0502a3', '0502b8', '0502ce', '0502e6',\n    '0602ff', '235cb1', '307ef3', '269db1', '30c8e2', '32d3ef',\n    '3be285', '3ff38f', '86e26f', '3ae237', 'b5e22e', 'd6e21f',\n    'fff705', 'ffd611', 'ffb613', 'ff8b13', 'ff6e08', 'ff500d',\n    'ff0000', 'de0101', 'c21301', 'a71001', '911003'\n  ],\n};\n\nMap.addLayer(mean_aqua_terra, landSurfaceTemperatureVis,\n    'MODIS Land Surface Temperature');\n\nWhat do you see ?"
  },
  {
    "objectID": "8_temperature.html#timeseries",
    "href": "8_temperature.html#timeseries",
    "title": "\n8  Temperature\n",
    "section": "\n8.5 Timeseries",
    "text": "8.5 Timeseries\nSo the real benefit of MODIS data is that we can plot the time series…however in doing so we lose the spatial element…\n\nvar aqua_terra = MODIS_Aqua_day.merge(MODIS_Terra_day) //merge the two collections\n\nvar timeseries = ui.Chart.image.series({\n  imageCollection: aqua_terra,\n  region: Beijing,\n  reducer: ee.Reducer.mean(),\n  scale: 1000,\n  xProperty: 'system:time_start'})\n  .setOptions({\n     title: 'Temperature time series',\n     vAxis: {title: 'LST Celsius'}});\nprint(timeseries);\n\n\n\n\n\n\n\n\n\nTimes series modelling….https://developers.google.com/earth-engine/tutorials/community/time-series-modeling"
  },
  {
    "objectID": "8_temperature.html#statistics-per-spatial-unit",
    "href": "8_temperature.html#statistics-per-spatial-unit",
    "title": "\n8  Temperature\n",
    "section": "\n8.6 Statistics per spatial unit",
    "text": "8.6 Statistics per spatial unit\n\n8.6.1 Landsat\nTo start with let’s just explore the average temperature per GAUL level 2 area within the Beijing level 1 area….we can do this using the reduceRegions function….\n\n//--------------------------Statistics per level 2---------------------------.\n\nvar GAUL_2 = ee.FeatureCollection(\"FAO/GAUL/2015/level2\");\n\nvar Beijing_level2 = GAUL_2.filter('ADM1_CODE == 899');\n\nMap.addLayer(Beijing_level2, {}, 'Second Level Administrative Units_2');\n\nprint(subtracted_mean)\n\nvar mean_Landsat_level2 = subtracted_mean.reduceRegions({\n  collection: Beijing_level2,\n  reducer: ee.Reducer.mean(),\n  scale: 30,\n});\n\nMap.addLayer(mean_Landsat_level2, {}, 'mean_Landsat_level2');\n\nWhen we inspect the new vector file we can see that a mean column has been added to the attribute table….\n\n\n\n\n\n\n\n\nNotes of the reduce regions:\n\nThere is a reduce percentile function…reducer: ee.Reducer.percentile([10, 20, 30, 40, 50, 60, 70, 80, 90]) that might also be useful. This takes all the pixels in each polygon and lists the value of the percentiles specified in the attribute table.\n\n\n8.6.1.1 Export\nTo export a feature collection like this…\n\n// Export the FeatureCollection to a SHP file.\nExport.table.toDrive({\n  collection: mean_Landsat_level2,\n  description:'mean_Landsat_level2',\n  fileFormat: 'SHP'\n});\n\nWe can then open it in QGIS to see our average tempearture per spatial untit.\n\n8.6.2 MODIS\nTo get the time series data per spatial unit in MODIS we can use the similar code as before, but this time we:\n\nuse the function ui.Chart.image.seriesByRegion\n\nset our regions argument to the lower level spatial data….\n\n\nvar chart = ui.Chart.image.seriesByRegion({\n    imageCollection: MODIS_Aqua_day,\n    regions: Beijing_level2, //this is the difference\n    reducer: ee.Reducer.mean()\n})\n\nprint(chart)\n\nWe could make the plot a bit fancier in GEE…here, i have adapted the code from the documentation…\n\nvar timeseries_per_unit = ui.Chart.image.seriesByRegion({\n  imageCollection: aqua_terra,\n  //band: 'NDVI',\n  regions: Beijing_level2,\n  reducer: ee.Reducer.mean(),\n  scale: 1000,\n  //seriesProperty: 'label',\n  xProperty: 'system:time_start'\n})\n.setOptions({\n  title: 'Average temp per spatial unit',\n  hAxis: {title: 'Date', titleTextStyle: {italic: false, bold: true}},\n  vAxis: {\n    title: 'LST Celsius',\n    titleTextStyle: {italic: false, bold: true}\n  },\n  lineWidth: 1\n//  colors: ['0f8755', '808080'],\n});\n\n\n\n\n\n\n\n\n\n\n8.6.2.1 Export\nWe could then export the data to run some time series analysis for the spatial units (e.g. in R), this might be interesting if we know characteristics of land cover or development change over time…..\n\n// Collect block, image, value triplets.\nvar triplets = aqua_terra.map(function(image) {\n  return image.select('LST_Day_1km').reduceRegions({\n    collection: Beijing_level2.select(['ADM2_CODE']), \n    reducer: ee.Reducer.mean(), \n    scale: 30\n  }).filter(ee.Filter.neq('mean', null))\n    .map(function(f) { \n      return f.set('imageId', image.id());\n    });\n}).flatten();\n\nprint(triplets.first(), \"trip\")\n\nExport.table.toDrive({\n  collection: triplets, \n  description: triplets, \n  fileNamePrefix: triplets,\n  fileFormat: 'CSV'\n});\n\nYou can also change how the data is exported, although I would now just do some wrangling in R. See the:\n\nGEE presentation\nExample code for exporting time series per spatial unit"
  },
  {
    "objectID": "8_temperature.html#trend-analysis",
    "href": "8_temperature.html#trend-analysis",
    "title": "\n8  Temperature\n",
    "section": "\n8.7 Trend analysis",
    "text": "8.7 Trend analysis\nTo extended our analysis we could explore statistics that will establish if we have a significant increasing or decreasing trend within our data. For example…the Mann-Kendall trend test. Once we have identified these pixels or areas we could then investigate them further with Landsat surface temperature data and landcover data."
  },
  {
    "objectID": "8_temperature.html#heat-index",
    "href": "8_temperature.html#heat-index",
    "title": "\n8  Temperature\n",
    "section": "\n8.8 Heat index",
    "text": "8.8 Heat index\nIn the next part of the practical we are going to replicate a heat index produced by the BBC. Here it says…\n\nA statistical method published by academics was used to standardise land surface temperatures for each postcode area, which involved combining satellite images for different dates over the past three years.\n\nFurther…\n\nUsing satellite data from 4 Earth Intelligence, the BBC mapped how vulnerable postcode areas were to extreme heat in England, Wales and Scotland during periods of hot weather over the past three summers - shown with a heat hazard score.\n\nThe temperature was from….\n\nThe potential heat hazard score for each postcode area was calculated by 4EI, who measured the average land surface temperature over a sample of days in the past three summers across Britain.\n\nThe score was calculated ….\nIndex 1 = 40th percentile and lower, if you lined up all the postcodes by heat hazard score, your postcode is in the coolest 40%.\nIndex 2 = 40th - 70th percentile, your postcode is in the mid-range. 40% of postcodes have a lower heat hazard than yours but 30% have a higher one.\nIndex 3 is the 70-90th, 4 is the 90-99th and 5 is the 99th.\nHowever, feel free to change these assumptions, for example, as noted earlier is it more appropriate to use the percentile within each spatial unit than compare then across the entire study area?\nTo calculate the percentile rank of the mean temperature per spatial unit, in my first attempt at this I downloaded my data from GEE and ran it in R….I then consulted Ollie Ballinger who helped me re-create the case_when() that you see below from R…First the R attempt…\n\n8.8.0.1 R\nFirst make a quick map of our mean temperature per spatial unit, in my case wards…\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\ntemp <- st_read(here::here(\"prac_8\",\n                                  \"mean_Landsat_ward.shp\"))\n\nReading layer `mean_Landsat_ward' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\CASA0023\\CASA0023-book\\prac_8\\mean_Landsat_ward.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 622 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -0.5103773 ymin: 51.28676 xmax: 0.3340128 ymax: 51.69187\nGeodetic CRS:  WGS 84\n\ntmap_mode(\"plot\")\n# plot each map\ntm1 <- tm_shape(temp) + \n  tm_polygons(\"mean\", \n             # breaks=breaks,\n              palette=\"Reds\")+\n  tm_legend(show=TRUE)+\n  tm_layout(frame=FALSE)\n\ntm1\n\n\n\n\nThen for percentile rank…the percentile code is from stackexchange\nNote, that percentile rank is “percentage of scores within a norm group that is lower than the score you have”….if you are in the 25% then 25% of values are below that value and it is the 25 rank…\n\nperc.rank <- function(x) trunc(rank(x))/length(x)\n\npercentile <- temp %>% \n  mutate(percentile1 = perc.rank(mean))\n\nNow assign the classes…\n\nperc.rank <- function(x) trunc(rank(x))/length(x)\n\n\npercentile <- temp %>% \n  mutate(percentile1 = (perc.rank(mean))*100)%>%\n    mutate(Level = case_when(between(percentile1, 0, 39.99)~ \"1\",\n                           between(percentile1, 40, 69.99)~ \"2\",\n                           between(percentile1, 70, 89.99)~ \"3\",\n                           between(percentile1, 90, 98.99)~ \"4\",\n                           between(percentile1, 99, 100)~ \"5\"))\n\nTo understand this a bit further check out the percentiles of the breaks we set…a percentile is..” > a statistical measure that indicates the value below which a percentage of data falls, Rbloggers\nRun the code below then open your percentile data above, you should see that the ranks correspond to the percentiles set…\n\ntest<-quantile(temp$mean, c(0.39999, 0.69999, 0.8999, 0.98999, 1.0))\n\ntest\n\n 39.999%  69.999%   89.99%  98.999%     100% \n38.76971 39.87305 40.75850 41.60319 42.37533 \n\n\nIn my case a temperature of 39.87138 is set to level 2, whilst a temperature of 39.87861 is set to level 3…\nMap the heat index….\n\n# plot each map\ntm1 <- tm_shape(percentile) + \n  tm_polygons(\"Level\", \n             # breaks=breaks,\n              palette=\"Reds\")+\n  tm_legend(show=TRUE)+\n  tm_layout(frame=FALSE)\n\ntm1\n\n\n\n\nInteractive…\n\nlibrary(terra)\ntemp_rast <- rast(\"prac_8/subtracted_mean_HI.tif\")\n\ntmap_mode(\"view\")\n\ninteractive <- tm_shape(temp_rast) +\n  tm_raster(\"subtracted_mean_HI.tif\", \n            palette = \"Reds\") + \ntm_shape(percentile) + \n  tm_polygons(\"Level\", \n             # breaks=breaks,\n              palette=\"Reds\",\n              fillOpacity = 0.7,\n)+\n  tm_legend(show=TRUE)+\n  tm_layout(frame=FALSE)\n\ninteractive\n\n\n\n\n\n\n\n8.8.0.2 GEE\nThis is very similar (but slightly different) in GEE….\nFirst of all we need to get the percentiles of the mean column…\n\nvar percentiles = mean_Landsat_ward.reduceColumns({\n  reducer: ee.Reducer.percentile([10, 20, 30, 40, 50, 60, 70, 80, 90, 99, 100]),\n  selectors: ['mean']\n});\n\nprint(percentiles, \"percentile\") ///gives the overall percentile then need to map\n\nNext, pull out these values are numbers….\n\nvar p40 = ee.Number(percentiles.get('p40'))\nvar p70 = ee.Number(percentiles.get('p70'))\nvar p90 = ee.Number(percentiles.get('p90'))\nvar p99 = ee.Number(percentiles.get('p99'))\nvar p100 = ee.Number(percentiles.get('p100'))\n\nprint(p40, \"p40\")// check one\n\nNow, do the case_when() equivalent ….it’s a lot of code!\n\nvar resub= mean_Landsat_ward.map(function(feat){\n  return ee.Algorithms.If(ee.Number(feat.get('mean')).lt(p40).and(ee.Number(feat.get('mean')).gt(0)),\n  feat.set({'percentile_cat': 1}),\n  ee.Algorithms.If(ee.Number(feat.get('mean')).gt(p40).and(ee.Number(feat.get('mean')).lt(p70)),\n  feat.set({'percentile_cat': 2}),\n  ee.Algorithms.If(ee.Number(feat.get('mean')).gt(p70).and(ee.Number(feat.get('mean')).lt(p90)),\n  feat.set({'percentile_cat': 3}),\n  ee.Algorithms.If(ee.Number(feat.get('mean')).gt(p90).and(ee.Number(feat.get('mean')).lt(p99)),\n  feat.set({'percentile_cat': 4}),\n  ee.Algorithms.If(ee.Number(feat.get('mean')).gt(p99).and(ee.Number(feat.get('mean')).lte(p100)),\n  feat.set({'percentile_cat': 5}),  \n  feat.set({'percentile_cat': 0}))\n))))})\n\nNext up is setting some visualization parameters, i got the CSS colour codes (HEX codes) from colorBrewer\n\nvar visParams_vec = {\n    'palette': ['#fee5d9', '#fcae91', '#fb6a4a', '#de2d26', '#a50f15'],\n    'min': 0.0,\n    'max': 5,\n    'opacity': 0.8,\n}\n\nNow, we make an image from out our column in the vector data….\n\nvar image = ee.Image().float().paint(resub, 'percentile_cat')\n\nFinally, we map it….\n\nMap.addLayer(image, visParams_vec, 'Percentile_cat')\n\n\n\n\n\n\n\n\n\nAs as side note here, it we want to add columns in GEE like we would in R using mutate() or a field calculator in GIS software we use something like this…\n\n//example of field calculator or mutate in R...\n\nvar collectionWithCount = mean_Landsat_ward.map(function (feature) {\n  ///make a new column\n  return feature.set('percentage',\n  // take the mean column and * 100                   \n  feature.getNumber('mean').multiply(100));\n});\n\nNow we have the temperature per ward (or the index), we could explore factors of the built environment that might contribute to that temperature….such as…buildings, lack of vegetation, lack of water areas and so on. This could include some more analysis in GEE such as NDVI, building area per spatial unit or other data from local authorities…"
  },
  {
    "objectID": "8_temperature.html#considerations",
    "href": "8_temperature.html#considerations",
    "title": "\n8  Temperature\n",
    "section": "\n8.9 Considerations",
    "text": "8.9 Considerations\n\nGoogle Earth Engine: Analyzing Land Surface Temperature Data\nFollowing the lecture, could red lined areas experience higher temperatures?\nCould a similar analysis be done for other variables? Such as pollution\nCould we make a pollution index similar to the heat index\nCould we make a flood index or explore flooding over time in cities / determine areas that need remediation"
  },
  {
    "objectID": "8_temperature.html#learning-diary",
    "href": "8_temperature.html#learning-diary",
    "title": "\n8  Temperature\n",
    "section": "\n8.10 Learning diary",
    "text": "8.10 Learning diary\nConsult the assignment requirements document and complete your learning diary entry in your Quarto learning diary."
  },
  {
    "objectID": "8_temperature.html#feedback",
    "href": "8_temperature.html#feedback",
    "title": "\n8  Temperature\n",
    "section": "\n8.11 Feedback",
    "text": "8.11 Feedback\nWas anything that we explained unclear this week or was something really clear…let us know using the feedback form. It’s anonymous and we’ll use the responses to clear any issues up in the future / adapt the material."
  },
  {
    "objectID": "9_SAR.html#resources",
    "href": "9_SAR.html#resources",
    "title": "9  SAR",
    "section": "9.1 Resources",
    "text": "9.1 Resources\n\n\n\n\n\n\nThis week\n\n\n\n\nSchulte to Bühne, H., Pettorelli, N., 2018. Better together: Integrating and fusing multispectral and radar satellite imagery to inform biodiversity monitoring, ecological research and conservation science. Methods in Ecology and Evolution 9, 849–865.\n\nSections 1 and 2.\n\nLu, D., Li, G., Moran, E., Dutra, L., Batistella, M., 2011. A Comparison of Multisensor Integration Methods for Land Cover Classification in the Brazilian Amazon. GIScience & Remote Sensing 48, 345–370.\n\nUpto page 354.\n\nDabboor, M., Brisco, B., Dabboor, M., Brisco, B., 2018. Wetland Monitoring and Mapping Using Synthetic Aperture Radar, Wetlands Management - Assessing Risk and Sustainable Solutions. IntechOpen.\n\nJust section 2 on SAR concepts.\n\nASF SAR userguide\nNASA Data Made Easy: Part 2- Introduction to SAR"
  },
  {
    "objectID": "9_SAR.html#learning-diary",
    "href": "9_SAR.html#learning-diary",
    "title": "9  SAR",
    "section": "9.2 Learning diary",
    "text": "9.2 Learning diary\nConsult the assignment requirements document and complete your learning diary entry in your Quarto learning diary."
  },
  {
    "objectID": "9_SAR.html#feedback",
    "href": "9_SAR.html#feedback",
    "title": "9  SAR",
    "section": "9.3 Feedback",
    "text": "9.3 Feedback\nWas anything that we explained unclear this week or was something really clear…let us know using the feedback form. It’s anonymous and we’ll use the responses to clear any issues up in the future / adapt the material."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "10  Resources",
    "section": "",
    "text": "https://github.com/robmarkcole/satellite-image-deep-learning#techniques\nhttps://urbanspatial.github.io/classifying_satellite_imagery_in_R/#Supervised_Classification\nhttps://opengislab.com/blog/2018/5/14/flood-mapping-with-sentinel-1-data-using-snap-and-qgis"
  }
]