[
  {
    "objectID": "3_corrections.html#atmosphereic-correction",
    "href": "3_corrections.html#atmosphereic-correction",
    "title": "3  Corrections",
    "section": "3.1 Atmosphereic correction",
    "text": "3.1 Atmosphereic correction\n\n3.1.1 DOS\nAs described in the lecture, a rather simple method to correct raw satellite (or any other) imagery is called Dark Object Subtraction (DOS). This uses the logic that the darkest pixel within the image should be 0 and therefore any value that it has can be attributed to the atmosphere.\nSo in order to remove the effect of the atmosphere we can the subtract the value from the rest of the pixels within the image.\nHere, we will need to download some raw satellite imagery, that is in Digital Number (DN).\nThen we will apply the formula to calculate the reluctance of the pixels by removing atmosphere effects.The formula for the cosine of the solar zenith angle correction (COST) is, this is the same as DOS but DOS omits TAUz. The following has made use of the documentation from GIS Ag Maps.com\n\\[\\rho_{\\lambda}= \\frac{(Lsat_{rad} - Lhaze1percent_{rad})\\pi * d^2}{EO_{\\lambda} * cos\\theta S * TUAv + TAUz}\\]\nWhere…\n\n\\(\\rho_{\\lambda}\\) is the corrected value (DOS applied)\n\nTop line of equation…\n\n\\(Lsat_{rad}\\) = at sensor radiance (recall DN goes to radiance through the regression equation we saw!)\n\\(Lhaze1percent_{rad}\\) = amount of radiance that is due to the atmosphere (atmospheric haze) from path or scatter radiance. Very few surfaces are completely black so it is assume that the darkest surface has a 1% reflectance. Various methods to caclcualte this…\n\nLook up tables\nSelecting the darkest pixels (shadow, water)\n\n\nWhen we have the haze amount then deduct 1% from that value per band as few targets are absolutely black.\nFor COST this this:\n\\[ 0.01 reflectance = 0.01 *\\frac{Eoλ * cosθs^2} {d² * pi}\\]\nFor DOS it’s\n\\[ 0.01 reflectance = 0.01 *\\frac{Eoλ * cosθs} {d² * pi}\\]\n\n\\(EO_{\\lambda}\\) or \\(ESUN_{\\lambda}\\) = mean exoatmospheric irradiance\n\nirradiance = power per unit area received from the Sun\nexoatmospheric = just outside the Earth’s atmosphere\nThese values are available from the Landsat user manual such as table 5.3 in the Landsat 7 user guide\n\n\\(cosθs\\) = cosine of the solar azimuth, remember from the lecture that this is 90 - solar elevation.\n\\(d\\) = the Earth-sun distance and is in the .MTL file\n\\(pi\\) = 3.14159265\n\nOnce we have all these values then we can do the following:\n\nCompute the haze value for each band (although not beyond NIR) - this is the amount of radiance that is due to the atmosphere (atmospheric haze), see above methods.\nConvert DN to radiance\nCompute the 1% reflectance value using the equations above\nSubtract the 1% reflectance value from the radiance. Here we are saying that we have a pixel (e.g. darkest pixel), we know what 1% of the total radiance is and we are subtracting that from the darkest pixel (which still has atmospheric effects) to account for most targets not being completely black.\n\nWe can now plug the values in:\n\\[\\rho_{\\lambda}= \\frac{(Lsat_{rad} - Lhaze1percent_{rad})\\pi * d^2}{EO_{\\lambda} * cos\\theta S * TUAv + TAUz}\\]\nWhere the \\(Lhaze1percent_{rad}\\) is the haze value (e.g. darkest pixel) minus 1% of the total radiance. This 1% was computed from the equations above.\nWithin this equation:\n\\(TAUv\\) = 1.0 for Landsat and \\(TAUz\\) = cosθs for COST method. DOS is the same, but without \\(TAUz\\)\nOf course we can do this in R (or any other software) with just one function! First we need to download some raw satellite data that comes in the Digital Number (DN) format. This is the exact same process as we saw in week 1, expect this time select the Collection 1, Level-1 bundle. At the moment this process won’t work with Landsat 9. However, as this involves a large amount of data and it’s unlikely you will need to do this in the module of data read through the following code and then move to the next section\n\nlibrary(terra)\nlibrary(raster)\nlibrary(RStoolbox)\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(rgdal)\n\n## Import meta-data and bands based on MTL file\nmtlFile  <- (\"prac_3/Lsatdata8/LC08_L1TP_175083_20211005_20211013_01_T1_MTL.txt\")\n                        \nmetaData <- readMeta(mtlFile)\n\nlsatMeta  <- stackMeta(metaData)\n\n# surface reflectance with DOS\n\nl8_boa_ref <- radCor(lsatMeta, metaData, method = \"dos\")\n\n#terra::writeRaster(l8_boa_ref, datatype=\"FLT4S\", filename = \"prac_3/Lsatdata8/l8_boa_ref.tif\", format = \"GTiff\", overwrite=TRUE)\n\n# Radiance \n\nlsat_rad <- radCor(lsatMeta, metaData = metaData, method = \"rad\")\n\n#terra::writeRaster(lsat_rad, datatype=\"FLT4S\", filename = \"prac_3/Lsatdata8/lsat_rad.tif\", format = \"GTiff\", overwrite=TRUE)\n\n\nhazeDN    <- RStoolbox::estimateHaze(lsat, hazeBands = 2:4, darkProp = 0.01, plot = TRUE)\n\nlsat_sref <- radCor(lsatMeta, metaData = metaData, method = \"dos\", \n                    hazeValues = hazeDN, hazeBands = 2:4)\n\nhttps://rpubs.com/delViento/atm_corr\n\n\n3.1.2 Radiance (or DN) to Reflectance\nAs noted in the lecture there are a wide range of more sophisticated methods (beyond Dark Object Subtraction) to convert raw Digital Numbers or radiance to surface reflectance.\nWhilst this is a bit beyond the scope of this module, if you look again at the Landsat 7 Data Users Handbook you will see a radiance to reflectance calculation that can be used…“For relatively clear Landsat scenes”:\n\\[\\rho_{\\rho}= \\frac{\\pi* L_{\\lambda} * d ^2}{ESUN_{\\lambda} * cos\\theta_S}\\] and…we’ve seen all these values in the DOS formula, except \\(\\rho_{\\rho}\\) which is Unitless planetary reflectance\n…this method is still used in current research too, for example it’s listed in this paper of Land Surface Temperature retrieval by Sekertekin and Bonafonu, 2020. See Appendix C for Landsat 5, 7 (that use the above equation) and Landsat 8, that uses this slight variation…\n\\[\\rho_{\\rho}= \\frac{M_{p}* Q_{CAL} + A_p}{sin\\theta_{SE}}\\] Where…\n\n\\(M_p\\) is the band-specific multiplicative rescaling factor from the metadata\n\\(A_p\\) is the band-specific additive rescaling factor from the metadata\n\\(QCAL\\) is the digital number\n\\(\\theta_{SE}\\) is the sun elevation angle from the metadata file.\n\nAlthough it’s worth noting that this is Top of Atmosphere reflectance (TOA).\nRadiance is how much light the sensor sees.\nReflectance is the ratio of light leaving the target to amount striking the target. Here will still have atmopsheric effects in the way of our true apparent reflectance. Confusingly all of these can be termed reflectance and indeed sometimes radiance is referred to as reflectance.\nTOA reflectance changes the data from what the sensor sees to the ratio of light leaving compared to striking it. BUT, the atmosphere is still present. If we remove the atmopshere we have apparent reflectance (sometimes called Bottom of Atmosphere reflectance). DOS gives us a version of apparent reflectance."
  },
  {
    "objectID": "3_corrections.html#accessing-data",
    "href": "3_corrections.html#accessing-data",
    "title": "\n3  Corrections\n",
    "section": "\n3.2 Accessing data",
    "text": "3.2 Accessing data\nOk, so we can deal with a single image, but what happens when an image doesn’t cover your entire study area. We must select two images are mosaic (or merge) them together. Landsat data (and most satellite data) is collected using some form of Worldwide Reference System. This splits the data based on PATH (columns) and ROWS.\n\n\n\n\nSource: Samuel Akande\n\n\n\n\nBefore we start with merging we need to download two satellite tiles to merge together. The problem is that Landsat tiles won’t align with administration boundraies (or they might if you are lucky). For my example city of Cape Town i need at least two, possible three Landsat tiles to be merged together. In USGS Earth Explorer you can upload a shapefile or KML so you can search for tiles to cover the area. However, this is rather slow and the shapefile must only contain one polygon (cape town includes an island too). You can, however, draw a boundary to search within:\n\n\n\n\n\n\n\n\nIn my example i am going to select two tiles, i know which two to select from looking at the GADM boundary for Cape Town.\nWhen doing this:\n\nSelect two images as temporally close to each other as possible\nTry and make sure there is no (or very little cloud cover)\nIn this case select Landsat Collection 2 level 2 to get surface reflectance.\n\nNotes on Landsat Collections. The different collections denote a major difference in processing of the data. Where as the levels denote a specific product. There is no clear guide online that explains this, so be careful when reading papers!\nFor example…\n\nA primary characteristic of Collection 2 is the substantial improvement in the absolute geolocation accuracy of the global ground reference dataset used in the Landsat Level-1 processing flow\n\nCollection 2 was released in 2020 and has some updates, see this summary of differences for collection 1 vs collection 2. The collection 2 auxiliary data can also be downloaded from Earth Explorer.\nWhereas levels will provide:\n\nLevel 1 is delivered as a Digital Number\nLevel 2 includes surface reflectance and surface temperature\nLevel 3 science products are specific products generated from the data such as Burned Area, surface water extent\nMost of the level datasets are tiered, this is denoted through the file name that might end with T1. Tiers are based on data quality and level of processing. Tier 1 datasets are the best quality, tier 2 are good but might have some cloud that affects radiometric calibration covering ground control points.\nThere is also a U.S. Analysis Read Dataset (ARD) that includes a bundle of data (Top of Atmosphere (TOA) reflectance, TOA Brightness Temperature, Surface Reflectance , Surface Temperature and Quality Assessment) in a specific US grid strucutre. This removes the need to process data between the difference stages for applications in the US.\n\nTo conclude, we have collections, followed by levels, followed by tiers.\n\n\n\n\nSource: USGS\n\n\n\n\nIn this case I want to download two tiles from Collection 2, Level 2. You can view the tiles and preview the data in Earth Explorer:\n\n\n\n\n\n\n\n\nLooking at the options below each tile icon in the search results you can either download the tiles individually or add them to a basked (it’s the box icon). After clicking the box on the products you want to download > click the basket (top right) > you will then be presented with this screen where you click start order and then follow the prompts to download multiple tiles at once."
  },
  {
    "objectID": "3_corrections.html#merging-imagery",
    "href": "3_corrections.html#merging-imagery",
    "title": "\n3  Corrections\n",
    "section": "\n3.3 Merging imagery",
    "text": "3.3 Merging imagery\nOpen our two tiles, in my case i have move my tiles into two separate folders. So i will do this twice, note that we could automate this if we had a large number of tiles. I have also downloaded one Landsat 8 tile and one Landsat 9 tile.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(terra)\n\nterra 1.5.34\n\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(fs)\n\n# List your raster files excluding band 8 using the patter argument\nlistlandsat_8<-dir_info(here::here(\"prac_3\", \"Landsat\", \"Lsat8\"))%>%\n  dplyr::filter(str_detect(path, \"[B123456790].TIF\")) %>%\n  dplyr::select(path)%>%\n  pull()%>%\n  as.character()%>%\n  # Load our raster layers into a stack\n  terra::rast()\n\nFor Landsat 9\n\n# List your raster files excluding band 8 using the patter argument\nlistlandsat_9<-dir_info(here::here(\"prac_3\", \"Landsat\", \"Lsat9\"))%>%\n  dplyr::filter(str_detect(path, \"[1B23456790].TIF\")) %>%\n  dplyr::select(path)%>%\n  pull()%>%\n  as.character()%>%\n  # Load our raster layers into a stack\n  terra::rast()\n\nThis might take about 2 minutes… ::: {.cell}\nm1 <- terra::mosaic(listlandsat_8, listlandsat_9, fun=\"mean\")\n:::"
  },
  {
    "objectID": "3_corrections.html#enhancements",
    "href": "3_corrections.html#enhancements",
    "title": "\n3  Corrections\n",
    "section": "\n3.4 Enhancements",
    "text": "3.4 Enhancements\nUsing our merged Landsat data we can now undertake some basic enhancements to try and emphasize / exaggerate certain features or spectral traits.\n\n3.4.1 Ratio\nRatioing is the difference between two spectral bands that have a certain spectral response meaning it is easier to identify a certain landscape feature…for example…\n\nThe Normalised Difference Vegetation Index is based on the fact that healthy and green vegetation reflects more in the NIR but absorbs in the Red wavelength\n\n\n\n\n\nSource: PhysicsOpenLab\n\n\n\n\nHere, we can visually see the spectral trait:\n\n\n\n\nSource: PhysicsOpenLab\n\n\n\n\nWe can leverage the fact that healthy vegetation has this spectral trait and use the NDVI index should we wish to highlight areas with healthy vegetation.\n\\[NDVI= \\frac{NIR-Red}{NIR+Red}\\]\nIn R this would be:\n\nm1_NDVI <- (m1$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B5 - m1$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B4 ) / (m1$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B5 + m1$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B4)\n\nm1_NDVI %>%\n  plot(.)\n\nWe can reclassify this to pull out certain areas, for example, only where NDVI is equal to or greater than 0.2\n\nveg <- m1_NDVI %>%\n  terra::classify(., cbind(-Inf, 0.2, NA))\n\nveg %>%\n  plot(.)\n\n\n\n\nThere are many other ratios, all of which are detailed on the Index Database and most follow the same formula. For example, the Normalized Difference Moisture Index (NDMI):\nFor Landsat sensors 4-7:\n\\[NDMI= \\frac{Band 4-Band 5}{Band 4 + Band 5}\\]\nFor Landsat 8, bands are increased by 1.\n\n3.4.2 Filtering\nFiltering refers to any kind of moving window operation to our data which can be saved as a separate raster file. As we saw in the lecture this can include low or high pass filters.\n\n\n\n\nSource: Introduction to Geographic Information Systems in Forest Resources\n\n\n\n\n\n# for a 3 by 3 filter on 1 band\nm1_filter <- terra::focal(m1$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B4, w=matrix(1/9,nrow=3,ncol=3))\n\n\n3.4.3 Texture\nThe basics of texture were covered in the lecture. To apply texture analysis to data in R, we can use the glcm package which has a selection of eight texture measures, and we can apply these per band…for example…Note, to use this you must have RTools installed as it makes use of the C++ language.\nBelow we can specify:\n\nthe size of the moving window\nthe shift for co-occurrency (or second order) as seen in the lecture. If multiple shifts are supplied, glcm will calculate each texture statistic using all the specified shifts and return the mean value of the texture for each pixel\nthe measures we want, for full equations see Texture Metrics Background%20of%20the%20occurrence%20values.&text=It%20normalizes%20the%20occurrence%20values,(9%20in%20this%20example).)\n\nNote, that we commonly don’t use the pancrhomatic band in landcover classification, however as it is 15m it can produce some useful outputs.\nCurrently the glcm package only accepts raster layers from the raster package so we first need to convert this to a raster layer…this will take 7-10 minutes…increasing the window size and selecting less statistics should speed this up.\n\nlibrary(glcm)\nlibrary(raster)\n\nband4_raster<-raster::raster(m1$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B4)\n\nglcm <- glcm(band4_raster,\n                   window = c(7, 7),\n                   #shift=list(c(0,1), c(1,1), c(1,0), c(1,-1)), \n                   statistics = c(\"homogeneity\"))\n\nglcm$glcm_homogeneity %>%\n  plot(.)\n\nDepending on your study area the texture measures might not show much, but in this example from Lu et al. 2012 what does it highlight or make more prominent.\n\n\n\n\nSource: Land use/cover classification in the Brazilian Amazon using satellite images\n\n\n\n\n\n3.4.4 Data fusion\nIn the simplest form data fusion is appending new raster data to the existing data or making a new raster dataset with different bands…here we can do this with the texture measure we have created (and the original spectral data if you wish). We are getting to the stage now where remote sensing is a merge of science and art. Specifically the science is how we correct and apply methods, the art is about how we select the right data / transform it / alter it to produce an output. There is never a completely right answer as we will see in future practicals.\nTo create decision level fusion we append our new datasets to our existing data…\n\n# for the next step of PCA we need to keep this in a raster (and not terra) format...\nm1_raster <- stack(m1)\n\nFuse <- stack(m1_raster, glcm.red)\n\nRecall from the lecture there is also object fusion and image fusion.\n\n3.4.5 PCA\nPrincipal Component Analysis is designed to reduce the dimensionality of our data. In this case we might want to scale our data, meaning that we can compare data that isn’t measured in the same way (as we have spectral and texture data). To do so we can use the function scale, that by default standarize the data by subtracting the mean and dividing by the standard deviation. If we just wanted to subtract the mean we could set scale=FALSE as an argument.\nWhen we use rasterPCA() we can set the number of samples nsamples to be used for PCA which are then applied to the rest of the data to try and make this more efficient…\nAlso note that spca=TRUE which corresponds to centered and scaled input data\n\nlibrary(RStoolbox)\n\nFuse_3_bands <- stack(Fuse$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B4, Fuse$LC08_L2SP_175083_20220501_20220504_02_T1_SR_B5, Fuse$glcm_homogeneity)\n\nscale_fuse<-scale(Fuse_3_bands)\n\npca <- rasterPCA(Fuse, \n                 nSamples =100,\n                 spca = TRUE)\n\nOnce the rasterPCA() has completed run the following code to get the proportion of variance explained by each PCA component and the cumulative proportion of the variance explained. Remember that PCA is trying to:\n\nTransform multi-spectral data into uncorrelated and smaller dataset\nKeep most of the original information\nThe first component will (should) capture most of the variance within the dataset\n\n\nsummary(pca$model)\n\nIn my case component 1 (of 10) explains 77.62% of the variance from the entire dataset…how might this be useful in future analysis?\nFinally, to plot one of the PCA bands….\n\nplot(pca$map$PC1)\n\n\n3.4.6 Review questions\nFor the task this week select one concept from the lecture to explore in greater detail (e.g. texture analysis to remotely sensed images).\nWrite 2-3 paragraphs supporting your responses with plots and literature:\n\nExplain the concept (e.g. what texture analysis is)\nDetail how it has been applied in 2 studies\nReflect on the applicability / usefulness\n\n3.4.7 Useful blogs\n\nTexture and PCA in R"
  },
  {
    "objectID": "4_policy.html#before-the-practical-session",
    "href": "4_policy.html#before-the-practical-session",
    "title": "4  Policy",
    "section": "4.1 Before the practical session",
    "text": "4.1 Before the practical session\nFor the practical session this week come prepared to talk about the following:\n\nSearch for one metropolitan policy challenge (any city in the World) that could be solved by incorporating remotely sensed data\nIdentify and evaluate a remotely sensed data set that could used to assist with contributing to the policy goal\nDemonstrate how this links to global agendas / goals\nExplain how it advances current local, national or global approaches.\n\nCities will have a diverse range of documentation available…"
  },
  {
    "objectID": "4_policy.html#after-the-practical-session",
    "href": "4_policy.html#after-the-practical-session",
    "title": "4  Policy",
    "section": "4.2 After the practical session",
    "text": "4.2 After the practical session\nFollowing the practical and subsequent discussion write up your case study city in three paragraphs, you should:\n\nDetail the relevant policy that can be assisted with remotely sensed data\nEvaluate how remotely sensed data set(s) could used to assist with contributing to the policy goal\nPlace it within local / global agendas and current approaches\nCite the relevant policy and where appropriate literature.\n\nShould you struggle to find current approaches within your city explore other cities discussed within the practical."
  },
  {
    "objectID": "4_policy.html#policies",
    "href": "4_policy.html#policies",
    "title": "4  Policy",
    "section": "4.3 Policies",
    "text": "4.3 Policies\nA few examples to get you started …you are not limited to this list. We explore some of these in more detail within future lectures.\n\n4.3.1 Metropolitan\n\nCape Town Municipal Spatial Development Framework\nAhmedabad 2016 Heat Action Plan\nSouth Asia’s First Heat-Health Action Plan in Ahmedabad (Gujarat, India)\nOneNYC 2050\nLondon Plan\nPerth and Peel @ 3.5 million\n\n\n\n4.3.2 National\n\nSingapore Master Plan\n\n\n\n4.3.3 International\n\nNEW!! World Cities Report 2022\nC40 Cities\nUnited Nations New Urban Agenda\nARUP City Resilience Framework\nUnited Nations International Strategy for Disaster Reduction Sendai Framework\nUniversal Sustainable Development Goals\n\nThe SDG targets\n\nBeat the Heat Handbook"
  },
  {
    "objectID": "4_policy.html#example",
    "href": "4_policy.html#example",
    "title": "4  Policy",
    "section": "4.4 Example",
    "text": "4.4 Example\nFor a written example read up to the study area section in my paper on temperature mitigation (first 2 pages). Take note of table 1."
  },
  {
    "objectID": "6_classification_II.html",
    "href": "6_classification_II.html",
    "title": "5  Classification II",
    "section": "",
    "text": "5.0.1 Review questions"
  },
  {
    "objectID": "7_temperature.html#learning-objectives",
    "href": "7_temperature.html#learning-objectives",
    "title": "6  Temperature",
    "section": "6.1 Learning objectives",
    "text": "6.1 Learning objectives\nBy the end of this practical you should be able to:\n\nExplain and execute appropriate pre-processing steps of raster data\nReplicate published methodologies using raster data\nDesign new R code to undertake further analysis\n\nThis week:\n\nAppendix “Raster operations in R” from Intro to GIS and Spatial Analysis by Gimond (2019)\nRaster manipulation from Spatial data science by Hijmans (2016). This last one is another tutorial — it seems there aren’t any decent free raster textbook chapters, let me know if you find one.\n\nRemember this is just a starting point, explore the reading list, practical and lecture for more ideas."
  },
  {
    "objectID": "7_temperature.html#introduction",
    "href": "7_temperature.html#introduction",
    "title": "6  Temperature",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nWithin this practical we are going to be using data from the Landsat satellite series provided for free by the United States Geological Survey (USGS) to replicate published methods. Landsat imagery is the longest free temporal image repository of consistent medium resolution data. It collects data at each point on Earth each every 16 days (temporal resolution) in a raster grid composed of 30 by 30 m cells (spatial resolution). Geographical analysis and concepts are becoming ever more entwined with remote sensing and Earth observation."
  },
  {
    "objectID": "7_temperature.html#data",
    "href": "7_temperature.html#data",
    "title": "6  Temperature",
    "section": "6.3 Data",
    "text": "6.3 Data\n\n6.3.1 Shapefile\nThe shapefile of Manchester is available from the data folder for this week on GitHub. To download this consult [How to download data and files from GitHub], i’d used Option 1.\n\n\n6.3.2 Raster data (Landsat)\nTo download the data it’s the same process we saw in week 1 on the USGS Earth Explorer website:\n\nEnter Manchester in the address/place box > select the country as United Kingdom > click Show and then click on the word Manchester in the box that appears. ::: {.cell layout-align=“center” hash=‘7_temperature_cache/html/unnamed-chunk-1_cb5a647eaa64b52fe7b26e86019bf6b4’} ::: {.cell-output-display}  ::: :::\nSelect the date range between the 12/5/2019 and 14/5/2019 — it’s a US website so check the dates are correct.\nClick dataset and select Landsat, then Landsat Collection 1 Level-1, check Landsat 8 (level 2 is surface reflectance — see [Remote sensing background (optional)]\nClick results, there should be one image (GEOTiff), download it..it might take a while\nLandsat data comes zipped twice as a .tar.gz. Use 7Zip or another file extractor, extract it once to get to a .tar then extract again and files should appear. Or the code below will also let you extract Landsat data…\n\n\n6.3.2.1 Alternative raster data\nOccasionally the earth explorer website can go down for maintenance or during government shutdowns. If possible I strongly advise you to learn how to use its interface as multiple other data providers have similar interfaces. GitHub also place a strict size limit on files of 100MB. However, in order to account for situations like this I’ve placed the zipped file on GoogleDrive and will demonstrate how to access this from R using the new googledrive package.\nThis could be a great option for you to gain reproducibility points if you have large files that you can’t upload to GitHub.\nIn GoogleDrive you need to ensure your file is shareable with others — right click on it > Share > then copy the link. I have done this for my file in the example below, but if you try and replicate this, make sure you’ve done it otherwise it might not work when other people try and run your code, as they won’t have access to the file on your GoogleDrive.\nDepending on your internet speed this example might take some time…\nBe sure to change the path to your practical 7 folder but make sure you include the filename within it and set overwrite to T (or TRUE) if you are going to run this again. ::: {.cell hash=‘7_temperature_cache/html/unnamed-chunk-2_69162eac4836a4c80fe0349f6dce5bbe’}\nlibrary(\"googledrive\")\n\no<-drive_download(\"https://drive.google.com/open?id=1MV7ym_LW3Pz3MxHrk-qErN1c_nR0NWXy\",\n                  path=\"prac_7/exampleGoogleDrivedata/LC08_L1TP_203023_20190513_20190521_01_T1.tar.gz\", \n                  overwrite=T)\n:::\nNext we need to uncompress and unzip the file with untar(), first list the files that end in the extension .gz then pass that to untar with the pipe %>% remember this basically means after this function… then…do this other function with that data\n\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(stringr)\nlibrary(utils)\n\nlistfiles<-dir_info(here::here(\"prac_7\", \"exampleGoogleDrivedata\")) %>%\n  dplyr::filter(str_detect(path, \".gz\")) %>%\n  dplyr::select(path)%>%\n  dplyr::pull()%>%\n  #print out the .gz file\n  print()%>%\n  as.character()%>%\n  utils::untar(exdir=here::here(\"prac_7\", \"exampleGoogleDrivedata\"))"
  },
  {
    "objectID": "7_temperature.html#processing-raster-data",
    "href": "7_temperature.html#processing-raster-data",
    "title": "6  Temperature",
    "section": "6.4 Processing raster data",
    "text": "6.4 Processing raster data\n\n6.4.1 Loading\nToday, we are going to be using a Landsat 8 raster of Manchester. The vector shape file for Manchester has been taken from an ESRI repository.\n\nLet’s load the majority of packages we will need here.\n\n\n## listing all possible libraries that all presenters may need following each practical\nlibrary(sp)\n#library(raster)\nlibrary(rgeos)\nlibrary(rgdal)\nlibrary(rasterVis)\nlibrary(ggplot2)\nlibrary(terra)\nlibrary(sf)\n\n\nNow let’s list all our Landsat bands except band 8 (band 8 is the panchromatic band and we don’t need it here) along with our study area shapefile. Each band is a separate .TIF file.\n\n\nlibrary(stringr)\nlibrary(raster)\nlibrary(fs)\nlibrary(sf)\nlibrary(tidyverse)\n\n# List your raster files excluding band 8 using the patter argument\nlistlandsat<-dir_info(here::here(\"prac_7\", \"Lsatdata\"))%>%\n  dplyr::filter(str_detect(path, \"[B123456790].TIF\")) %>%\n  dplyr::select(path)%>%\n  pull()%>%\n  as.character()%>%\n  # Load our raster layers into a stack\n  raster::stack()\n\n# Load the manchester boundary\nmanchester_boundary <- st_read(here::here(\"prac_7\", \n                                          \"Manchester_boundary\",\n                                          \"manchester_boundary.shp\"))\n\nReading layer `manchester_boundary' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\CASA0023\\CASA0023-book\\prac_7\\Manchester_boundary\\manchester_boundary.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 540926.6 ymin: 5917117 xmax: 558663.4 ymax: 5933117\nProjected CRS: WGS 84 / UTM zone 30N\n\n#check they have the same Coordinate Reference System (CRS)\ncrs(manchester_boundary)\n\nCoordinate Reference System:\nDeprecated Proj.4 representation:\n +proj=utm +zone=30 +datum=WGS84 +units=m +no_defs \nWKT2 2019 representation:\nPROJCRS[\"WGS 84 / UTM zone 30N\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 30N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-3,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    ID[\"EPSG\",32630]] \n\ncrs(listlandsat)\n\nCoordinate Reference System:\nDeprecated Proj.4 representation:\n +proj=utm +zone=30 +datum=WGS84 +units=m +no_defs \nWKT2 2019 representation:\nPROJCRS[\"WGS 84 / UTM zone 30N\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 30N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-3,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 6°W and 0°W, northern hemisphere between equator and 84°N, onshore and offshore. Algeria. Burkina Faso. Côte' Ivoire (Ivory Coast). Faroe Islands - offshore. France. Ghana. Gibraltar. Ireland - offshore Irish Sea. Mali. Mauritania. Morocco. Spain. United Kingdom (UK).\"],\n        BBOX[0,-6,84,0]],\n    ID[\"EPSG\",32630]] \n\n\n\n\n6.4.2 Clipping\n\nOur raster is currently the size of the scene which satellite data is distributed in, to clip it to our study area it’s best to first crop it to the extent of the shapefile and then mask it as we have done in previous practicals…\n\n\nlsatmask <- listlandsat %>%\n  # now crop our temp data to the extent\n  terra::crop(.,manchester_boundary)%>%\n  terra::mask(.,  manchester_boundary)\n\n\nIf all we wanted to do was clip our data, we could now change our filenames in the raster stack and write the .TIFF files out again…\n\n\n# add mask to the filenames within the raster stack\n\nnames(lsatmask) <- names(lsatmask)%>%\n  str_c(., \n        \"mask\", \n        sep=\"_\")\n\n# I need to write mine out in another location\noutputfilenames <-\n  str_c(\"prac_7/Lsatdata/\", \"mask/\", names(lsatmask) ,sep=\"\")\n\nIn the first line of code i’m taking the original names of the raster layers and adding “mask” to the end of them. This is done using str_c() from the stringr package and the arguments\n\nnames(lsatmask): original raster layer names\n\"mask\": what i want to add to the names\nsep=\"\": how the names and “mask” should be seperated — “” means no spaces\n\nAs i can’t upload my Landsat files to GitHub i’m storing them in a folder that is not linked (remember this is all sotred on GitHub) – so you won’t find prac8_data/Lsatdata on there. If you want to store your clipped Landsat files in your project directory just use:\n\nlsatmask %>%\n  terra::writeRaster(., names(lsatmask), \n              bylayer=TRUE, \n              format='raster', \n              overwrite=TRUE)\n\nFor me though it’s:\n\nlsatmask %>%\n  terra::writeRaster(., outputfilenames, \n              bylayer=TRUE, \n              format='raster', \n              overwrite=TRUE)\n\nHere i write out each raster layer individually though specifying bylayer=TRUE.You can either use the format=GTiffor the native raster format from the raster package - format='raster' it doesn’t really matter as all GIS software can read all types."
  },
  {
    "objectID": "7_temperature.html#data-exploration",
    "href": "7_temperature.html#data-exploration",
    "title": "6  Temperature",
    "section": "6.5 Data exploration",
    "text": "6.5 Data exploration\n\n6.5.1 More loading and manipulating\n\nFor the next stage of analysis we are only interested in bands 1-7, we can either load them back in from the files we just saved or take them directly from the original raster stack.\n\n\n# either read them back in from the saved file:\n\nmanc_files<-dir_info(here::here(\"prac_7\", \"Lsatdata\", \"mask\")) %>%\n  dplyr::filter(str_detect(path, \"[B1234567]_mask.grd\")) %>%\n  dplyr::filter(str_detect(path, \"B11\", negate=TRUE))%>%\n  dplyr::select(path)%>%\n  pull()%>%\n  stack()\n\n# or extract them from the original stack\nmanc<-stack(lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B1_mask,\n                   lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B2_mask,\n                   lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B3_mask,\n                   lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B4_mask,\n                   lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B5_mask,\n                   lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B6_mask,\n                   lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B7_mask)\n\n# Name the Bands based on where they sample the electromagentic spectrum\nnames(manc) <- c('ultra-blue', 'blue', 'green', 'red', 'NIR', 'SWIR1', 'SWIR2') \n\n\nIf you want to extract specific information from a raster stack use:\n\n\ncrs(manc) # projection\nextent(manc) # extent\nncell(manc) # number of cells\ndim(manc) # number of rows, columns, layers\nnlayers(manc) # number of layers\nres(manc) # xres, yres\n\n\n\n6.5.2 Plotting data\n\nLet’s actually have a look at our raster data, first in true colour (how humans see the world) and then false colour composites (using any other bands but not the combination of red, green and blue).\n\n\n# true colour composite\nmanc_rgb <- stack(manc$red, manc$green, manc$blue)\n# false colour composite\nmanc_false <- stack(manc$NIR, manc$red, manc$green)\n\nmanc_rgb %>%\n  plotRGB(.,axes=TRUE, stretch=\"lin\")\n\n\n\nmanc_false %>%\n    plotRGB(.,axes=TRUE, stretch=\"lin\")\n\n\n\n\n\n\n6.5.3 Data similarity\n\nWhat if you wanted to look at signle bands and also check the similarity between bands? ::: {.cell hash=‘7_temperature_cache/html/unnamed-chunk-13_d1d711231fce66b6d795c2621f8e76da’}\n\n# Looking at single bands\nplot(manc$SWIR2)\n\n\n\n## How are these bands different?\n#set the plot window size (2 by 2)\npar(mfrow = c(2,2))\n#plot the bands\nplot(manc$blue, main = \"Blue\")\nplot(manc$green, main = \"Green\")\nplot(manc$red, main = \"Red\")\nplot(manc$NIR, main = \"NIR\")\n\n\n\n## Look at the stats of these bands\npairs(manc[[1:7]])\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n:::\nLow statistical significance means that the bands are sufficiently different enough in their wavelength reflectance to show different things in the image. We can also make this look a bit nicer with ggplot2 and GGally\n\nlibrary(ggplot2)\nlibrary(GGally)\n\nmanc %>%\n  terra::as.data.frame(., na.rm=TRUE)%>%\n  dplyr::sample_n(., 100)%>%\n  ggpairs(.,axisLabels=\"none\")\n\n\n\n\nYou can do much more using GGally have a look at the great documentation"
  },
  {
    "objectID": "7_temperature.html#raster-calculations",
    "href": "7_temperature.html#raster-calculations",
    "title": "6  Temperature",
    "section": "6.6 Raster calculations",
    "text": "6.6 Raster calculations\nNow we will move on to raster analysis in order to compute temperature from this raster data. To do so we need to generate additional raster layers, the first of which is NDVI\n\n6.6.1 NDVI\nLive green vegetation can be represented with the NIR and Red Bands through the normalised difference vegetation index (NDVI) as chlorophyll reflects in the NIR wavelength, but absorbs in the Red wavelength.\n\\[NDVI= \\frac{NIR-Red}{NIR+Red}\\]\n\n\n6.6.2 NDVI function\nOne of the great strengths of R is that is lets users define their own functions. Here we will practice writing a couple of basic functions to process some of the data we have been working with.\nOne of the benefits of a function is that it generalises some set of operations that can then be repeated over and again on different data… the structure of a function in R is given below:\n\nmyfunction <- function(arg1, arg2, ... ){\n  statements\n  return(object)\n}\n\nWe can use NDVI as an example…\n\nLet’s make a function called NDVIfun\n\n\nNDVIfun <- function(NIR, Red) {\n  NDVI <- (NIR - Red) / (NIR + Red)\n  return(NDVI)\n}\n\nHere we have said our function needs two arguments NIR and Red, the next line calculates NDVI based on the formula and returns it. To be able to use this function throughout our analysis either copy it into the console or make a new R script, save it in your project then call it within this code using the source() function e.g…\n\nsource('insert file name')\n\n\nTo use the function do so through…\n\n\nndvi <- NDVIfun(manc$NIR, manc$red)\n\nHere we call the function NDVIfun() and then provide the NIR and Red band.\n\nCheck the output\n\n\nndvi %>%\n  plot(.,col = rev(terrain.colors(10)), main = \"Landsat-NDVI\")\n\n\n\n# Let's look at the histogram for this dataset\nndvi %>%\n  hist(., breaks = 40, main = \"NDVI Histogram\", xlim = c(-.3,.8))\n\n\n\n\n\nWe can reclassify to the raster to show use what is most likely going to vegetation based on the histogram using the 3rd quartile — anything above the 3rd quartile we assume is vegetation.\n\n\nNote, this is an assumption for demonstration purposes, if you were to do something similar in future analysis be sure to provide reasoning with linkage to literature (e.g. policy or academic)\n\n\nveg <- ndvi %>%\n  reclassify(., cbind(-Inf, 0.3, NA))\n\nveg %>%\n  plot(.,main = 'Possible Veg cover')\n\n\n\n\n\nLet’s look at this in relation to Manchester as a whole\n\n\nmanc_rgb %>%\n  plotRGB(.,axes = TRUE, stretch = \"lin\", main = \"Landsat True Color Composite\")\n\nveg %>%\n  plot(., add=TRUE, legend=FALSE)"
  },
  {
    "objectID": "7_temperature.html#advanced-raster-calculations",
    "href": "7_temperature.html#advanced-raster-calculations",
    "title": "6  Temperature",
    "section": "6.7 Advanced raster calculations",
    "text": "6.7 Advanced raster calculations\nThe goal of this final section is to set up a mini investigation to see if there is a relationship between urban area and temperature. If our hypothesis is that there is a relationship then our null is that there is not a relationship…\n\n6.7.1 Calculating tempearture from Landsat data\nHere we are going to compute temperature from Landsat data — there are many methods that can be found within literature to do so but we will use the one originally developed by Artis & Carnahan (1982), recently summarised by Guha et al. 2018 and and Avdan and Jovanovska (2016).\nSome of the terms used our outlined in the terms section at the end of the document.\n\nCalculate the Top of Atmosphere (TOA) spectral radiance from the Digital Number (DN) using:\n\n\\[\\lambda= Grescale * QCAL + Brescale\\]\nTOA spectral radiance is light reflected off the Earth as seen from the satellite measure in radiance units.\nIn this equation Grescale and Brescale represent the gain and bias of the image, with QCAL the Digital Number (DN) — how the raw Landsat image is captured. To go from DN to spectral radiance we use the calibration curve, created before the launch of the sensor. Bias is the spectral radiance of the sensor for a DN of 0, Gain is the gradient of the slope for other values of DN. See page 84 in RADIOMETRIC CORRECTION OF SATELLITE IMAGES: WHEN AND WHY RADIOMETRIC CORRECTION IS NECESSARY for further information.\nGrescale and Brescale are available from the .MTL file provided when you downloaded the Landsat data. Either open this file in notepad and extract the required values for band 10 gain (MULT_BAND) and bias (ADD_BAND)\n…Or we can automate it using the MTL() function within the RStoolbox package\n\nlibrary(RStoolbox)\n\nMTL<-dir_info(here::here(\"prac_7\", \"Lsatdata\")) %>%\n  dplyr::filter(str_detect(path, \"MTL.txt\")) %>%\n  dplyr::select(path)%>%\n  pull()%>%\n  readMeta()\n\n #To see all the attributes\nhead(MTL)\n\n\nNow let’s extract the values from the readMTL variable for Band 10…we can either use the function getMeta() from RStoolbox of just extract the values ourselves…\n\n\noffsetandgain <-MTL %>%\n  getMeta(\"B10_dn\", metaData = ., what = \"CALRAD\")\n\noffsetandgain\n\n       offset      gain\nB10_dn    0.1 0.0003342\n\n##OR  \noffsetandgain <- subset(MTL$CALRAD, rownames(MTL$CALRAD) == \"B10_dn\")\n\n\nRun the calculation using the band 10 raster layer\n\n\nTOA <- offsetandgain$gain *\n  lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B10_mask + \n  offsetandgain$offset\n\n\nNext convert the TOA to Brightness Temperature \\(T_b\\) using the following equation:\n\n\\[T_b=\\frac{K_2}{ln((K_1/\\lambda)+1)}\\]\nBrightness temperature is the radiance travelling upward from the top of the atmosphere to the satellite in units of the temperature of an equivalent black body.\nK1 (774.8853) and K2 (1321.0789) are pre launch calibration constants provided by USGS.\nCheck the handbook for these values\n\nInstead of hardcoding these values…yep, you guessed it… we can extract them from our MTL\n\n\nCalidata <- MTL$CALBT%>%\n  terra::as.data.frame()%>%\n  mutate(Band=rownames(.))%>%\n  filter(Band==\"B10_dn\")\n\n# subset the columns\nK1 <- Calidata %>%\n  dplyr::select(K1)%>%\n  pull()\n\nK2 <- Calidata %>%\n  dplyr::select(K2)%>%\n  pull()\n\nBrighttemp <- (K2 / log((K1 / TOA) + 1))\n\nEarlier we calculated NDVI, let’s use that to determine emissivity of each pixel.\n\nFirst we need to calculate the fractional vegetation of each pixel, through the equation:\n\n\\[F_v= \\left( \\frac{NDVI - NDVI_{min}}{NDVI_{max}-NDVI_{min}} \\right)^2\\] ::: {.cell hash=‘7_temperature_cache/html/unnamed-chunk-25_797e04879f4c13f2e77fae9e97235aaa’}\nfacveg <- (ndvi-0.2/0.5-0.2)^2\n:::\nFractional vegetation cover is the ratio of vertically projected area of vegetation to the total surface extent.\nHere, \\(NDVI_{min}\\) is the minimum NDVI value (0.2) where pixels are considered bare earth and \\(NDVI_{max}\\) is the value at which pixels are considered healthy vegetation (0.5)\n\nNow compute the emissivity using:\n\n\\[\\varepsilon = 0.004*F_v+0.986\\]\n\nemiss <- 0.004*facveg+0.986\n\nEmissivity is the ratio absorbed radiation energy to total incoming radiation energy compared to a blackbody (which would absorb everything), being a measure of absorptivity.\n\nGreat, we’re nearly there… get our LST following the equation from Weng et al. 2004 (also summarised in Guja et al. (2018) and Avdan and Jovanovska (2016)):\n\n\\[LST= \\frac{T_b}{1+(\\lambda \\varrho T_b / (p))ln\\varepsilon}\\]\nWhere:\n\\[p= h\\frac{c}{\\varrho}\\]\nOk, don’t freak out….let’s start with calculating \\(p\\)\nHere we have:\n\n\\(h\\) which is Plank’s constant \\(6.626 × 10^-34 Js\\)\n\\(c\\) which is the velocity of light in a vaccum \\(2.998 × 10^8 m/sec\\)\n\\(\\varrho\\) which is the Boltzmann constant of \\(1.38 × 10^-23 J/K\\)\n\n\nBoltzmann <- 1.38*10e-23\nPlank <- 6.626*10e-34\nc <- 2.998*10e8\n\np <- Plank*(c/Boltzmann)\n\nNow for the rest of the equation….we have the values for:\n\n\\(\\lambda\\) which is the effective wavelength of our data (10.9 for Landsat 8 band 10)\n\\(\\varepsilon\\) emissivity\n\\(T_b\\) Brightness Temperature\n\n\nRun the equation with our data\n\n\n#define remaining varaibles\nlambda <- 1.09e-5\n#run the LST calculation\nLST <- Brighttemp/(1 +(lambda*Brighttemp/p)*log(emiss))\n# check the values\nLST\n\nclass      : RasterLayer \ndimensions : 533, 592, 315536  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : 540915, 558675, 5917125, 5933115  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=30 +datum=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : 281.4855, 305.9571  (min, max)\n\n\n\nAre the values very high?… That’s because we are in Kevlin not degrees Celcius…let’s fix that and plot the map\n\n\nLST <- LST-273.15\nplot(LST)\n\n\n\n\nNice that’s our temperature data sorted."
  },
  {
    "objectID": "7_temperature.html#calucating-urban-area-from-landsat-data",
    "href": "7_temperature.html#calucating-urban-area-from-landsat-data",
    "title": "6  Temperature",
    "section": "6.8 Calucating urban area from Landsat data",
    "text": "6.8 Calucating urban area from Landsat data\nHow about we extract some urban area using another index and then see how our temperature data is related?\nWe will use the Normalized Difference Built-up Index (NDBI) algorithm for identification of built up regions using the reflective bands: Red, Near-Infrared (NIR) and Mid-Infrared (MIR) originally proposed by Zha et al. (2003).\nIt is very similar to our earlier NDVI calculation but using different bands…\n\\[NDBI= \\frac{Short-wave Infrared (SWIR)-Near Infrared (NIR)}{Short-wave Infrared (SWIR)+Near Infrared (NIR)}\\]\nIn Landsat 8 data the SWIR is band 6 and the NIR band 5\n\nLet’s compute this index now…\n\n\nNDBI=((lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B6_mask-\n         lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B5_mask)/\n        (lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B6_mask+\n        lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B5_mask))\n\nBut do you remember our function? …Well this is the same calculation we used there just with different raster layers (or bands) so we could reuse it…\n\nNDBIfunexample <- NDVIfun(lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B6_mask,\n                          lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B5_mask)"
  },
  {
    "objectID": "7_temperature.html#urban-area-and-temperature-relationship",
    "href": "7_temperature.html#urban-area-and-temperature-relationship",
    "title": "6  Temperature",
    "section": "6.9 Urban area and temperature relationship",
    "text": "6.9 Urban area and temperature relationship\n\nWe could plot the varaibles agaisnt each other but there are a lot of data points\n\n\nplot(values(NDBI), values(LST))\n\n\n\n\nThis is termed the overplotting problem. So, let’s just take a random subset of the same pixels from both raster layers.\n\nTo do so we need to again stack our layers\n\n\n# stack the layers\n\ncomputeddata <- LST%>%\n  stack(.,NDBI)%>%\n  terra::as.data.frame()%>%\n  na.omit()%>%\n  # take a random subset\n  dplyr::sample_n(., 500)%>%\n  dplyr::rename(Temp=\"layer.1\", NDBI=\"layer.2\")\n\n # check the output\nplot(computeddata$Temp, computeddata$NDBI)\n\n\n\n\n\nLet’s jazz things up, load some more packages\n\n\nlibrary(plotly)\nlibrary(htmlwidgets)\n\n\nTransfrom the data to a data.frame to work with ggplot, then plot\n\n\nheat<-ggplot(computeddata, aes(x = NDBI, y = Temp))+\n  geom_point(alpha=2, colour = \"#51A0D5\")+\n  labs(x = \"Temperature\", \n       y = \"Urban index\",\n       title = \"Manchester urban and temperature relationship\")+\n   geom_smooth(method='lm', se=FALSE)+\n  theme_classic()+\n  theme(plot.title = element_text(hjust = 0.5))\n\n# interactive plot\nggplotly(heat)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nIt’s a masterpiece!\n\n\n\n\n\nggplot2 masterpiece. Source: Allison Horst data science and stats illustrations\n\n\n\n\n\nHow about plotting the whole dataset rather than a random subset…\n\n\ncomputeddatafull <- LST%>%\n  stack(.,NDBI)%>%\n  terra::as.data.frame()%>%\n  na.omit()%>%\n  # take a random subset\n  dplyr::rename(Temp=\"layer.1\", NDBI=\"layer.2\")\n\nhexbins <- ggplot(computeddatafull, \n                  aes(x=NDBI, y=Temp)) +\n  geom_hex(bins=100, na.rm=TRUE) +\n  labs(fill = \"Count per bin\")+\n  geom_smooth(method='lm', se=FALSE, size=0.6)+\n  theme_bw()\n\nggplotly(hexbins)"
  },
  {
    "objectID": "7_temperature.html#statistical-summary",
    "href": "7_temperature.html#statistical-summary",
    "title": "6  Temperature",
    "section": "6.10 Statistical summary",
    "text": "6.10 Statistical summary\n\nTo see if our variables are related let’s run some basic correlation\n\n\nlibrary(rstatix)\nCorrelation <- computeddatafull %>%\n  cor_test(Temp, NDBI, use = \"complete.obs\", method = c(\"pearson\"))\n\nCorrelation\n\n# A tibble: 1 × 8\n  var1  var2    cor statistic     p conf.low conf.high method \n  <chr> <chr> <dbl>     <dbl> <dbl>    <dbl>     <dbl> <chr>  \n1 Temp  NDBI   0.66      394.     0    0.660     0.665 Pearson\n\n\nLet’s walk through the results here…\n\np-value: tells us whether there is a statistically significant correlation between the datasets and if that we can reject the null hypothesis if p<0.05 (there is a 95% chance that the relationship is real).\ncor: Product moment correlation coefficient\nconf.low and con.high intervals: 95% confident that the population correlation coefficient is within this interval\nstatistic value (or t, or test statistic)\n\nWe can work out the critical t value using:\n\nabs(qt(0.05/2, 198268))\n\n[1] 1.959976\n\n\nWithin this formula\n\n0.05 is the confidence level (95%)\n2 means a 2 sided test\n198268 is the degrees of freedom (df), being the number of values we have -2\n\n\ncomputeddatafull %>%\n  pull(Temp)%>%\n  length()\n\n[1] 198270\n\nlength(computeddatafull)\n\n[1] 2\n\n\nHere, as our t values is > than the critical value we can say that there is a relationship between the datasets. However, we would normally report the p-value…\nAs p<0.05 is shows that are variables are have a statistically significant correlation… so as urban area (assuming the index in representative) per pixel increases so does temperature…therefore we can reject our null hypothesis… but remember that this does not imply causation!!\nIf you want more information on statistics in R go and read YaRrr! A Pirate’s Guide to R, chapter 13 on hypothesis tests."
  },
  {
    "objectID": "7_temperature.html#lsoamsoa-stats",
    "href": "7_temperature.html#lsoamsoa-stats",
    "title": "6  Temperature",
    "section": "6.11 LSOA/MSOA stats",
    "text": "6.11 LSOA/MSOA stats\nThis is all useful, but what if we wanted to present this analysis to local leaders in Manchester to show which areas experience the highest temperature (remember, with the limitation of this being one day!)\nNext, we will aggregate our raster data to LSOAs, taking a mean of the pixels in each LSOA.\nLSOA data: https://data-communities.opendata.arcgis.com/datasets/5e1c399d787e48c0902e5fe4fc1ccfe3\nMOSA data: https://data.cambridgeshireinsight.org.uk/dataset/output-areas/resource/0e5ac3b8-de71-4123-a334-0d1506a50288\nLet’s prepare the new spatial data: ::: {.cell}\nlibrary(dplyr)\nlibrary(sf)\n\n# read in LSOA data\nUK_LSOA <- st_read(here::here(\"prac_7\", \n                                          \"LSOA\",\n                                          \"Lower_Super_Output_Area_(LSOA)_IMD_2019__(OSGB1936).shp\"))\n\nReading layer `Lower_Super_Output_Area_(LSOA)_IMD_2019__(OSGB1936)' from data source `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\CASA0023\\CASA0023-book\\prac_7\\LSOA\\Lower_Super_Output_Area_(LSOA)_IMD_2019__(OSGB1936).shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 34753 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -6.418524 ymin: 49.86474 xmax: 1.762942 ymax: 55.81107\nGeodetic CRS:  WGS 84\n\n# project it to match Manchester boundary\nUK_LSOA <- UK_LSOA %>%\n  st_transform(., 32630)\n\n# read in MSOA and project it\nMSOA <- st_read(here::here(\"prac_7\",\n                           \"MSOA\",\n                           \"Middle_Layer_Super_Output_Areas_December_2011_Generalised_Clipped_Boundaries_in_England_and_Wales.shp\")) %>%\n   st_transform(., 32630)\n\nReading layer `Middle_Layer_Super_Output_Areas_December_2011_Generalised_Clipped_Boundaries_in_England_and_Wales' from data source `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\CASA0023\\CASA0023-book\\prac_7\\MSOA\\Middle_Layer_Super_Output_Areas_December_2011_Generalised_Clipped_Boundaries_in_England_and_Wales.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 7201 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 82675.02 ymin: 5343.575 xmax: 655606.3 ymax: 657533.7\nProjected CRS: OSGB 1936 / British National Grid\n\n#select only MSOA within boundary\nmanchester_MSOA <- MSOA[manchester_boundary, , op=st_within]\n\n#select only LSOA that intersect MSOA\nmanchester_LSOA <- UK_LSOA[manchester_MSOA,]\n:::\nNext, we need to do the extraction with raster::extract(). fun() specifies how to summarise the pixels within the spatial unit (LSOA), na.rm()=TRUE ignores NA values and df=TRUE outputs the result to a dataframe.\n\n# extract mean LST value per LSOA\nLST_per_LSOA <- terra::extract(LST, manchester_LSOA, fun=mean, na.rm=TRUE, df=TRUE)\n\n# add the LSOA ID back\nLST_per_LSOA$FID<-manchester_LSOA$FID\n\n# join the average temp to the sf\nmanchester_LSOA_temp <- manchester_LSOA %>%\n  left_join(.,\n            LST_per_LSOA,\n            by=\"FID\")%>%\n  dplyr::rename(temp=layer)\n\nNow we have the temperature per LSOA, but what about the amount of land considered urban? Here, we will assume that any NDBI value above 0 means the whole pixel is considered urban. raster::extract() can also be used to get all the pixels within each spatial area (LSOA)..\n\n#define urban as NDBI greater than 0\nNDBI_urban<- NDBI > 0\n\n# Sum the pixels that are grater than 0 per LSOA\nNDBI_urban_per_LSOA <- terra::extract(NDBI_urban, manchester_LSOA, na.rm=TRUE, df=TRUE, fun=sum)\n\n# list the pixels per LSOA\nNDBI_per_LSOA_cells <- terra::extract(NDBI_urban, manchester_LSOA, na.rm=TRUE, df=TRUE, cellnumbers=TRUE)\n\n#count the pixels per LSOA\nNDBI_per_LSOA2_cells<- NDBI_per_LSOA_cells %>%\n  count(ID)\n\n#add the LSOA ID to the urban area\nNDBI_urban_per_LSOA$FID<-manchester_LSOA$FID\n\n#add the LSOA ID to the number of cells\nNDBI_per_LSOA2_cells$FID<-manchester_LSOA$FID\n\n#join these two\nUrban_info_LSOA <- NDBI_urban_per_LSOA %>%\n  left_join(.,\n            NDBI_per_LSOA2_cells,\n            by=\"FID\")\n\n# remove what you don't need and rename\nUrban_info_LSOA_core_needed <- Urban_info_LSOA %>%\n  dplyr::rename(urban_count=layer, \n                LSOA_cells=n) %>%\n  dplyr::select(urban_count,\n         LSOA_cells,\n         FID)%>%\n  dplyr::mutate(percent_urban=urban_count/LSOA_cells*100)\n\n# join the data \n# one sf with temp and % urban per LSOA\nmanchester_LSOA_temp_urban <- manchester_LSOA_temp %>%\n  left_join(.,\n             Urban_info_LSOA_core_needed,\n             by=\"FID\")"
  },
  {
    "objectID": "7_temperature.html#mapping",
    "href": "7_temperature.html#mapping",
    "title": "6  Temperature",
    "section": "6.12 Mapping",
    "text": "6.12 Mapping\nNow, we could map both temperature (and the % of urban area) within a LSOA individually….In our map we want to include some place names from Open Street Map, so let’s get those from OSM that is stored on Geofabrik\n\nPlaces <- st_read(here::here(\"prac_7\", \n                                          \"OSM\",\n                                          \"gis_osm_places_free_1.shp\")) %>%\n   st_transform(., 32630)\n\nReading layer `gis_osm_places_free_1' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\CASA0023\\CASA0023-book\\prac_7\\OSM\\gis_osm_places_free_1.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 635 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -2.717277 ymin: 53.33978 xmax: -1.921855 ymax: 53.67854\nGeodetic CRS:  WGS 84\n\nmanchester_Places <- Places[manchester_boundary,]%>%\n  filter(fclass==\"city\")\n\nLet’s make a map, like we did earlier CASA0005 using the tmap package, remember to add a caption in Rmarkdown include the argument fig.cap=\"caption here\" in the code chunk header.\n\n# this first bit makes the box bigger\n# so we can have a north arrow not overlapping the data\n# see: https://www.jla-data.net/eng/adjusting-bounding-box-of-a-tmap-map/\nbbox_new <- st_bbox(manchester_LSOA_temp_urban) # current bounding box\n\nyrange <- bbox_new$ymax - bbox_new$ymin # range of y values\n\nbbox_new[4] <- bbox_new[4] + (0.1 * yrange) # ymax - top\nbbox_new[2] <- bbox_new[2] - (0.1 * yrange) # ymin - bottom\n\n# the plot starts here\nlibrary(tmap)\ntmap_mode(\"plot\")\n# set the new bbox\n# remove bbox=bbox_new to see the difference\ntm1 <- tm_shape(manchester_LSOA_temp_urban, bbox = bbox_new) + \n  tm_polygons(\"temp\",\n              palette=\"OrRd\",\n              legend.hist=TRUE,\n              title=\"Temperature\")+\n  tm_shape(manchester_Places, bbox=bbox_new)+\n  tm_dots(size=0.1, col=\"white\")+\n  tm_text(text=\"name\", size=0.75, ymod=-0.5, col=\"white\", fontface = \"bold\")+\n  #tm_legend(show=FALSE)+\n  tm_layout(frame=FALSE,\n            legend.outside=TRUE)+\n  tm_compass(type = \"arrow\", size=1, position = c(\"left\", \"top\")) +\n  tm_scale_bar(position= c(\"left\", \"bottom\"), breaks=c(0,2,4), text.size = .75)\n  #tm_credits(\"(a)\", position=c(0,0.85), size=1.5)\n\ntm1\n\n\n\n\nAverage temperature per LSOA in Manchester, calcualted from Landsat imagery dated 13/5/19, following the methodology specified by Guha et al. 2018\n\n\n\n\nThis could also be repeated for urban area and plotted in one figure, perhaps with an inset map like we previously did. But, we can also now plot two choropleth maps over each other, this is made much easier with the biscale() package, however, now we must use ggplot as opposed to tmap, you can force tmap to do it, but the feature is still in development."
  },
  {
    "objectID": "7_temperature.html#bivariate-mapping-optional",
    "href": "7_temperature.html#bivariate-mapping-optional",
    "title": "6  Temperature",
    "section": "6.13 Bivariate mapping (optional)",
    "text": "6.13 Bivariate mapping (optional)\nWhilst bivaraite maps look cool, they don’t tell us anything about the actual data values, simply dividing the data into classes based on the style parameter which we have seen before - jenks, equal and so on. So here I want to produce:\n\nA central bivariate map of LSOAs within Manchester\nThe central bivariate will have the MSOA boundaries and some place names\nTwo plots showing the distribution of the data\n\n\nlibrary(biscale)\nlibrary(cowplot)\nlibrary(sysfonts)\nlibrary(extrafont) \nlibrary(showtext) # more fonts\n#font_add_google(\"Lato\", regular.wt = 300, bold.wt = 700) # I like using Lato for data viz (and everything else...). Open sans is also great for web viewing.\nshowtext_auto()\n\n# create classes\ndata <- bi_class(manchester_LSOA_temp_urban, x = temp, y = percent_urban, style = \"jenks\", dim = 3)\n\n#ggplot map\nmap <- ggplot() +\n geom_sf(data = data, mapping = aes(fill = bi_class), color=NA, lwd = 0.1, show.legend = FALSE) +\n  bi_scale_fill(pal = \"DkViolet\", dim = 3) +\n  geom_sf(data = manchester_MSOA, mapping = aes(fill=NA), color=\"black\", alpha=0, show.legend = FALSE)+\n  geom_sf(data=manchester_Places, mapping=aes(fill=NA), color=\"white\", show.legend = FALSE)+\n  geom_sf_text(data=manchester_Places, aes(label = name, hjust = 0.5, vjust = -0.5),\n               nudge_x = 0, nudge_y = 0,\n               fontface = \"bold\",\n             color = \"white\",\n             show.legend = FALSE,\n             inherit.aes = TRUE)+\n  labs(\n    title = \"\",\n    x=\"\", y=\"\"\n  ) +\n  bi_theme()\n\nlegend <- bi_legend(pal = \"DkViolet\",\n                    dim = 3,\n                    xlab = \"Temperature \",\n                    ylab = \"% Urban\",\n                    size = 8)\n\ncredit<- (\"Landsat dervied temperature and urban area, taken 13/5/19\")\n\n# combine map with legend\nfinalPlot <- ggdraw() +\n  draw_plot(map, 0, 0, 1, 1) +\n  draw_plot(legend, 0.1, 0.1, 0.2, 0.2)\n  #draw_text(credit, 0.68, 0.1, 0.2, 0.2, size=10)\nfinalPlot\n\n\n\n\nThat’s the main bivaraite plot, now to the side plots…\n\nurban_box<-ggplot(data, aes(x=bi_class, y=percent_urban, fill=bi_class)) +\n  geom_boxplot()+\n  scale_fill_manual(values=c(\"#CABED0\", \"#BC7C8F\", \"#806A8A\", \"#435786\", \"#AE3A4E\", \"#77324C\", \"#3F2949\", \"#3F2949\"))+\n  labs(x=\"Bivariate class (temp, urban)\", \n       y=\"Urban %\")+\n  theme_light()+\n  theme(legend.position=\"none\") # Remove legend\n\ntemp_violin<-ggplot(data, aes(x=bi_class, y=temp, fill=bi_class))+\n  geom_violin()+\n  scale_fill_manual(values=c(\"#CABED0\", \"#BC7C8F\", \"#806A8A\", \"#435786\", \"#AE3A4E\", \"#77324C\", \"#3F2949\", \"#3F2949\"))+\n  labs(x=\"\", \n       y=\"Temperature\")+\n   guides(fill=guide_legend(title=\"Class\"))+\n  theme_light()+\n  theme(legend.position=\"none\") # Remove legend\n\nJoin them all together in two steps - make the side plots, then join that to the main plot\n\nside <- plot_grid(temp_violin, urban_box, labels=c(\"B\",\"C\"),label_size = 12, ncol=1)\n\nall <- plot_grid(finalPlot, side, labels = c('A'), label_size = 12, ncol = 2,  rel_widths = c(2, 1))\n\nNow this looks a bit rubbish on the page. Usually, you might want to export the map to a .png to use elsewhere, but you will need to resize it. I like to plot the map in the console (type all in the console then press enter), click export in the plots tab, reszie the image and note down the numbers then input them below..\n\nall\n\n\n\ndev.copy(device = png, filename = here::here(\"prac_7\", \"bivaraite.png\"), width = 687, height = 455) \n\npng \n  3 \n\ndev.off()\n\npng \n  2 \n\n\n\n\n\n\n\n\n\n\n\n\n6.13.1 Map notes\n\nThe Manchester outline shapefile doesn’t match up with either the MSOA or LSOA boundaries, this could be solved by making sure the boundary data (e.g. city outline) matches any other spatial units.\nWhen selecting the data (within the Manchester boundary) any MSOA shape that was within the boundary selected\nThen any LSOA that intersected the MSOA layer was selected\nTo resolve this you could clip the MSOA layer to the current displayed layer to force a boundary around the map\nA box plot was used as the data was very spread out in the urban % plot, so a violin plot didn’t show much."
  },
  {
    "objectID": "7_temperature.html#considerations",
    "href": "7_temperature.html#considerations",
    "title": "6  Temperature",
    "section": "6.14 Considerations",
    "text": "6.14 Considerations\nIf you wanted to explore this type of analysis further then you would need to consider the following:\n\nOther methods for extracting temperature from Landsat data\nOther methods for identifying urban area from Landsat data such as image classificaiton\nThe formula used to calculate emissivity — there are many\nThe use of raw satellite data as opposed to remove the effects of the atmosphere. Within this practical we have only used relative spatial indexes (e.g. NDVI). However, if you were to use alternative methods it might be more appropriate to use surface reflectance data (also provided by USGS)."
  },
  {
    "objectID": "7_temperature.html#references",
    "href": "7_temperature.html#references",
    "title": "6  Temperature",
    "section": "6.15 References",
    "text": "6.15 References\nThanks to former CASA graduate student Dr Matt Ng for providing the outline to the start of this practical\nAvdan, U. and Jovanovska, G., 2016. Algorithm for automated mapping of land surface temperature using LANDSAT 8 satellite data. Journal of Sensors, 2016.\nGuha, S., Govil, H., Dey, A. and Gill, N., 2018. Analytical study of land surface temperature with NDVI and NDBI using Landsat 8 OLI and TIRS data in Florence and Naples city, Italy. European Journal of Remote Sensing, 51(1), pp.667-678.\nWeng, Q., Lu, D. and Schubring, J., 2004. Estimation of land surface temperature–vegetation abundance relationship for urban heat island studies. Remote sensing of Environment, 89(4), pp.467-483.\nYoung, N.E., Anderson, R.S., Chignell, S.M., Vorster, A.G., Lawrence, R. and Evangelista, P.H., 2017. A survival guide to Landsat preprocessing. Ecology, 98(4), pp.920-932.\nZha, Y., Gao, J. and Ni, S., 2003. Use of normalized difference built-up index in automatically mapping urban areas from TM imagery. International journal of remote sensing, 24(3), pp.583-594."
  },
  {
    "objectID": "7_temperature.html#remote-sensing-refresher",
    "href": "7_temperature.html#remote-sensing-refresher",
    "title": "6  Temperature",
    "section": "6.16 Remote sensing refresher",
    "text": "6.16 Remote sensing refresher\nLandsat sensors capture reflected solar energy, convert these data to radiance, then rescale this data into a Digital Number (DN), the latter representing the intensity of the electromagnetic radiation per pixel. The range of possible DN values depends on the sensor radiometric resolution. For example Landsat Thematic Mapper 5 (TM) measures between 0 and 255 (termed 8 bit), whilst Landsat 8 OLI measures between 0 and 65536 (termed 12 bit). These DN values can then be converted into Top of Atmosphere (TOA) radiance and TOA reflectance through available equations and known constants that are preloaded into certain software. The former is how much light the instrument sees in meaningful units whilst the latter removes the effects of the light source. However, TOA reflectance is still influenced by atmospheric effects. These atmospheric effects can be removed through atmospheric correction achievable in software such as ENVI and QGIS to give surface reflectance representing a ratio of the amount of light leaving a target to the amount of light striking it.\nWe must also consider the spectral resolution of satellite imagery, Landsat 8 OLI has 11 spectral bands and as a result is a multi-spectral sensor. As humans we see in the visible part of the electromagnetic spectrum (red-green-blue) — this would be three bands of satellite imagery — however satellites can take advantage of the rest of the spectrum. Each band of Landsat measures in a certain part of the spectrum to produce a DN value. We can then combine these values to produce ‘colour composites’. So a ‘true’ colour composite is where red, green and blue Landsat bands are displayed (the visible spectrum). Based on the differing DN values obtained, we can pick out the unique signatures (values of all spectral bands) of each land cover type, termed spectral signature.\nFor more information read Young et al. (2017) A survival guide to Landsat preprocessing"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "7  Resources",
    "section": "",
    "text": "https://github.com/robmarkcole/satellite-image-deep-learning#techniques\nhttps://urbanspatial.github.io/classifying_satellite_imagery_in_R/#Supervised_Classification\nhttps://opengislab.com/blog/2018/5/14/flood-mapping-with-sentinel-1-data-using-snap-and-qgis"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "8  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Remotely Sensing Cities and Environments",
    "section": "",
    "text": "Andy MacLachlan1\nLast updated: 2022-07-20"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "CASA0023 Remotely Sensing Cities and Environments",
    "section": "Welcome",
    "text": "Welcome\nHello  and welcome to the Term 2 module Remotely Sensing Cities and Environments.\nSimilarly, to my Term 1 MSc module, CASA0005, this website holds all the practical instructions for the module. CASA0005 Geographic Information Systems and Science (or a similar module) is a pre-requisite of this module so concepts taught there will mainly be assumed here."
  },
  {
    "objectID": "index.html#acknowledgement",
    "href": "index.html#acknowledgement",
    "title": "CASA0023 Remotely Sensing Cities and Environments",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThanks to the following academics who inspired the creating of the module and various concepts within it:\n\nDr Ellie Biggs\nDr Gareth Roberts\nDr Bryan Boruff\nProfessor Ted Milton\nDr Laurie Chisholm\n\nThanks again to the following people who have either contributed directly or provided code in repositories that have helped me style this book:\n\n\nR Studio\n\nSTAT 545\nrstudio4edu\nHadley Wickham\nAlison Presmanes Hill\nDesirée De Leon\nYihui Xie\nJulia Silge\nJenny Bryan\nOthers\n\nRobin Lovelace\nTwitter for R programmers\nMatt Ng\nStatQuest with Josh Starmer\nGarrick Aden‑Buie\n\n\n\nThe R package and analysis artwork used within this book has been produced by allison_horst, whilst artwork used in information boxes has been produced by Desirée De Leon. You can find Allison’s images on the stats illustration GitHub repository and Desirée’s on the rstudio4edu GitHub repository.\nI’ve certainly learnt a lot from their open code repositories!"
  },
  {
    "objectID": "5_classification_I.html",
    "href": "5_classification_I.html",
    "title": "\n5  Google Earth Engine I\n",
    "section": "",
    "text": "For this week we are going to move away form R and focus on Google Earth Engine, which is still quite new for me too.\nGoogle Earth Engine (GEE) is a platform that let’s us analyse data that is stored remotely, each time we write some code it is sent to a server to be evaluated and the results are returned to us. As we don’t compute locally it speeds all of our processes up!\nGEE uses Javascript…and according to the GEE team this is all of the Javascript you need to know:\nTo start with GEE i want to pull up the same image from a specific date for a city, i will use\nWe can make a new point in the code editor with:"
  },
  {
    "objectID": "5_classification_I.html#points",
    "href": "5_classification_I.html#points",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.1 Points",
    "text": "5.1 Points\nIn this case my city is Dheli, when i enter this code you will get a message asking if you want to convert it to an imported record, click convert and it will appear in the imports…\nAlternatively we can click the point icon on the map and add a new point…> new layer > give an suitable name\n\n\n\n\n\n\n\n\nTo center the map on this point…where the second argument is the zoom level\n\nMap.centerObject(Dheli, 10)"
  },
  {
    "objectID": "5_classification_I.html#landsat-data",
    "href": "5_classification_I.html#landsat-data",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.2 Landsat data",
    "text": "5.2 Landsat data\nNow we need to load some surface reflectance data > browse data catalogue > click through to a Landsat surface reflectance dataset, Collection 2 and tier 1. You will see the code to import the data to your script and if you click the open window icon you can see an example…\n\n\n\n\n\n\n\n\nHere through let’s just load some data…\n\nvar dataset = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')\n    .filterDate('2022-01-01', '2022-02-01');\n\nThis doesn’t do anything until we actually add it to our map….\n\nMap.addLayer(dataset, {bands: [\"SR_B4\", \"SR_B3\", \"SR_B2\"]})\n\nWhat has this actually done? Try zooming out and inspecting the image. On the right hand side click inspector and click around the image…\nAt the moment we aren’t sure exactly what images we are using or how much cloud cover is in them…to sort this…\nNote, that in GEE // comments out a line of code…\n\n// Load Landsat 8 data, filter by date, month, and bounds.\nvar dataset = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')\n  .filterDate('2020-01-01', '2022-10-10')\n // .filter(ee.Filter.calendarRange(1, 2, 'month'))  // Jan and Feb images\n  .filterBounds(Dheli);  // Intersecting ROI\n\nvar filtered = dataset.filter(ee.Filter.eq('CLOUD_COVER', 0));\n\nprint(filtered)"
  },
  {
    "objectID": "5_classification_I.html#problems",
    "href": "5_classification_I.html#problems",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.3 Problems",
    "text": "5.3 Problems\nIn my case this provides three images…and if we wanted to export the metadata of these images we could using…\n\nExport.table.toDrive(filtered, 'exportAsCSV', 'GEE', 'CSVexport', 'CSV');\n\nNow I have two problems:\n\nThe collection has three images within it and when i display it Map.addLayer(filtered, {bands: [\"SR_B4\", \"SR_B3\", \"SR_B2\"]}) we get the top most image.\nThe image only covers my single point and not the whole of Dehli / the administration area.\n\n\n5.3.1 Polygons\nLet’s deal with problem two first…\n\nI could change my point to a shape that i can draw. Next to the point icon (in the map/layer window) there is a polygon icon.\nI could also add another point to my current single point where i want the next tile to be\nI could filter based on tiles from the image collection\nI can upload a file, such as one from GADM. To do so, download the appropraite GADM boundary for your city, query it in QGIS to get a city outline. For me this was the shapefile gadm41_IND_2 and I need to filter (in GEE or other software) the GID_1 column for the row IND.25_1.\n\nOn GEE there is an assets button where you can upload data, upload your shapefile. Next we load it and filter what you need…\n\nvar india = ee.FeatureCollection('users/andrewmaclachlan/india')\n    .filter('GID_1 == \"IND.25_1\"');\n\nTo now filter based on this we include it as our filter bounds. At the time of writing there weren’t enough cloud free Landsat 9 scenes over Dheli to create a complete image, so I have changed to Landsat 8…\n```{r. eval=FALSE} var oneimage_study_area = ee.ImageCollection(‘LANDSAT/LC08/C02/T1_L2’) .filterDate(‘2021-01-03’, ‘2022-10-10’) .filterBounds(india); // Intersecting ROI\nvar oneimage_study_area_cloud = oneimage_study_area.filter(ee.Filter.eq(‘CLOUD_COVER’, 0));\n\nWithin the Landsat 8 filename we can see which path and rows our tiles are on...\n\n`LANDSAT/LC08/C02/T1_L2/LC08_147040_20210526`\n\n\n### Single images \n\nNow problem one...\n\n* I could just select a single image from the collection by filtering using the specific date, which you can get from the console window when printing the images within the collection. Landsat (and most other EO datasets) have the date within the file path name, this is....`LC09_147040_20220403`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar oneimage = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')\n  .filterDate('2022-04-03', '2022-04-04')\n  .filterBounds(Dheli);  // Intersecting ROI\n:::\n\nI can reduce the collection to a single image through taking the mean, median using the imageCollection.reduce() function…\n\n\nvar median = filtered.reduce(ee.Reducer.median());\n// print the image info\nprint(median)\n\nvar img = ee.Image(‘COPERNICUS/S2_SR/20210109T185751_20210109T185931_T10SEG’);\nvar dataset = ee.Image(‘LANDFIRE/Fire/MFRI/v1_2_0/CONUS’);\nvar combined_img = ee.ImageCollection([img_1, img_2]).mosaic();\n\n5.3.2 Review questions"
  },
  {
    "objectID": "5_GEE_I.html",
    "href": "5_GEE_I.html",
    "title": "\n5  Google Earth Engine I\n",
    "section": "",
    "text": "For this week we are going to move away form R and focus on Google Earth Engine, which is still quite new for me too.\nGoogle Earth Engine (GEE) is a platform that let’s us analyse data that is stored remotely, each time we write some code it is sent to a server to be evaluated and the results are returned to us. As we don’t compute locally it speeds all of our processes up!\nGEE uses Javascript…and according to the GEE team this is all of the Javascript you need to know:\nTo start with GEE i want to pull up the same image from a specific date for a city, i will use\nWe can make a new point in the code editor with:"
  },
  {
    "objectID": "5_GEE_I.html#points",
    "href": "5_GEE_I.html#points",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.1 Points",
    "text": "5.1 Points\nIn this case my city is Dheli, when i enter this code you will get a message asking if you want to convert it to an imported record, click convert and it will appear in the imports…\nAlternatively we can click the point icon on the map and add a new point…> new layer > give an suitable name\n\n\n\n\n\n\n\n\nTo center the map on this point…where the second argument is the zoom level\n\nMap.centerObject(Dheli, 10)"
  },
  {
    "objectID": "5_GEE_I.html#landsat-data",
    "href": "5_GEE_I.html#landsat-data",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.2 Landsat data",
    "text": "5.2 Landsat data\nNow we need to load some surface reflectance data > browse data catalogue > click through to a Landsat surface reflectance dataset, Collection 2 and tier 1. You will see the code to import the data to your script and if you click the open window icon you can see an example…\n\n\n\n\n\n\n\n\nHere through let’s just load some data…\n\nvar dataset = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')\n    .filterDate('2022-01-01', '2022-02-01');\n\nThis doesn’t do anything until we actually add it to our map….\n\nMap.addLayer(dataset, {bands: [\"SR_B4\", \"SR_B3\", \"SR_B2\"]})\n\nWhat has this actually done? Try zooming out and inspecting the image. On the right hand side click inspector and click around the image…\nAt the moment we aren’t sure exactly what images we are using or how much cloud cover is in them…to sort this…\nNote, that in GEE // comments out a line of code…\n\n// Load Landsat 9 data, filter by date, month, and bounds.\nvar dataset = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')\n  .filterDate('2020-01-01', '2022-10-10')\n // .filter(ee.Filter.calendarRange(1, 2, 'month'))  // Jan and Feb images\n  .filterBounds(Dheli)  // Intersecting ROI\n  .filter(ee.Filter.lt(\"CLOUD_COVER\", 0.1));\n  \n  print(dataset)"
  },
  {
    "objectID": "5_GEE_I.html#problems",
    "href": "5_GEE_I.html#problems",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.3 Problems",
    "text": "5.3 Problems\nIn my case this provides three images…and if we wanted to export the metadata of these images we could using…\n\nExport.table.toDrive(dataset, 'exportAsCSV', 'GEE', 'CSVexport', 'CSV');\n\nNow I have two problems:\n\nThe collection has three images within it and when i display it Map.addLayer(filtered, {bands: [\"SR_B4\", \"SR_B3\", \"SR_B2\"]}) we get the top most image.\nThe image only covers my single point and not the whole of Dehli / the administration area.\n\n\n5.3.1 Polygons\nLet’s deal with problem two first…\n\nI could change my point to a shape that i can draw. Next to the point icon (in the map/layer window) there is a polygon icon.\nI could also add another point to my current single point where i want the next tile to be\nI could filter based on tiles from the image collection\nI can upload a file, such as one from GADM. To do so, download the appropraite GADM boundary for your city, query it in QGIS to get a city outline. For me this was the shapefile gadm41_IND_2 and I need to filter (in GEE or other software) the GID_1 column for the row IND.25_1.\n\nOn GEE there is an assets button where you can upload data, upload your shapefile. Next we load it and filter what you need…\n\nvar india = ee.FeatureCollection('users/andrewmaclachlan/india')\n    .filter('GID_1 == \"IND.25_1\"');\n\nTo now filter based on this we include it as our filter bounds. At the time of writing there weren’t enough cloud free Landsat 9 scenes over Dheli to create a complete image, so I have changed to Landsat 8…\n\nvar oneimage_study_area = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n  .filterDate('2021-06-01', '2022-10-10')\n  .filterBounds(india);  // Intersecting ROI\n\nvar oneimage_study_area_cloud = oneimage_study_area.filter(ee.Filter.eq('CLOUD_COVER', 0));\n\nWithin the Landsat 8 filename we can see which path and rows our tiles are on…LANDSAT/LC08/C02/T1_L2/LC08_146040_20211127\nUsing these dates I have 1 image from path 146, row 40, 1 image from path 146, row 41 and 4 images from path 147, row 40. See the Landsat Acquisition Tool to check your path and row\n\n5.3.2 Single images\nNow problem one…\n\nI could just select a single image from the collection by filtering using the specific date, which you can get from the console window when printing the images within the collection. Landsat (and most other EO datasets) have the date within the file path name, this is….LC09_147040_20220403\n\n\n\nvar oneimage = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2')\n  .filterDate('2022-04-03', '2022-04-04')\n  .filterBounds(india);  // Intersecting ROI\n\n\nI can load the specific image(s) i want…\n\n\nvar image_146_40 = ee.Image('LANDSAT/LC08/C02/T1_L2/LC08_146040_20211127')\n\n\nI can reduce the collection to a single image through taking the mean, median, max or min using the imageCollection.reduce() function…\n\n\nvar median = oneimage_study_area_cloud.reduce(ee.Reducer.median());\n// print the image info\nprint(median)"
  },
  {
    "objectID": "5_GEE_I.html#better-images",
    "href": "5_GEE_I.html#better-images",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.4 Better images",
    "text": "5.4 Better images\nUntil now we haven’t really dealt with the scaling factors from the Landsat surface reflectance product…\n\nLandsat Collection 2 surface reflectance has a scale factor of 0.0000275 and an additional offset of -0.2 per pixel.\n\nWe do this through making a function and then calling our collection to the function\n\n// Applies scaling factors in a function\nfunction applyScaleFactors(image) {\n  var opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n  var thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0);\n  return image.addBands(opticalBands, null, true)\n              .addBands(thermalBands, null, true);\n}\n\n// call our collection to the function and assign it to a new variable \noneimage_study_area_cloud_scale = oneimage_study_area_cloud.map(applyScaleFactors);\n\n// apply the median reducer\nvar oneimage_study_area_cloud_scale_median = oneimage_study_area_cloud_scale.reduce(ee.Reducer.median());\n\nWe can then map this…\n\n// set up some of the visualisation paramters \nvar vis_params = {\n  bands: ['SR_B4_median', 'SR_B3_median', 'SR_B2_median'],\n  min: 0.0,\n  max: 0.3,\n};\n\n// add a layer to the map\nMap.addLayer(oneimage_study_area_cloud_scale_median, visualization, 'True Color (432)');\n\nYou should have produced something like this…\n\n\n\n\n\n\n\n\nNote here that we give this layer a name 'True Color (432)' this means on the map under the layer button it will have that same name. Here, you can also set the transparency of each layer and even change the bands that display (although this doesn’t change the code)"
  },
  {
    "objectID": "5_GEE_I.html#mosaic-images",
    "href": "5_GEE_I.html#mosaic-images",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.5 Mosaic images",
    "text": "5.5 Mosaic images\nWhen we look at these images there might be very apparent differences between the tiles - this is probably due to the date of collection and the atmospheric correction applied (remember that it’s a model of the atmosphere).\n\nvar mosaic = oneimage_study_area_cloud_scale.mosaic();\n\nvar vis_params2 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0.0,\n  max: 0.3,\n};\n\nMap.addLayer(mosaic, vis_params2, 'spatial mosaic');\n\nIn this example i’ve mosaiced the image collection (before taking the median values). This has taken images according to their order in the collection (last on top).You’ll notice there isn’t much difference…but you will see the effect of the last on top rule with clear demarcations across where images overlap…making the problem we wanted to resolve worse!\n\n\n\n\n\n\n\n\nThe best easy solution we have here is to take a mean of all overlapping pixels…\n\nvar meanImage = oneimage_study_area_cloud_scale.mean();\n\nMap.addLayer(meanImage, vis_params2, 'mean');"
  },
  {
    "objectID": "5_GEE_I.html#texture-measures",
    "href": "5_GEE_I.html#texture-measures",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.6 Texture measures",
    "text": "5.6 Texture measures\nAs we saw earlier in the module we can compute texture…in GEE this is…\n\nvar glcm = meanImage.toUint16()\n  .select(['B1','B2', 'B3', 'B4', 'B5', 'B6', 'B7'])\n  .glcmTexture({size: 4});\n\nHere note:\n\nthe function .toUint16() as.glcmTexture won’t work with 32 bit data\nsize 4, represents the size of the kernel to consider for the texture measure of the central pixel.\n\n\nvar glcm = nir.glcmTexture({size: 4});\nvar contrast = glcm.select('N_contrast');\nMap.addLayer(contrast,\n             {min: 0, max: 1500, palette: ['0000CC', 'CC0000']},\n             'contrast');"
  },
  {
    "objectID": "5_GEE_I.html#pca",
    "href": "5_GEE_I.html#pca",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.8 PCA",
    "text": "5.8 PCA"
  },
  {
    "objectID": "3_corrections.html",
    "href": "3_corrections.html",
    "title": "\n3  Corrections\n",
    "section": "",
    "text": "As described in the lecture, a rather simple method to correct raw satellite (or any other) imagery is called Dark Object Subtraction (DOS). This uses the logic that the darkest pixel within the image should be 0 and therefore any value that it has can be attributed to the atmosphere.\nSo in order to remove the effect of the atmosphere we can the subtract the value from the rest of the pixels within the image.\nHere, we will need to download some raw satellite imagery, that is in Digital Number (DN).\nThen we will apply the formula to calculate the reluctance of the pixels by removing atmosphere effects.The formula for the cosine of the solar zenith angle correction (COST) is, this is the same as DOS but DOS omits TAUz. The following has made use of the documentation from GIS Ag Maps.com\n\\[\\rho_{\\lambda}= \\frac{(Lsat_{rad} - Lhaze1percent_{rad})\\pi * d^2}{EO_{\\lambda} * cos\\theta S * TUAv + TAUz}\\]\nWhere…\n\n\n\\(\\rho_{\\lambda}\\) is the corrected value (DOS applied)\n\nTop line of equation…\n\n\\(Lsat_{rad}\\) = at sensor radiance (recall DN goes to radiance through the regression equation we saw!)\n\n\\(Lhaze1percent_{rad}\\) = amount of radiance that is due to the atmosphere (atmospheric haze) from path or scatter radiance. Very few surfaces are completely black so it is assume that the darkest surface has a 1% reflectance. Various methods to caclcualte this…\n\nLook up tables\nSelecting the darkest pixels (shadow, water)\n\n\n\nWhen we have the haze amount then deduct 1% from that value per band as few targets are absolutely black.\nFor COST this this:\n\\[ 0.01 reflectance = 0.01 *\\frac{Eoλ * cosθs^2} {d² * pi}\\]\nFor DOS it’s\n\\[ 0.01 reflectance = 0.01 *\\frac{Eoλ * cosθs} {d² * pi}\\]\n\n\n\\(EO_{\\lambda}\\) or \\(ESUN_{\\lambda}\\) = mean exoatmospheric irradiance\n\nirradiance = power per unit area received from the Sun\nexoatmospheric = just outside the Earth’s atmosphere\nThese values are available from the Landsat user manual such as table 5.3 in the Landsat 7 user guide\n\n\n\n\\(cosθs\\) = cosine of the solar azimuth, remember from the lecture that this is 90 - solar elevation.\n\\(d\\) = the Earth-sun distance and is in the .MTL file\n\\(pi\\) = 3.14159265\n\nOnce we have all these values then we can do the following:\n\nCompute the haze value for each band (although not beyond NIR) - this is the amount of radiance that is due to the atmosphere (atmospheric haze), see above methods.\nConvert DN to radiance\nCompute the 1% reflectance value using the equations above\nSubtract the 1% reflectance value from the radiance. Here we are saying that we have a pixel (e.g. darkest pixel), we know what 1% of the total radiance is and we are subtracting that from the darkest pixel (which still has atmospheric effects) to account for most targets not being completely black.\n\nWe can now plug the values in:\n\\[\\rho_{\\lambda}= \\frac{(Lsat_{rad} - Lhaze1percent_{rad})\\pi * d^2}{EO_{\\lambda} * cos\\theta S * TUAv + TAUz}\\]\nWhere the \\(Lhaze1percent_{rad}\\) is the haze value (e.g. darkest pixel) minus 1% of the total radiance. This 1% was computed from the equations above.\nWithin this equation:\n\\(TAUv\\) = 1.0 for Landsat and \\(TAUz\\) = cosθs for COST method. DOS is the same, but without \\(TAUz\\)\nOf course we can do this in R (or any other software) with just one function! First we need to download some raw satellite data that comes in the Digital Number (DN) format. This is the exact same process as we saw in week 1, expect this time select the Collection 1, Level-1 bundle. At the moment this process won’t work with Landsat 9. However, as this involves a large amount of data and it’s unlikely you will need to do this in the module of data read through the following code and then move to the next section\n\nlibrary(terra)\nlibrary(raster)\nlibrary(RStoolbox)\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(rgdal)\n\n## Import meta-data and bands based on MTL file\nmtlFile  <- (\"prac_3/Lsatdata8/LC08_L1TP_175083_20211005_20211013_01_T1_MTL.txt\")\n                        \nmetaData <- readMeta(mtlFile)\n\nlsatMeta  <- stackMeta(metaData)\n\n# surface reflectance with DOS\n\nl8_boa_ref <- radCor(lsatMeta, metaData, method = \"dos\")\n\n#terra::writeRaster(l8_boa_ref, datatype=\"FLT4S\", filename = \"prac_3/Lsatdata8/l8_boa_ref.tif\", format = \"GTiff\", overwrite=TRUE)\n\n# Radiance \n\nlsat_rad <- radCor(lsatMeta, metaData = metaData, method = \"rad\")\n\n#terra::writeRaster(lsat_rad, datatype=\"FLT4S\", filename = \"prac_3/Lsatdata8/lsat_rad.tif\", format = \"GTiff\", overwrite=TRUE)\n\n\nhazeDN    <- RStoolbox::estimateHaze(lsat, hazeBands = 2:4, darkProp = 0.01, plot = TRUE)\n\nlsat_sref <- radCor(lsatMeta, metaData = metaData, method = \"dos\", \n                    hazeValues = hazeDN, hazeBands = 2:4)\n\nhttps://rpubs.com/delViento/atm_corr\n\nAs noted in the lecture there are a wide range of more sophisticated methods (beyond Dark Object Subtraction) to convert raw Digital Numbers or radiance to surface reflectance.\nWhilst this is a bit beyond the scope of this module, if you look again at the Landsat 7 Data Users Handbook you will see a radiance to reflectance calculation that can be used…“For relatively clear Landsat scenes”:\n\\[\\rho_{\\rho}= \\frac{\\pi* L_{\\lambda} * d ^2}{ESUN_{\\lambda} * cos\\theta_S}\\]\nand…we’ve seen all these values in the DOS formula, except \\(\\rho_{\\rho}\\) which is Unitless planetary reflectance\n…this method is still used in current research too, for example it’s listed in this paper of Land Surface Temperature retrieval by Sekertekin and Bonafonu, 2020. See Appendix C for Landsat 5, 7 (that use the above equation) and Landsat 8, that uses this slight variation…\n\\[\\rho_{\\rho}= \\frac{M_{p}* Q_{CAL} + A_p}{sin\\theta_{SE}}\\]\nWhere…\n\n\\(M_p\\) is the band-specific multiplicative rescaling factor from the metadata\n\\(A_p\\) is the band-specific additive rescaling factor from the metadata\n\\(QCAL\\) is the digital number\n\\(\\theta_{SE}\\) is the sun elevation angle from the metadata file.\n\nAlthough it’s worth noting that this is Top of Atmosphere reflectance (TOA).\nRadiance is how much light the sensor sees.\nReflectance is the ratio of light leaving the target to amount striking the target. Here will still have atmopsheric effects in the way of our true apparent reflectance. Confusingly all of these can be termed reflectance and indeed sometimes radiance is referred to as reflectance.\nTOA reflectance changes the data from what the sensor sees to the ratio of light leaving compared to striking it. BUT, the atmosphere is still present. If we remove the atmopshere we have apparent reflectance (sometimes called Bottom of Atmosphere reflectance). DOS gives us a version of apparent reflectance."
  },
  {
    "objectID": "5_GEE_I.html#exporting",
    "href": "5_GEE_I.html#exporting",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.9 Exporting",
    "text": "5.9 Exporting\n\n// Export a cloud-optimized GeoTIFF.\nExport.image.toDrive({\n  image: landsat,\n  description: 'imageToCOGeoTiffExample',\n  fileFormat: 'GeoTIFF',\n  formatOptions: {\n    cloudOptimized: true\n  }\n});\n\n\nI can var img = ee.Image(‘COPERNICUS/S2_SR/20210109T185751_20210109T185931_T10SEG’);\n\nvar dataset = ee.Image(‘LANDFIRE/Fire/MFRI/v1_2_0/CONUS’);\nvar combined_img = ee.ImageCollection([img_1, img_2]).mosaic();\n\n5.9.1 Review questions"
  },
  {
    "objectID": "5_GEE_I.html#clip-images",
    "href": "5_GEE_I.html#clip-images",
    "title": "\n5  Google Earth Engine I\n",
    "section": "\n5.7 Clip images",
    "text": "5.7 Clip images\n\n// do the clip\nvar clip = meanImage.clip(india);\n\n// map the layer\nMap.addLayer(clip, vis_params2, 'clip');"
  }
]