[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "CASA0023 Remotely Sensing Cities and Environments",
    "section": "Welcome",
    "text": "Welcome\nHello  and welcome to the Term 2 module Remotely Sensing Cities and Environments.\nSimilarly, to my Term 1 MSc module, CASA0005, this website holds all the practical instructions for the module. CASA0005 Geographic Information Systems and Science (or a similar module) is a pre-requisite of this module so concepts taught there will mainly be assumed here."
  },
  {
    "objectID": "index.html#acknowledgement",
    "href": "index.html#acknowledgement",
    "title": "CASA0023 Remotely Sensing Cities and Environments",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThanks to the following academics who inspired the creating of the module and various concepts within it:\n\nDr Ellie Biggs\nDr Gareth Roberts\nDr Bryan Boruff\nProfessor Ted Milton\nDr Laurie Chisholm\n\nThanks again to the following people who have either contributed directly or provided code in repositories that have helped me style this book:\n\n\n\nR Studio\n\nSTAT 545\nrstudio4edu\nHadley Wickham\nAlison Presmanes Hill\nDesirÃ©e De Leon\nYihui Xie\nJulia Silge\nJenny Bryan\n\n\n\nOthers\n\nRobin Lovelace\nTwitter for R programmers\nMatt Ng\nStatQuest with Josh Starmer\nGarrick Adenâ€‘Buie\n\n\n\n\nThe R package and analysis artwork used within this book has been produced by allison_horst, whilst artwork used in information boxes has been produced by DesirÃ©e De Leon. You can find Allisonâ€™s images on the stats illustration GitHub repository and DesirÃ©eâ€™s on the rstudio4edu GitHub repository.\nIâ€™ve certainly learnt a lot from their open code repositories!"
  },
  {
    "objectID": "intro.html#learning-outcomes",
    "href": "intro.html#learning-outcomes",
    "title": "1Â  Getting started with remote sensing",
    "section": "1.1 Learning outcomes",
    "text": "1.1 Learning outcomes\nBy the end of this practical you should be able to:\n\nSource, load and articulate the differences between Landsat and Sentinel data\nUndertake basic raster image statistics and processing\nEvaluate the (dis)advantages of each type of software you have used\nPull out and statistically compare spectral signatures"
  },
  {
    "objectID": "intro.html#gitignore",
    "href": "intro.html#gitignore",
    "title": "1Â  Getting started with remote sensing",
    "section": "1.2 .gitignore",
    "text": "1.2 .gitignore\nIf you are using Git with your project and have large data itâ€™s best to set up a .gitignore file. This file tells git to ignore certain files so they arenâ€™t in your local repository that is then pushed to GitHub. If you try and push large files to GitHub you will get an error and could run into lots of problems, trust me, iâ€™ve been there. Look at my .gitignore for this repository and you will notice:\n/prac_1/*\nThis means ignore all files within these folders, all is denoted by the *. The .gitignore has to be in your main project folder, if you have made a a git repo for your project then you should have a .gitignore so all you have to do is open it and add the folder or file paths you wish to ignore.\nGitHub has a maximum file upload size of 50mbâ€¦.\n\n\n\n\n\nGitHub maximum file upload of 50mb. Source: reddit r/ProgrammerHumor\n\n\n\n\n\n1.2.1 Sentinel download\n\nGo to the Copernicus Open Access Hub\nYou will need to make a free account\n\nDraw around the study area\nSelect image filter criteria\nSelect search icon\nDownload the S2MSI2A data\n\n\n\n\n\n\n\n\n\n\n\nWithin this image we have:\n\nAllows you to either move around the global or draw a study area\nAn example of a study area\nHow to sort the image results\nSentinel mission selection :\n\n\nSentinel product types refer to the amount of processing that has been undertaken on the multi-spectral imagery. S2MSI2A = Bottom of Atmosphere (BOA) or otherwise known as surface reflectance. Consult the product specification for more details.\n\nYou can also use the Sentinel 2 toolbox to replicate the conversation from TOA to BOA\n\nThe platform refers to either sentinel 2A or 2B. Theses are the same sensors but they operate at 180 degrees from each other reducing the revisit time from 10 to 5 days. 2A was launched first in 2015 followed by 2B in 2017.\nCloud cover e.g.Â [0 TO 5]\n\n_ Mu image was from the 15/5/2022\n\n1.2.1.1 Open\nOnce downloaded and unzipped youâ€™ll be presented with a load of folders! Here we are interested in the 10m bandsâ€¦which are:\n\n\n\n\n\n\n\n\n\nYouâ€™ll find then in the GRANULE > sensor number > IMG_DATA > R10.\nNext open them up in QGIS for some exploration, if there is a TCI image this is a True Colour Image of B02 (Blue), B03 (Green), and B04 (Red) Bands - open this first, itâ€™s just a single raster layer. See the Sentinel user guide definitions for any other acronyms you might need.\n\n\n\n\n\n\n\n\n\nUsing the Identify tool we can create a spectral signature by changing the view option to graph (look under the graph in the image above).\nHowever the TCI values are coded between 0 and 255 which will limit what we can do with it. As the the radiometric resolution of Sentinel-2 is 12-bit, meaning brightness levels from 0 - 4095 itâ€™s not clear how this product has been made.\nSo we can make our own raster stack using the BOA bands, if you recall we did this in CASA0005 in R.\nBut in QGIS itâ€™s easier to visualise the output\nFirst load the four 10M bands from the same folder e.gâ€¦T34HBH_20220515T081611_B02_10m\nFind the merge tool from the Raster miscellaneous tool box and select the following:\n\n\n\n\n\n\n\n\n\nNote:\n\nThe input layers are the four 10m bands that I loaded into QGIS\nThe tick box selected meaning each raster file will be itâ€™s own layer\nThe file being saved into a .tiff as opposed to memory\n\nOnce merged we can created a true colour composite using the BOA dataâ€¦to do so:\n\nRight click on the merged layer in the attribute table\nSymbology > Render type > select multiband color\n\nIn remote sensing the Red, Green, Blue display options are often called colour guns that are used to display images on the screen. For a true colour composite B1=Blue, B2=Green, B3=Red.\n\n\n\n\n\n\n\n\n\nTry changing the contrast enhancement and see what happens, then consult GIS stack exchange to understand what is happening..\nOf course we have only used the 10m bands so farâ€¦there are two options that we can take to use the full range of spectral data:\n\nDownscale the other bands to 10m forming a super-resolution\nUpscale the 10m to 20m\n\nDown scaling is quite an intensive process and beyond the scope of this practical. However, it can be achieved using Sen2Res that is another plug in for the SNAP toolbox. Arguably SNAP is just a difference type of GIS software specific to Sentinel, but we will explore it later as it makes some of these concepts easier to understand.\nUpscaling aggregates the images to a more coarse resolution (e.g.Â 10m to 20m). The Sentinel user guide states that bands will be resampled and provided (e.g.Â within the 20m folder there are 10 bands). However, itâ€™s not clear what method has been used, the documentation suggests nearest neighbour - https://docs.sentinel-hub.com/api/latest/data/sentinel-2-l2a/.\nThis can also be termed resampling, and nearest neighbour simply means taking the new raster resolution and then assiging the value from the closest value in the original data. Others approaches include bilinear or cubic convolution resampling.\n\n\n1.2.1.2 SNAP\nSNAP stands for Sentinels Application Platform it is a collection of toolboxes specifically for pre-processing and analysing remotely sensed data.\nDownload the all toolboxes version\nSNAP allows us to easily do undertake many GIS raster type analysis that weâ€™ve seen / discussed in other modules (like CASA0005) and that we will come across within this module including:\n\nre sampling raster data\nre projecting\nmasking\nclassifying data\nprincipal component analysis\northorectification\nmany more methods!\n\nAside from these methods in a GUI the real benefit is that itâ€™s made to use remotely sensed data. Letâ€™s explore some features of SNAP.\n\n1.2.1.2.1 Load data\n\nFile > Open Product > select the .zip that was downloaded. Do not rename it or unzip it before hand!\nIn the side bar under Product Explorer there will be a lot on data that you can load. The remotely sensed data in under Bands and then each band is listed (e.g.Â B1 443nm). What does 443nm mean?\nDouble right click on a layer and it will appear in the viewer area\n\n\n\n1.2.1.2.2 SNAP layout\nThe layout of SNAP isnâ€™t too different to QGIS, we have:\n\nProducts on the left side bar\nMap info in the bottom left\n\nadditional panes of info can be added through view >toolbar windows\nnote i have added pixel info (updates when moving the cursor) and world map\n\nLayer / mask managers in the right sidebar\nProcessing tools in the top tool bars\n\n\n\n\n\n\n\n\n\n\nHere, i have also the two link buttons selected (bottom left window) these mean that if i move to another band i will still be in the same position on the image.\n\n\n1.2.1.2.3 Colour composites\nTo re-created a true colour composite, right click on the data product and open RGB image\n\n\n\n\n\n\n\n\n\nThere are a variety of other band combinations that we can use to show certain aspects of Earthâ€™s surface based on the absorption and reflection properties of the materials in the wavelengths of the bands we display â€œthrough the colour gunsâ€, for example:\n\nThe false colour composite: B8, B4, B3. Plants reflect near-infrared and green light whilst absorbing redâ€¦.\nAtmospheric penetration composite: B12, B11, B8A with no visible bands to penetrate atmospheric particles. Vegetation = blue, urban area = white, gray cyan or purple.\nThe short-wave infrared composite: B12, B8A and B4 shows vegetation in green, the darker the greener the denser it is and brown shows built up or bare soil\n\nFor other band colour combinations consult gisgeogrpahy\n\n\n1.2.1.2.4 Creating a project\nBefore we go any further we should create a project, that way all our processed outputs will be stored within the projected and loaded again when we open it in future - like projects in any other software such as QGIS or R. Go File > Project > Save project. A project tab next to the product explorer tab will appear, but not you canâ€™t access the bands from within the project that must be done through the product explorer.\n\n\n1.2.1.2.5 Image statistics\n\n\n1.2.1.2.6 Image histogram\nWhen we open an RGB image in SNAP the histogram is clipped by 1% at the lower end and 4% at the upper end and then mapped in bins between 0 and 255 for display. A computer screen in full RGB displays colours between 0 and 255, hence why this is done.\nThis is similar to the contrast enhancement we saw in QGIS and we can manually change it here again through View > Tool Windows > Colour Manipulation.\nChanging the distribution displayed will impact the colour of the image - try using the sliders or selecting 95% and 100% of all pixels to display. To reset click the back arrow button.\n\n\n\n\n\n\n\n\n\n\n1.2.1.2.6.1 Scatterplots\nUnder the analysis button there are a variety of tools we can use to explore some image statisticsâ€¦.for example, here i have created a scatter plot of band 4 (x axis) and band 8 ( y axis). These bands are the red (vegetation absorbs) and Near-infrared (NIR, that vegetation strongly reflects)â€¦so where we have high values of NIR and low values of red the plot represents dense vegetation whilst low values of both red and NIR are usually wet bare soil. you need to click the refresh button to generate the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Remote Sensing 4113\n\n\n\n\nIn remote sensing this can be called â€œspectral feature spaceâ€, more on this later in the term. You might see the software ENVI in the source above - ENVI is propriety software that is similar to SNAP.\n\n\n1.2.1.2.6.2 Tasseled Caps\nThe output should look somewhat like a â€œTasseled Capâ€ (a wizards hat at an angle), although this is different to the tasseled cap transformation which was proposed by Kauth and Thomas (1976). The tasseled cap transformation was originally applied to Landsat data being composed of brightness, greeness, yellow stuff (yes!) and none-such. It was then modified in 1968 to brightness, greeness and wetness. This can can useful for identifying urban areas, they are usually bright (although Andy will have more to say on this), high biomass will show in the greeness and moisture in the wetness. The tasseled cap comes from the plot between brightness on the x axis and greenness on the y axis.\nTraditionally this is usually only applied to Landsat data, however if our bands in other sensors (like Sentinel) cover the same wavelengths we can apply itâ€¦ \\[\n\\begin{split}\nBrightness = 0.3037(B2)+0.2793(B3)+\\\\0.4743(B4)+0.5585(B8)+\\\\\n0.5082(B11)+0.1863(B12)\n\\end{split}\n\\] \\[\n\\begin{split}\nGreeness = âˆ’0.2848(B2)âˆ’0.2435(B3)\\\\âˆ’0.5436(B4)+0.7243(B8)+\\\\\n0.0840(B11)âˆ’0.1800(B12)\\end{split}\n\\] \\[\n\\begin{split}\nWetness = 0.1509(B2)+0.1973(B3)\\\\+0.3279(B4)+0.3406(B8)\\\\âˆ’\n0.7112(B11)âˆ’0.4572(B12)\n\\end{split}\n\\]\nNow, this is fairly straight forward to do in SNAP (or R) using Band Maths (Raster > Band Maths) and then clicking edit expression, before we can apply this we have two problems:\n\nB11 and B12 are at a 20 meter resolution where as all the others at a 10 meter resolution, to fix this we must re-sample to 20m\nUsing the image provided will mean that the entire tile is computed when we really only care about our study area\n\nNote, that there are many spatial indexes that can be applied to remotely sensed data, the Index DataBase holds them all or Chapter 8, pages 325 from Jensen is a good place to start.\n\n\n\n1.2.1.2.7 Masking and resampling\nThis is sometimes also called clipping in CASA0005 we saw cropping (to the extent of a polygon) and then masking. Here I am only interested in a study area of the City of Cape Town District Municipality, which is in the gadm40_ZAF_2 from the GADM data, it also appears in the gadm40_ZAF_3 data too!\nExplore the GADM data in QGIS to work out what spatial aggregation you can use to mask your study area. For the sake of this example you might want to use the smallest spatial unit, GADM level 4, which for South Africa is wards.\nSNAP only permits ESRI shapefile to be loaded! To do so you must select the product on interest in the product explorer > Vector > Import > ESRI Shapefile\nWhen you load the vector file it will appear under the image product that you had selected. To show it on the map, move to the right of the screen > select layer manager > check the layer you want to show.\n\n\n\n\n\n\n\n\n\nWhen you open a Shapefile in SNAP each polygon within your current extent will be made into an individual feature, opening the feature (from the Vector Data folder) will open the attribute table for the row of that polygon. You also might notice that the shapefile may have a larger extent than the image, this means weâ€™d need to find another image and â€œmosaicâ€ (merge or join together) them but we will see this later in the module, for now just accept that some area might be missing.\nIn my case the City of Capetown has been called ZAF_1 and now i will clip the raster to the polygon. However, the problem here is that we can only mask bands on the same spatial resolution, so we need to re sample bands 2,3,4 and 8 to 20mâ€¦here, we can use the Sentinel 2 resampling toolbox to resample the image and then move to masking it.\nThere are two options to resample within SNAP, a traditional resample which just considers the neighbouring pixels or the Sentinel 2 products resample to account for the particularities of the angle (satellite viewing) bands. For the sake of time we will use the traditional resample to generate a 20m raster datasetâ€¦Raster > Geometric > Resamplingâ€¦\n\n\n\n\n\n\n\n\n\nNote that there will be both upsampling and downsampling here as iâ€™ve selected a 20m output.\nThis will create a new product in the product explorer so make sure you use that from now we, next we can mask out the all the bands we need:\nTo mask out our study area use the Land/Sea mask (Raster > Masks > Land/Sea mask) and select the vector to use as the mask. Here, i only take forward the bands i need (scroll up to check):\n\n\n\n\n\n\n\n\n\nAt this stage you may want to remove the shapefile that will still be within the new product created. Under the vector folder remove the relevant polygons.\nThen compute the tasseled cap transformation (Raster > Band Maths) with the equations above. Note that when using a subtraction (-) you might get invalid expression so use the inbuilt subtraction within the edit expression button. My brightness equation was:\n\n(0.3037* $3.B2) + (0.2793 * $3.B3) + (0.4743 * $3.B4) + (0.5585 * $3.B8) + (0.5082 * 0.5082) + (0.1863 * 0.1863)\n\nThe new layers weâ€™ve created will be added to the bands folder of the product you had selected (remember that we can only do this with bands that have the same resolution, hence our up and down sampling). We can now display the bands through the colour guns, R: Brightness, G: Greeness, B: Wetness and also create a scatter plot between Greeness (y) and Brightness (x) like we did earlier.\nAt this stage it is useful to also have open a true colour composite to compare to our tasseled cap transformation RBG imageâ€¦what do the values and colours show? The Tasseled Cap function from ArcPro will help explain this further.\n\n\n?(caption)\n\n\n\n\n\n\nin the caption\n\n\n\n\n\n\n\n\n\n1.2.2 Landsat\nLandsat imagery is the longest free temporal image repository of consistent medium resolution data. It collects data at each point on Earth each every 16 days (temporal resolution) in a raster grid composed of 30 by 30 m cells (spatial resolution). Geographical analysis and concepts are becoming ever more entwined with remote sensing and Earth observation.\nTo access the Landsat data we will use in this practical you can need to sign up for a free account at: https://earthexplorer.usgs.gov/.\nTo download the data:\n\nEnter your city in the address/place box > select the country > click Show and then click on the word of your city in the box that appears. ::: {.cell layout-align=â€œcenterâ€} ::: {.cell-output-display}  ::: :::\n\nOnce downloaded unzip the file.\nIn SNAP go File > Open Product, navigate to the .MTL and open it. The product should appear. This is a real benefit of SNAP, having data from multiple sensors in the same software that can be explored together - when you move around on the Landsat image then the Sentinel image will also move to the same location.\nTo see the bands (or RGB images) side by side use the window dividers in the top right and also ensure the views are linked in the bottom left window.\n\n\n\n\n\n\n\n\n\nFor the next part of the practical we will compare the spectral signatures from both Sentinel and Landsat. To do so we need to generate a series of points of interest (called POIs) that are coincident in both images. First we need to do two things:\n\nClip all the spectral data from the Sentinel image to the vector outline\n\nwe will need to resample the Sentinel data to the same pixel size so consider what you want to do here\n\nupscale or downscale ?\nWhat bands over Sentinel and Landsat actually overlap, do you need all of them\n\nwhat resolution is useful\n\n\nClip the Landsat image to the same vector outline as the Sentinel image, during the mask you can select the bands to mask and output\n\nOnce you have completed these steps you output should look something like this, however, note that my Landsat tile is a different extent to my Sentinel tile, we cover this later in the module.\n\n\n\n\n\n\n\n\n\n\n1.2.2.1 Select POIs\n\nTo create points of interest we want select pixels representative of certain land cover types (e.g.Â Bare earth, water, grass, forest, urban) so we can compare them from both Landsat and Sentinel. Of course the pixels need to be present in both datasets.\nWe can select pixels through QGIS, R or SNAP. Iâ€™ll focus on the latter here.\nMake a new vector data container for a land cover type (Vector > New Vector Data Container). Note that it will be created within the data product that is selected. Next click the square icon with a plus to add vectors. Once finished repeat the process of adding a new vector data container for the next land cover type.\n\n\n\n\n\n\n\n\n\n\n\nNext use a drawing tool to draw around areas of land cover, when you select the drawing tool you will be asked which landcover the polygon is for, make sure the area is in both imagery - switch between them as we have done or display them side by side. Whilst doing this it is useful to have the spectrum view open (View > Tool Windows > Optical >Spectrum Viewer:\n\n\n\n\n\n\n\n\n\n\n\nOnce done create another vector data container and repeat for the next land cover type.\nConsider have a separate class for highly reflective urban (e.g.Â industrial areas)\n\n\n\n\n1.2.3 Spectral signatures\nTo compare out spectral signatures we have a few options\n\nright click on the image in SNAP and then export mask pixels as a .txt, but you will need to do this for each land cover class and for each image (Sentinel and Landsat).\n\nor\n\nexport the imagery (to Geotiff) and vector files (to shapefiles) and then subset in R\n\nI will do the second option here:\n\nRight clicking on the vector containers will allow export to shapefile - do this for all the land covers you have\nTo export the â€œproductâ€ which is what your image will be un SNAP. Select it (just click it) then go File > Export > GeoTIFF. When exporting the Landsat data click the subset button in the save box > band subset (select only spectral bands) and then metadata subset and remove all the selection.\nOpen the GeoTIFFS and shapefiles in QGIS to check everything.This is another good opportunity to understand how contrast enhancement works - in QGIS > Symbology > expand min / max and then change the cumulative count cut the value for each band will change. For Landsat B3 is red, B2 is green and B1 is blue. This does not change the values of the pixels, just how they are displayed on a colour screen.\n\n\n\n1.2.4 Using R\nRemember that before we deal with large data sets in R, we must sort out the .gitignore file, if we intend to use Git and GitHub at any stage (hint: you will for the assessment). Go back to CASA0005 to remember how if you need to\nRaster packages in R have and are still evolving rapidly, we have probably seen and used the raster package and may have seen the terra or stars packages. All can handle raster data and do the same analysis (pretty much).\nIn 2023 some key packages will retire like maptools, rgdal and rgeos as their maintainer, Roger Bivand will retire. raster uses sp objects for vector data and also the rgdal package - the Geospatial Data Abstraction Library (GDAL) for reading, writing and converting between spatial formats. sp (that we saw in CASA0005) also uses rgdal and suggests rgeos.\nThe terra package (2020) is somewhat mixed with raster, however the retiring packages are only needed by raster, with the terra package replacing raster. Terra is much faster than raster as datasets that canâ€™t be loaded into RAM are stored on disc and when loaded doesnâ€™t read the values. Then when computations occur the do so in chunks. Terra is very well documented and the functions are very similar to raster - https://rspatial.org/terra/pkg/index.html\nThe stars package (2018) works with sf!!, this means many of the functions weâ€™ve seen and are familiar with it will work - e.g.Â st_transform() which is much easier than doing it in raster. The real benefit of stars is its ability to handle large data (like satellite data) that canâ€™t fit into memory. It does this through not loading the pixel values but keeping a reference to them through a proxy and only loading / running computations when specifically needed (e.g.Â plotting, will only plot the pixels that can be seen)! The stars section from Spatial Data Science in Pebesma and Bivnad 2022 gives a good overview with examples\nStars is faster than terra, terra is faster than raster.\n\n1.2.4.1 Data loading\n\nlibrary(sf)\nlibrary(terra)\nlibrary(raster)\n\n\nbare_earth <- st_read(\"prac_1/Data/Bare_earth_Polygon.shp\")\ngrass <- st_read(\"prac_1/Data/Grass_Polygon.shp\")\nforest <- st_read(\"prac_1/Data/Forest_Polygon.shp\")\nurban <- st_read(\"prac_1/Data/Low_urban_Polygon.shp\")\nhigh_urban <- st_read(\"prac_1/Data/High_urban_Polygon.shp\")\n\n\n#Landsat equivalent\nbands <- c(\"1\",\"2\", \"3\", \"4\", \"5\", \"6\", \"7\")\n\nsentinel <- rast(\"prac_1/Data/S2A_LSAT_msk.tif\")\n\nnames(sentinel) <- bands\n  \nlandsat<-rast(\"prac_1/Data/LSAT_msk.tif\")\n\nnames(landsat) <- bands\n\nHere i have chosen to use terra, this is because i want to extract the pixel values from within the polygons and at the moment stars will only permit aggregation - e.g.Â the mean of the pixels in the polygons. This is fine unless you want to explore the variation in signatures! then you will need the values.\nTo use a vector layer in terra it needs to be in a SpatVector (in terra the raster is a SpatRaster). Now there are two ways we can do thisâ€¦the first is with the vect() function from terra:\n\nurban <- vect(urban)\n\nIn the second we can actually convert the SpatRaster to a raster and then just use the sf object! I know, i know, how many formats do we need here! But this will mean we have a raster brick as we have all of our bands and a raster brick wonâ€™t work here.\n\nlandsat <- as(landsat, \"Raster\")\n\nOf course, we can just do this straight from loading the data with the pipeâ€¦\n\nbare_earth <- st_read(\"prac_1/Data/Bare_earth_Polygon.shp\") %>%\n  vect()\ngrass <- st_read(\"prac_1/Data/Grass_Polygon.shp\")%>%\n    vect()\nforest <- st_read(\"prac_1/Data/Forest_Polygon.shp\")%>%\n    vect()\nurban <- st_read(\"prac_1/Data/Low_urban_Polygon.shp\")%>%\n    vect()\nhigh_urban <- st_read(\"prac_1/Data/High_urban_Polygon.shp\")%>%\n    vect()\n\nBefore we can start the extraction you might have seen that my Landsat data is not in the same CRS as the rest of the data, itâ€™s close, but different. We can also just use the raster to get the CRS info, like\n\ncrs(landsat)\ncrs(sentinel)\n\n# reproject landsat\nlandsat <- project(landsat, sentinel)\n\nNow letâ€™s pull out those values, and get the mean and standard deviation, starting with urban from sentinel dataâ€¦\n\nlibrary(tidyverse)\nsen_urban<- terra::extract(sentinel, urban, progress = F)%>%\n  as_tibble()%>%\n  pivot_longer(cols = 2:7, \n               names_to=\"bands\", \n               values_to=\"band_values\")%>%\n  add_column(sensor=\"sentinel\")%>%\n  add_column(land=\"urban\")\n\nNow, because this process will be the same for the other landcover types we can make a functionâ€¦this is the exact same code as above but iâ€™ve replaced two arguments with sensor and lancover that i can now change for each version (e.g.Â bare earth in Landsat or forest in Sentinel)\n\nband_fun <- function(sensor, landcover) {\n  col_sensor <- deparse(substitute(sensor))\n  col_land <- deparse(substitute(landcover))\n\n  sen_urban<- terra::extract(sensor, landcover, progress = F)%>%\n    as_tibble()%>%\n    pivot_longer(cols = 2:7, \n               names_to=\"bands\", \n               values_to=\"band_values\")%>%\n    add_column(sensor=col_sensor)%>%\n    add_column(land=col_land)\n                 \n}\n\nLater on we will also create a density plot, so letâ€™s write a function for all the valuesâ€¦itâ€™s very similarâ€¦we will call this one when we need it laterâ€¦\n\nband_fun_all_values <- function(sensor, landcover) {\n  col_sensor <- deparse(substitute(sensor))\n  col_land <- deparse(substitute(landcover))\n\n  sen_urban<- terra::extract(sensor, landcover, progress = F)%>%\n    as_tibble()%>%\n    pivot_longer(cols = 2:7, \n               names_to=\"bands\", \n               values_to=\"band_values\")\n                 \n}\n\nThen itâ€™s simplyâ€¦\n\nsen_bare <- band_fun(sentinel, bare_earth)\nsen_grass<- band_fun(sentinel, grass) \nsen_forest<- band_fun(sentinel, forest) \nsen_high_urban <- band_fun(sentinel, high_urban) \n\nlsat_urban<- band_fun(landsat, urban)\nlsat_bare<- band_fun(landsat, bare_earth)\nlsat_grass<- band_fun(landsat, grass)\nlsat_forest<- band_fun(landsat, forest)\nlsat_high_urban <- band_fun(sentinel, high_urban) \n\nThink about what this has given us?\nThe next stage is to put then into a tibbleâ€¦\n\nsen_lsat <- bind_rows(sen_urban, sen_bare, sen_grass,\n                      sen_forest, sen_high_urban,\n                      lsat_urban, lsat_bare, lsat_grass,\n                      lsat_forest, lsat_high_urban)\n\nThe next stage is to get the mean (and standard deviation) values for each band per sensor and land cover type:\n\nmeans<- sen_lsat%>%\n  group_by(bands, sensor, land)%>%\n  summarise(Mean=mean(band_values), Std=sd(band_values))\n\nPlot some spectral profiles, first for Sentinelâ€¦\n\np1 <- means %>%\n  filter(sensor==\"sentinel\") %>%\n  ggplot(., aes(x = bands, y = Mean,\n                col=land))+\n  geom_point()+\n  geom_line(aes(group = land)) +\n  geom_errorbar(aes(ymin = (Mean-Std), ymax = (Mean+Std), width = 0.2))\np1\n\n\n\n\nWe can also look at a density plotâ€¦.ideally each land cover has a clear and separate historgramâ€¦\n\np2 <- sen_lsat %>%\n  filter(sensor==\"sentinel\") %>%\nggplot(., aes(x=band_values, group=land, fill=land)) + \n  geom_density(alpha = 0.6)+\n#Add a mean vertical line\n  geom_vline(data = . %>% group_by(land) %>% summarise(group_mean = mean(band_values)),\n             aes(xintercept=group_mean, color = land), linetype=\"dashed\", size=1)\n\np2\n\n\n\n\nThanks to Sydney Goldstein for some of this code\n\n\n\n\nRemember this mean vertical line is for all values per land cover, itâ€™s not band specific.\nAndy will talk about these outputs.\nWe can arrange our grids nicely with the cowplot packageâ€¦\n\nlibrary(cowplot)\n\noutput<-plot_grid(p1, p2, labels = c('A', 'B'), label_size = 12, ncol=1)\n\noutput\n\n\n\n\nWe can also export this if we wanted with ggsave() to,ggsave has a lot of options and can output to most formats, it also gives examples of not using ggsave\n\nggsave(\"spectral_reflectance.pdf\", width = 20, height = 20, units = \"cm\")\n\nHere, i have just shown the process for Sentinel data. Your task is to respond to the review questions which will include doing a bit similar analysis of the Landsat data / some comparisonsâ€¦perhaps even a statistical test (e.g.Â a t-test) to compare the difference between the reflectance of Landsat and Sentinel data e.g..\n\nt1<- sen_lsat %>%\n           filter(sensor==\"sentinel\" & land ==\"urban\")%>%\n           select(band_values)%>%\n           pull()\n\nt2<- sen_lsat %>%\n           filter(sensor==\"landsat\" & land ==\"urban\")%>%\n           select(band_values)%>%\n           pull()\n\nt.test(t1, t2)\n\n\n    Welch Two Sample t-test\n\ndata:  t1 and t2\nt = -355.74, df = 5970.1, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -9300.307 -9198.368\nsample estimates:\nmean of x mean of y \n 2190.602 11439.939 \n\n\nHow do we interpret this?\nIf you make plots of reflectance be careful to use the same scale, you can do this by adding a new line under ggplot like:lims(y = c(0, 80000))\n\n\n\n1.2.5 Review questions\nWrite around 2-3 paragraphs on the following and support your responses with plots and literature:\n\nBriefly explain the process of sourcing, loading and manipulating remotely sensed datasets\nConsider the assumption made in comparing the spectral reflectance from Sentinel and Landsat\nReflect on why differences in the reflectance might occur between sensors."
  },
  {
    "objectID": "2_portfolio.html#learning-outcomes",
    "href": "2_portfolio.html#learning-outcomes",
    "title": "2Â  Portfolio",
    "section": "2.1 Learning outcomes",
    "text": "2.1 Learning outcomes\nBy the end of this week / the practicals you should be able to:\n\nDemonstrate appropriate syntax and file structure for Xaringan and Bookdown / Quarto\nCreate presentations and books appropriate to the module assignment requirements\nShare your resources online through Git and GitHub\n\nA lot of the concepts have been covered in the lecture (more like a tutorial this week), here some key notes are provided to support and demonstrate the code for creating the slides and book. From using RMarkdown in CASA0005, this should be a little familiar."
  },
  {
    "objectID": "2_portfolio.html#xaringan",
    "href": "2_portfolio.html#xaringan",
    "title": "2Â  Portfolio",
    "section": "2.2 Xaringan",
    "text": "2.2 Xaringan\nðŸ™Š shar-in-gen\n\nInstall / load the packages\n\n\nremotes::install_github(\"yihui/xaringan\")\nlibrary(xaringanExtra)\n\n\n2.2.1 Template:\nFile -> New File -> R Markdown -> From Template -> Ninja Presentation\nThe loaded template and lecture had a lot of info, but to get started you need to know the following tools:\n\n\n2.2.2 Themes\nTo change how the RMarkdown renders you can select a different themeâ€¦just change the css argument in the preambleâ€¦currently it will beâ€¦\ncss: [default, metropolis, metropolis-fonts]\nTo see other themes:\nnames(xaringan:::list_css())\nOr, use the package xaringanthemer. Simply load the package, have a code chunk at the start of the presentation like thisâ€¦\nChange the preamble to * css: [\"xaringan-themer.css\"] * highlightStyle: solarized-dark or which ever style you selected.\nSee the Xaringan CSS Theme Generator site for more details.\n\n\n2.2.3 Slide controls:\n\nAdd a new slide = ---\nAdd a click to open the next part of a slide = --\nAdd a flipped slide (colours reversed and message in centre) = class: inverse, center, middle (after the ---)\n\n\n\n2.2.4 Headings / lists:\n\n#, ## and so on for sub headings\nLists use * or 1.First item and 1.Second item, it will know to list them 1 and then 2.\n\n\n\n2.2.5 Slide sides (e.g.Â like power point)\n.pull-left[To have things on the left]\n.pull-right[To have things on the right]\n\n\n2.2.6 Images\n\nknitr::include_graphics('img/Lena-river.jpg')`\n\nto control the figure you can specify options within the chunk e.g.\n{r  echo=FALSE, out.width='60%', fig.align='center'}\n\n\n2.2.7 Equations\nTo use an equation itâ€™s just placing the values / text in an opening $$ and closing $$e.g.\n$$NDVI= \\frac{NIR-Red}{NIR+Red}$$\nYou may also want to split equations if they are rather longâ€¦with \\begin{split} and \\\\ where you want the splits and then \\end{split}\n\\[\\begin{split}\nWetness = 0.1509(B2)+0.1973(B3)\\\\+0.3279(B4)+0.3406(B8)\\\\âˆ’\n0.7112(B11)âˆ’0.4572(B12)\n\\end{split}\\]\n\n\n2.2.8 Preview\nLoad the add In â€œInfinite Moon Readerâ€ from Yihui Xie generates the slides every time you save the .Rmd. Tools > Addins.\n\n\n2.2.9 xaringanExtra\nxaringanExtra, developed by Garrick Aden-Buie is â€œa playground of enhancements and extensionsâ€..that are easy to add..for example, to add a search function in your slides just add..see xaringanExtra"
  },
  {
    "objectID": "2_portfolio.html#quarto",
    "href": "2_portfolio.html#quarto",
    "title": "2Â  Portfolio",
    "section": "2.3 Quarto",
    "text": "2.3 Quarto\nQuarto allows you to publish Python, R, Julia or Observable in a online book or presentation. To an extent it is an updated Bookdown package that the CASA0005 resources were made with, although Bookdown still exists Quarto makes it easier to incorporate different languages and from what i can tel publish to pdf.\nDownload Quarto: https://quarto.org/\nmake sure you have updated RStudio to at least v2022.02\nNext open a blank RStudio session > New Project > New Directory > Quarto Book\nTwo files will open:\n\nindex.qmd = the main landing page of the book\n_quarto.yml = configuration file\n\nIn the _quarto.yml file change the details to reflect your workbook, you will notice that the chapters are listed - these reference the other .qmds and need to be listed here in order to be rendered in the final book - each .qmd is a chapter.\nAt the bottom there is also the editor argument that can be changed to visual or source code, depending on how you want to edit the contents of each chapter.\nClick Render (in the tool bar at the top of the .qmd) to see what happens..\nThe syntax of Quarto, Xaringan and RMarkdown are the same, except you some features are specific to each packageâ€¦for example..\nTo have figures side by side in Quarto youâ€™d do:\n\n::: {#CHUNKNAME layout-ncol=2}\n\n![Fig name](FILE){#reference)\n\n![Fig name](FILE){#reference}\n\n:::\n\nWhere as in xaringan you might use .pull-left[]"
  },
  {
    "objectID": "2_portfolio.html#git-and-github",
    "href": "2_portfolio.html#git-and-github",
    "title": "2Â  Portfolio",
    "section": "2.4 Git and GitHub",
    "text": "2.4 Git and GitHub\nWe have seen the use of Git and GitHub in several other modules, so i wonâ€™t go into it here. For a refresher read over Git, GitHub and RMarkdown\nRemember donâ€™t upload large files use .gitignore\nOnce you have got your project set up with Git and GitHub:\n\nRender the presentation / book (if using Infinite Moon Reader this will be done for you / if using Quarto you can select render on save)\nAdd > Commit > Push to GitHub\nOn GitHub > Settings > Pages the under source\n\nFor Xaringan select /root\nFor Quarto select /docs\n\n\nThe page you are on will then provide a URL\n\n2.4.1 Review tasks\n\n2.4.1.1 Xaringan\nCreate a small 5 slide presentation and host it on GitHub - place the link in your Quarto portfolio\n\nSelect a sensor of your choice (any)\nCreate a short (maximum 5 slides) presentation on the sensor in xaringan\n\nThings to consider:\n\nWho runs the sensor\nWhat is the cost of the data\nWhere do you source the data\nResolutions (e.g.Â in a table)\nApplications of the dataâ€¦with an example\nImages of the data (not required to download it)\nAn example of the data being used within academic (or policy) literature\n\n\n\n2.4.1.2 Quarto\nFor the individual assignment of the module you are required to produce a online portfolio of independent study and response questions. In the practical sessions this week you should have created your online portfolio and added your response from week 1 to it. For week 2 you must add the URL of your small 5 slide presentation and also write 1 paragraph on the following:\n\nReflect on your experience using these reproducible presentation tools through considering the benefits and drawbacks."
  },
  {
    "objectID": "4_policy.html#before-the-practical-session",
    "href": "4_policy.html#before-the-practical-session",
    "title": "3Â  Policy",
    "section": "3.1 Before the practical session",
    "text": "3.1 Before the practical session\nFor the practical session this week come prepared to talk about the following:\n\nSearch for one metropolitan policy challenge (any city in the World) that could be solved by incorporating remotely sensed data\nIdentify and evaluate a remotely sensed data set that could used to assist with contributing to the policy goal\nDemonstrate how this links to global agendas / goals\nExplain how it advances current local, national or global approaches.\n\nCities will have a diverse range of documentation availableâ€¦"
  },
  {
    "objectID": "4_policy.html#after-the-practical-session",
    "href": "4_policy.html#after-the-practical-session",
    "title": "3Â  Policy",
    "section": "3.2 After the practical session",
    "text": "3.2 After the practical session\nFollowing the practical and subsequent discussion write up your case study city in three paragraphs, you should:\n\nDetail the relevant policy that can be assisted with remotely sensed data\nEvaluate how remotely sensed data set(s) could used to assist with contributing to the policy goal\nPlace it within local / global agendas and current approaches\nCite the relevant policy and where appropriate literature.\n\nShould you struggle to find current approaches within your city explore other cities discussed within the practical."
  },
  {
    "objectID": "4_policy.html#policies",
    "href": "4_policy.html#policies",
    "title": "3Â  Policy",
    "section": "3.3 Policies",
    "text": "3.3 Policies\nA few examples to get you started â€¦you are not limited to this list. We explore some of these in more detail within future lectures.\n\n3.3.1 Metropolitan\n\nCape Town Municipal Spatial Development Framework\nAhmedabad 2016 Heat Action Plan\nSouth Asiaâ€™s First Heat-Health Action Plan in Ahmedabad (Gujarat, India)\nOneNYC 2050\nLondon Plan\nPerth and Peel @ 3.5 million\n\n\n\n3.3.2 National\n\nSingapore Master Plan\n\n\n\n3.3.3 International\n\nC40 Cities\nUnited Nations New Urban Agenda\nARUP City Resilience Framework\nUnited Nations International Strategy for Disaster Reduction Sendai Framework\nUniversal Sustainable Development Goals\n\nThe SDG targets\n\nBeat the Heat Handbook"
  },
  {
    "objectID": "4_policy.html#example",
    "href": "4_policy.html#example",
    "title": "3Â  Policy",
    "section": "3.4 Example",
    "text": "3.4 Example\nFor a written example read up to the study area section in my paper on temperature mitigation (first 2 pages). Take note of table 1."
  },
  {
    "objectID": "6_classification_II.html",
    "href": "6_classification_II.html",
    "title": "4Â  Classification II",
    "section": "",
    "text": "4.0.1 Review questions"
  },
  {
    "objectID": "7_temperature.html#learning-objectives",
    "href": "7_temperature.html#learning-objectives",
    "title": "5Â  Temperature",
    "section": "5.1 Learning objectives",
    "text": "5.1 Learning objectives\nBy the end of this practical you should be able to:\n\nExplain and execute appropriate pre-processing steps of raster data\nReplicate published methodologies using raster data\nDesign new R code to undertake further analysis\n\nThis week:\n\nAppendix â€œRaster operations in Râ€ from Intro to GIS and Spatial Analysis by Gimond (2019)\nRaster manipulation from Spatial data science by Hijmans (2016). This last one is another tutorial â€” it seems there arenâ€™t any decent free raster textbook chapters, let me know if you find one.\n\nRemember this is just a starting point, explore the reading list, practical and lecture for more ideas."
  },
  {
    "objectID": "7_temperature.html#introduction",
    "href": "7_temperature.html#introduction",
    "title": "5Â  Temperature",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction\nWithin this practical we are going to be using data from the Landsat satellite series provided for free by the United States Geological Survey (USGS) to replicate published methods. Landsat imagery is the longest free temporal image repository of consistent medium resolution data. It collects data at each point on Earth each every 16 days (temporal resolution) in a raster grid composed of 30 by 30 m cells (spatial resolution). Geographical analysis and concepts are becoming ever more entwined with remote sensing and Earth observation."
  },
  {
    "objectID": "7_temperature.html#data",
    "href": "7_temperature.html#data",
    "title": "5Â  Temperature",
    "section": "5.3 Data",
    "text": "5.3 Data\n\n5.3.1 Shapefile\nThe shapefile of Manchester is available from the data folder for this week on GitHub. To download this consult [How to download data and files from GitHub], iâ€™d used Option 1.\n\n\n5.3.2 Raster data (Landsat)\nTo download the data itâ€™s the same process we saw in week 1 on the USGS Earth Explorer website:\n\nEnter Manchester in the address/place box > select the country as United Kingdom > click Show and then click on the word Manchester in the box that appears. ::: {.cell layout-align=â€œcenterâ€ hash=â€˜7_temperature_cache/html/unnamed-chunk-1_cb5a647eaa64b52fe7b26e86019bf6b4â€™} ::: {.cell-output-display}  ::: :::\nSelect the date range between the 12/5/2019 and 14/5/2019 â€” itâ€™s a US website so check the dates are correct.\nClick dataset and select Landsat, then Landsat Collection 1 Level-1, check Landsat 8 (level 2 is surface reflectance â€” see [Remote sensing background (optional)]\nClick results, there should be one image (GEOTiff), download it..it might take a while\nLandsat data comes zipped twice as a .tar.gz. Use 7Zip or another file extractor, extract it once to get to a .tar then extract again and files should appear. Or the code below will also let you extract Landsat dataâ€¦\n\n\n5.3.2.1 Alternative raster data\nOccasionally the earth explorer website can go down for maintenance or during government shutdowns. If possible I strongly advise you to learn how to use its interface as multiple other data providers have similar interfaces. GitHub also place a strict size limit on files of 100MB. However, in order to account for situations like this Iâ€™ve placed the zipped file on GoogleDrive and will demonstrate how to access this from R using the new googledrive package.\nThis could be a great option for you to gain reproducibility points if you have large files that you canâ€™t upload to GitHub.\nIn GoogleDrive you need to ensure your file is shareable with others â€” right click on it > Share > then copy the link. I have done this for my file in the example below, but if you try and replicate this, make sure youâ€™ve done it otherwise it might not work when other people try and run your code, as they wonâ€™t have access to the file on your GoogleDrive.\nDepending on your internet speed this example might take some timeâ€¦\nBe sure to change the path to your practical 7 folder but make sure you include the filename within it and set overwrite to T (or TRUE) if you are going to run this again. ::: {.cell hash=â€˜7_temperature_cache/html/unnamed-chunk-2_69162eac4836a4c80fe0349f6dce5bbeâ€™}\nlibrary(\"googledrive\")\n\no<-drive_download(\"https://drive.google.com/open?id=1MV7ym_LW3Pz3MxHrk-qErN1c_nR0NWXy\",\n                  path=\"prac_7/exampleGoogleDrivedata/LC08_L1TP_203023_20190513_20190521_01_T1.tar.gz\", \n                  overwrite=T)\n:::\nNext we need to uncompress and unzip the file with untar(), first list the files that end in the extension .gz then pass that to untar with the pipe %>% remember this basically means after this functionâ€¦ thenâ€¦do this other function with that data\n\nlibrary(tidyverse)\nlibrary(fs)\nlibrary(stringr)\nlibrary(utils)\n\nlistfiles<-dir_info(here::here(\"prac_7\", \"exampleGoogleDrivedata\")) %>%\n  dplyr::filter(str_detect(path, \".gz\")) %>%\n  dplyr::select(path)%>%\n  dplyr::pull()%>%\n  #print out the .gz file\n  print()%>%\n  as.character()%>%\n  utils::untar(exdir=here::here(\"prac_7\", \"exampleGoogleDrivedata\"))"
  },
  {
    "objectID": "7_temperature.html#processing-raster-data",
    "href": "7_temperature.html#processing-raster-data",
    "title": "5Â  Temperature",
    "section": "5.4 Processing raster data",
    "text": "5.4 Processing raster data\n\n5.4.1 Loading\nToday, we are going to be using a Landsat 8 raster of Manchester. The vector shape file for Manchester has been taken from an ESRI repository.\n\nLetâ€™s load the majority of packages we will need here.\n\n\n## listing all possible libraries that all presenters may need following each practical\nlibrary(sp)\n#library(raster)\nlibrary(rgeos)\nlibrary(rgdal)\nlibrary(rasterVis)\nlibrary(ggplot2)\nlibrary(terra)\nlibrary(sf)\n\n\nNow letâ€™s list all our Landsat bands except band 8 (band 8 is the panchromatic band and we donâ€™t need it here) along with our study area shapefile. Each band is a separate .TIF file.\n\n\nlibrary(stringr)\nlibrary(raster)\nlibrary(fs)\nlibrary(sf)\nlibrary(tidyverse)\n\n# List your raster files excluding band 8 using the patter argument\nlistlandsat<-dir_info(here::here(\"prac_7\", \"Lsatdata\"))%>%\n  dplyr::filter(str_detect(path, \"[B123456790].TIF\")) %>%\n  dplyr::select(path)%>%\n  pull()%>%\n  as.character()%>%\n  # Load our raster layers into a stack\n  raster::stack()\n\n# Load the manchester boundary\nmanchester_boundary <- st_read(here::here(\"prac_7\", \n                                          \"Manchester_boundary\",\n                                          \"manchester_boundary.shp\"))\n\nReading layer `manchester_boundary' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\CASA0023\\CASA0023-book\\prac_7\\Manchester_boundary\\manchester_boundary.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 540926.6 ymin: 5917117 xmax: 558663.4 ymax: 5933117\nProjected CRS: WGS 84 / UTM zone 30N\n\n#check they have the same Coordinate Reference System (CRS)\ncrs(manchester_boundary)\n\nCoordinate Reference System:\nDeprecated Proj.4 representation:\n +proj=utm +zone=30 +datum=WGS84 +units=m +no_defs \nWKT2 2019 representation:\nPROJCRS[\"WGS 84 / UTM zone 30N\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 30N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-3,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    ID[\"EPSG\",32630]] \n\ncrs(listlandsat)\n\nCoordinate Reference System:\nDeprecated Proj.4 representation:\n +proj=utm +zone=30 +datum=WGS84 +units=m +no_defs \nWKT2 2019 representation:\nPROJCRS[\"WGS 84 / UTM zone 30N\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 30N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-3,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 6Â°W and 0Â°W, northern hemisphere between equator and 84Â°N, onshore and offshore. Algeria. Burkina Faso. CÃ´te' Ivoire (Ivory Coast). Faroe Islands - offshore. France. Ghana. Gibraltar. Ireland - offshore Irish Sea. Mali. Mauritania. Morocco. Spain. United Kingdom (UK).\"],\n        BBOX[0,-6,84,0]],\n    ID[\"EPSG\",32630]] \n\n\n\n\n5.4.2 Clipping\n\nOur raster is currently the size of the scene which satellite data is distributed in, to clip it to our study area itâ€™s best to first crop it to the extent of the shapefile and then mask it as we have done in previous practicalsâ€¦\n\n\nlsatmask <- listlandsat %>%\n  # now crop our temp data to the extent\n  terra::crop(.,manchester_boundary)%>%\n  terra::mask(.,  manchester_boundary)\n\n\nIf all we wanted to do was clip our data, we could now change our filenames in the raster stack and write the .TIFF files out againâ€¦\n\n\n# add mask to the filenames within the raster stack\n\nnames(lsatmask) <- names(lsatmask)%>%\n  str_c(., \n        \"mask\", \n        sep=\"_\")\n\n# I need to write mine out in another location\noutputfilenames <-\n  str_c(\"prac_7/Lsatdata/\", \"mask/\", names(lsatmask) ,sep=\"\")\n\nIn the first line of code iâ€™m taking the original names of the raster layers and adding â€œmaskâ€ to the end of them. This is done using str_c() from the stringr package and the arguments\n\nnames(lsatmask): original raster layer names\n\"mask\": what i want to add to the names\nsep=\"\": how the names and â€œmaskâ€ should be seperated â€” â€œâ€ means no spaces\n\nAs i canâ€™t upload my Landsat files to GitHub iâ€™m storing them in a folder that is not linked (remember this is all sotred on GitHub) â€“ so you wonâ€™t find prac8_data/Lsatdata on there. If you want to store your clipped Landsat files in your project directory just use:\n\nlsatmask %>%\n  terra::writeRaster(., names(lsatmask), \n              bylayer=TRUE, \n              format='raster', \n              overwrite=TRUE)\n\nFor me though itâ€™s:\n\nlsatmask %>%\n  terra::writeRaster(., outputfilenames, \n              bylayer=TRUE, \n              format='raster', \n              overwrite=TRUE)\n\nHere i write out each raster layer individually though specifying bylayer=TRUE.You can either use the format=GTiffor the native raster format from the raster package - format='raster' it doesnâ€™t really matter as all GIS software can read all types."
  },
  {
    "objectID": "7_temperature.html#data-exploration",
    "href": "7_temperature.html#data-exploration",
    "title": "5Â  Temperature",
    "section": "5.5 Data exploration",
    "text": "5.5 Data exploration\n\n5.5.1 More loading and manipulating\n\nFor the next stage of analysis we are only interested in bands 1-7, we can either load them back in from the files we just saved or take them directly from the original raster stack.\n\n\n# either read them back in from the saved file:\n\nmanc_files<-dir_info(here::here(\"prac_7\", \"Lsatdata\", \"mask\")) %>%\n  dplyr::filter(str_detect(path, \"[B1234567]_mask.grd\")) %>%\n  dplyr::filter(str_detect(path, \"B11\", negate=TRUE))%>%\n  dplyr::select(path)%>%\n  pull()%>%\n  stack()\n\n# or extract them from the original stack\nmanc<-stack(lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B1_mask,\n                   lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B2_mask,\n                   lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B3_mask,\n                   lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B4_mask,\n                   lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B5_mask,\n                   lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B6_mask,\n                   lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B7_mask)\n\n# Name the Bands based on where they sample the electromagentic spectrum\nnames(manc) <- c('ultra-blue', 'blue', 'green', 'red', 'NIR', 'SWIR1', 'SWIR2') \n\n\nIf you want to extract specific information from a raster stack use:\n\n\ncrs(manc) # projection\nextent(manc) # extent\nncell(manc) # number of cells\ndim(manc) # number of rows, columns, layers\nnlayers(manc) # number of layers\nres(manc) # xres, yres\n\n\n\n5.5.2 Plotting data\n\nLetâ€™s actually have a look at our raster data, first in true colour (how humans see the world) and then false colour composites (using any other bands but not the combination of red, green and blue).\n\n\n# true colour composite\nmanc_rgb <- stack(manc$red, manc$green, manc$blue)\n# false colour composite\nmanc_false <- stack(manc$NIR, manc$red, manc$green)\n\nmanc_rgb %>%\n  plotRGB(.,axes=TRUE, stretch=\"lin\")\n\n\n\nmanc_false %>%\n    plotRGB(.,axes=TRUE, stretch=\"lin\")\n\n\n\n\n\n\n5.5.3 Data similarity\n\nWhat if you wanted to look at signle bands and also check the similarity between bands? ::: {.cell hash=â€˜7_temperature_cache/html/unnamed-chunk-13_d1d711231fce66b6d795c2621f8e76daâ€™}\n\n# Looking at single bands\nplot(manc$SWIR2)\n\n\n\n## How are these bands different?\n#set the plot window size (2 by 2)\npar(mfrow = c(2,2))\n#plot the bands\nplot(manc$blue, main = \"Blue\")\nplot(manc$green, main = \"Green\")\nplot(manc$red, main = \"Red\")\nplot(manc$NIR, main = \"NIR\")\n\n\n\n## Look at the stats of these bands\npairs(manc[[1:7]])\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\nWarning in graphics::par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n:::\nLow statistical significance means that the bands are sufficiently different enough in their wavelength reflectance to show different things in the image. We can also make this look a bit nicer with ggplot2 and GGally\n\nlibrary(ggplot2)\nlibrary(GGally)\n\nmanc %>%\n  terra::as.data.frame(., na.rm=TRUE)%>%\n  dplyr::sample_n(., 100)%>%\n  ggpairs(.,axisLabels=\"none\")\n\n\n\n\nYou can do much more using GGally have a look at the great documentation"
  },
  {
    "objectID": "7_temperature.html#raster-calculations",
    "href": "7_temperature.html#raster-calculations",
    "title": "5Â  Temperature",
    "section": "5.6 Raster calculations",
    "text": "5.6 Raster calculations\nNow we will move on to raster analysis in order to compute temperature from this raster data. To do so we need to generate additional raster layers, the first of which is NDVI\n\n5.6.1 NDVI\nLive green vegetation can be represented with the NIR and Red Bands through the normalised difference vegetation index (NDVI) as chlorophyll reflects in the NIR wavelength, but absorbs in the Red wavelength.\n\\[NDVI= \\frac{NIR-Red}{NIR+Red}\\]\n\n\n5.6.2 NDVI function\nOne of the great strengths of R is that is lets users define their own functions. Here we will practice writing a couple of basic functions to process some of the data we have been working with.\nOne of the benefits of a function is that it generalises some set of operations that can then be repeated over and again on different dataâ€¦ the structure of a function in R is given below:\n\nmyfunction <- function(arg1, arg2, ... ){\n  statements\n  return(object)\n}\n\nWe can use NDVI as an exampleâ€¦\n\nLetâ€™s make a function called NDVIfun\n\n\nNDVIfun <- function(NIR, Red) {\n  NDVI <- (NIR - Red) / (NIR + Red)\n  return(NDVI)\n}\n\nHere we have said our function needs two arguments NIR and Red, the next line calculates NDVI based on the formula and returns it. To be able to use this function throughout our analysis either copy it into the console or make a new R script, save it in your project then call it within this code using the source() function e.gâ€¦\n\nsource('insert file name')\n\n\nTo use the function do so throughâ€¦\n\n\nndvi <- NDVIfun(manc$NIR, manc$red)\n\nHere we call the function NDVIfun() and then provide the NIR and Red band.\n\nCheck the output\n\n\nndvi %>%\n  plot(.,col = rev(terrain.colors(10)), main = \"Landsat-NDVI\")\n\n\n\n# Let's look at the histogram for this dataset\nndvi %>%\n  hist(., breaks = 40, main = \"NDVI Histogram\", xlim = c(-.3,.8))\n\n\n\n\n\nWe can reclassify to the raster to show use what is most likely going to vegetation based on the histogram using the 3rd quartile â€” anything above the 3rd quartile we assume is vegetation.\n\n\nNote, this is an assumption for demonstration purposes, if you were to do something similar in future analysis be sure to provide reasoning with linkage to literature (e.g.Â policy or academic)\n\n\nveg <- ndvi %>%\n  reclassify(., cbind(-Inf, 0.3, NA))\n\nveg %>%\n  plot(.,main = 'Possible Veg cover')\n\n\n\n\n\nLetâ€™s look at this in relation to Manchester as a whole\n\n\nmanc_rgb %>%\n  plotRGB(.,axes = TRUE, stretch = \"lin\", main = \"Landsat True Color Composite\")\n\nveg %>%\n  plot(., add=TRUE, legend=FALSE)"
  },
  {
    "objectID": "7_temperature.html#advanced-raster-calculations",
    "href": "7_temperature.html#advanced-raster-calculations",
    "title": "5Â  Temperature",
    "section": "5.7 Advanced raster calculations",
    "text": "5.7 Advanced raster calculations\nThe goal of this final section is to set up a mini investigation to see if there is a relationship between urban area and temperature. If our hypothesis is that there is a relationship then our null is that there is not a relationshipâ€¦\n\n5.7.1 Calculating tempearture from Landsat data\nHere we are going to compute temperature from Landsat data â€” there are many methods that can be found within literature to do so but we will use the one originally developed by Artis & Carnahan (1982), recently summarised by Guha et al.Â 2018 and and Avdan and Jovanovska (2016).\nSome of the terms used our outlined in the terms section at the end of the document.\n\nCalculate the Top of Atmosphere (TOA) spectral radiance from the Digital Number (DN) using:\n\n\\[\\lambda= Grescale * QCAL + Brescale\\]\nTOA spectral radiance is light reflected off the Earth as seen from the satellite measure in radiance units.\nIn this equation Grescale and Brescale represent the gain and bias of the image, with QCAL the Digital Number (DN) â€” how the raw Landsat image is captured. To go from DN to spectral radiance we use the calibration curve, created before the launch of the sensor. Bias is the spectral radiance of the sensor for a DN of 0, Gain is the gradient of the slope for other values of DN. See page 84 in RADIOMETRIC CORRECTION OF SATELLITE IMAGES: WHEN AND WHY RADIOMETRIC CORRECTION IS NECESSARY for further information.\nGrescale and Brescale are available from the .MTL file provided when you downloaded the Landsat data. Either open this file in notepad and extract the required values for band 10 gain (MULT_BAND) and bias (ADD_BAND)\nâ€¦Or we can automate it using the MTL() function within the RStoolbox package\n\nlibrary(RStoolbox)\n\nMTL<-dir_info(here::here(\"prac_7\", \"Lsatdata\")) %>%\n  dplyr::filter(str_detect(path, \"MTL.txt\")) %>%\n  dplyr::select(path)%>%\n  pull()%>%\n  readMeta()\n\n #To see all the attributes\nhead(MTL)\n\n\nNow letâ€™s extract the values from the readMTL variable for Band 10â€¦we can either use the function getMeta() from RStoolbox of just extract the values ourselvesâ€¦\n\n\noffsetandgain <-MTL %>%\n  getMeta(\"B10_dn\", metaData = ., what = \"CALRAD\")\n\noffsetandgain\n\n       offset      gain\nB10_dn    0.1 0.0003342\n\n##OR  \noffsetandgain <- subset(MTL$CALRAD, rownames(MTL$CALRAD) == \"B10_dn\")\n\n\nRun the calculation using the band 10 raster layer\n\n\nTOA <- offsetandgain$gain *\n  lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B10_mask + \n  offsetandgain$offset\n\n\nNext convert the TOA to Brightness Temperature \\(T_b\\) using the following equation:\n\n\\[T_b=\\frac{K_2}{ln((K_1/\\lambda)+1)}\\]\nBrightness temperature is the radiance travelling upward from the top of the atmosphere to the satellite in units of the temperature of an equivalent black body.\nK1 (774.8853) and K2 (1321.0789) are pre launch calibration constants provided by USGS.\nCheck the handbook for these values\n\nInstead of hardcoding these valuesâ€¦yep, you guessed itâ€¦ we can extract them from our MTL\n\n\nCalidata <- MTL$CALBT%>%\n  terra::as.data.frame()%>%\n  mutate(Band=rownames(.))%>%\n  filter(Band==\"B10_dn\")\n\n# subset the columns\nK1 <- Calidata %>%\n  dplyr::select(K1)%>%\n  pull()\n\nK2 <- Calidata %>%\n  dplyr::select(K2)%>%\n  pull()\n\nBrighttemp <- (K2 / log((K1 / TOA) + 1))\n\nEarlier we calculated NDVI, letâ€™s use that to determine emissivity of each pixel.\n\nFirst we need to calculate the fractional vegetation of each pixel, through the equation:\n\n\\[F_v= \\left( \\frac{NDVI - NDVI_{min}}{NDVI_{max}-NDVI_{min}} \\right)^2\\] ::: {.cell hash=â€˜7_temperature_cache/html/unnamed-chunk-25_797e04879f4c13f2e77fae9e97235aaaâ€™}\nfacveg <- (ndvi-0.2/0.5-0.2)^2\n:::\nFractional vegetation cover is the ratio of vertically projected area of vegetation to the total surface extent.\nHere, \\(NDVI_{min}\\) is the minimum NDVI value (0.2) where pixels are considered bare earth and \\(NDVI_{max}\\) is the value at which pixels are considered healthy vegetation (0.5)\n\nNow compute the emissivity using:\n\n\\[\\varepsilon = 0.004*F_v+0.986\\]\n\nemiss <- 0.004*facveg+0.986\n\nEmissivity is the ratio absorbed radiation energy to total incoming radiation energy compared to a blackbody (which would absorb everything), being a measure of absorptivity.\n\nGreat, weâ€™re nearly thereâ€¦ get our LST following the equation from Weng et al.Â 2004 (also summarised in Guja et al.Â (2018) and Avdan and Jovanovska (2016)):\n\n\\[LST= \\frac{T_b}{1+(\\lambda \\varrho T_b / (p))ln\\varepsilon}\\]\nWhere:\n\\[p= h\\frac{c}{\\varrho}\\]\nOk, donâ€™t freak outâ€¦.letâ€™s start with calculating \\(p\\)\nHere we have:\n\n\\(h\\) which is Plankâ€™s constant \\(6.626 Ã— 10^-34 Js\\)\n\\(c\\) which is the velocity of light in a vaccum \\(2.998 Ã— 10^8 m/sec\\)\n\\(\\varrho\\) which is the Boltzmann constant of \\(1.38 Ã— 10^-23 J/K\\)\n\n\nBoltzmann <- 1.38*10e-23\nPlank <- 6.626*10e-34\nc <- 2.998*10e8\n\np <- Plank*(c/Boltzmann)\n\nNow for the rest of the equationâ€¦.we have the values for:\n\n\\(\\lambda\\) which is the effective wavelength of our data (10.9 for Landsat 8 band 10)\n\\(\\varepsilon\\) emissivity\n\\(T_b\\) Brightness Temperature\n\n\nRun the equation with our data\n\n\n#define remaining varaibles\nlambda <- 1.09e-5\n#run the LST calculation\nLST <- Brighttemp/(1 +(lambda*Brighttemp/p)*log(emiss))\n# check the values\nLST\n\nclass      : RasterLayer \ndimensions : 533, 592, 315536  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : 540915, 558675, 5917125, 5933115  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=30 +datum=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : 281.4855, 305.9571  (min, max)\n\n\n\nAre the values very high?â€¦ Thatâ€™s because we are in Kevlin not degrees Celciusâ€¦letâ€™s fix that and plot the map\n\n\nLST <- LST-273.15\nplot(LST)\n\n\n\n\nNice thatâ€™s our temperature data sorted."
  },
  {
    "objectID": "7_temperature.html#calucating-urban-area-from-landsat-data",
    "href": "7_temperature.html#calucating-urban-area-from-landsat-data",
    "title": "5Â  Temperature",
    "section": "5.8 Calucating urban area from Landsat data",
    "text": "5.8 Calucating urban area from Landsat data\nHow about we extract some urban area using another index and then see how our temperature data is related?\nWe will use the Normalized Difference Built-up Index (NDBI) algorithm for identification of built up regions using the reflective bands: Red, Near-Infrared (NIR) and Mid-Infrared (MIR) originally proposed by Zha et al.Â (2003).\nIt is very similar to our earlier NDVI calculation but using different bandsâ€¦\n\\[NDBI= \\frac{Short-wave Infrared (SWIR)-Near Infrared (NIR)}{Short-wave Infrared (SWIR)+Near Infrared (NIR)}\\]\nIn Landsat 8 data the SWIR is band 6 and the NIR band 5\n\nLetâ€™s compute this index nowâ€¦\n\n\nNDBI=((lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B6_mask-\n         lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B5_mask)/\n        (lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B6_mask+\n        lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B5_mask))\n\nBut do you remember our function? â€¦Well this is the same calculation we used there just with different raster layers (or bands) so we could reuse itâ€¦\n\nNDBIfunexample <- NDVIfun(lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B6_mask,\n                          lsatmask$LC08_L1TP_203023_20190513_20190521_01_T1_B5_mask)"
  },
  {
    "objectID": "7_temperature.html#urban-area-and-temperature-relationship",
    "href": "7_temperature.html#urban-area-and-temperature-relationship",
    "title": "5Â  Temperature",
    "section": "5.9 Urban area and temperature relationship",
    "text": "5.9 Urban area and temperature relationship\n\nWe could plot the varaibles agaisnt each other but there are a lot of data points\n\n\nplot(values(NDBI), values(LST))\n\n\n\n\nThis is termed the overplotting problem. So, letâ€™s just take a random subset of the same pixels from both raster layers.\n\nTo do so we need to again stack our layers\n\n\n# stack the layers\n\ncomputeddata <- LST%>%\n  stack(.,NDBI)%>%\n  terra::as.data.frame()%>%\n  na.omit()%>%\n  # take a random subset\n  dplyr::sample_n(., 500)%>%\n  dplyr::rename(Temp=\"layer.1\", NDBI=\"layer.2\")\n\n # check the output\nplot(computeddata$Temp, computeddata$NDBI)\n\n\n\n\n\nLetâ€™s jazz things up, load some more packages\n\n\nlibrary(plotly)\nlibrary(htmlwidgets)\n\n\nTransfrom the data to a data.frame to work with ggplot, then plot\n\n\nheat<-ggplot(computeddata, aes(x = NDBI, y = Temp))+\n  geom_point(alpha=2, colour = \"#51A0D5\")+\n  labs(x = \"Temperature\", \n       y = \"Urban index\",\n       title = \"Manchester urban and temperature relationship\")+\n   geom_smooth(method='lm', se=FALSE)+\n  theme_classic()+\n  theme(plot.title = element_text(hjust = 0.5))\n\n# interactive plot\nggplotly(heat)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nItâ€™s a masterpiece!\n\n\n\n\n\nggplot2 masterpiece. Source: Allison Horst data science and stats illustrations\n\n\n\n\n\nHow about plotting the whole dataset rather than a random subsetâ€¦\n\n\ncomputeddatafull <- LST%>%\n  stack(.,NDBI)%>%\n  terra::as.data.frame()%>%\n  na.omit()%>%\n  # take a random subset\n  dplyr::rename(Temp=\"layer.1\", NDBI=\"layer.2\")\n\nhexbins <- ggplot(computeddatafull, \n                  aes(x=NDBI, y=Temp)) +\n  geom_hex(bins=100, na.rm=TRUE) +\n  labs(fill = \"Count per bin\")+\n  geom_smooth(method='lm', se=FALSE, size=0.6)+\n  theme_bw()\n\nggplotly(hexbins)"
  },
  {
    "objectID": "7_temperature.html#statistical-summary",
    "href": "7_temperature.html#statistical-summary",
    "title": "5Â  Temperature",
    "section": "5.10 Statistical summary",
    "text": "5.10 Statistical summary\n\nTo see if our variables are related letâ€™s run some basic correlation\n\n\nlibrary(rstatix)\nCorrelation <- computeddatafull %>%\n  cor_test(Temp, NDBI, use = \"complete.obs\", method = c(\"pearson\"))\n\nCorrelation\n\n# A tibble: 1 Ã— 8\n  var1  var2    cor statistic     p conf.low conf.high method \n  <chr> <chr> <dbl>     <dbl> <dbl>    <dbl>     <dbl> <chr>  \n1 Temp  NDBI   0.66      394.     0    0.660     0.665 Pearson\n\n\nLetâ€™s walk through the results hereâ€¦\n\np-value: tells us whether there is a statistically significant correlation between the datasets and if that we can reject the null hypothesis if p<0.05 (there is a 95% chance that the relationship is real).\ncor: Product moment correlation coefficient\nconf.low and con.high intervals: 95% confident that the population correlation coefficient is within this interval\nstatistic value (or t, or test statistic)\n\nWe can work out the critical t value using:\n\nabs(qt(0.05/2, 198268))\n\n[1] 1.959976\n\n\nWithin this formula\n\n0.05 is the confidence level (95%)\n2 means a 2 sided test\n198268 is the degrees of freedom (df), being the number of values we have -2\n\n\ncomputeddatafull %>%\n  pull(Temp)%>%\n  length()\n\n[1] 198270\n\nlength(computeddatafull)\n\n[1] 2\n\n\nHere, as our t values is > than the critical value we can say that there is a relationship between the datasets. However, we would normally report the p-valueâ€¦\nAs p<0.05 is shows that are variables are have a statistically significant correlationâ€¦ so as urban area (assuming the index in representative) per pixel increases so does temperatureâ€¦therefore we can reject our null hypothesisâ€¦ but remember that this does not imply causation!!\nIf you want more information on statistics in R go and read YaRrr! A Pirateâ€™s Guide to R, chapter 13 on hypothesis tests."
  },
  {
    "objectID": "7_temperature.html#lsoamsoa-stats",
    "href": "7_temperature.html#lsoamsoa-stats",
    "title": "5Â  Temperature",
    "section": "5.11 LSOA/MSOA stats",
    "text": "5.11 LSOA/MSOA stats\nThis is all useful, but what if we wanted to present this analysis to local leaders in Manchester to show which areas experience the highest temperature (remember, with the limitation of this being one day!)\nNext, we will aggregate our raster data to LSOAs, taking a mean of the pixels in each LSOA.\nLSOA data: https://data-communities.opendata.arcgis.com/datasets/5e1c399d787e48c0902e5fe4fc1ccfe3\nMOSA data: https://data.cambridgeshireinsight.org.uk/dataset/output-areas/resource/0e5ac3b8-de71-4123-a334-0d1506a50288\nLetâ€™s prepare the new spatial data: ::: {.cell}\nlibrary(dplyr)\nlibrary(sf)\n\n# read in LSOA data\nUK_LSOA <- st_read(here::here(\"prac_7\", \n                                          \"LSOA\",\n                                          \"Lower_Super_Output_Area_(LSOA)_IMD_2019__(OSGB1936).shp\"))\n\nReading layer `Lower_Super_Output_Area_(LSOA)_IMD_2019__(OSGB1936)' from data source `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\CASA0023\\CASA0023-book\\prac_7\\LSOA\\Lower_Super_Output_Area_(LSOA)_IMD_2019__(OSGB1936).shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 34753 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -6.418524 ymin: 49.86474 xmax: 1.762942 ymax: 55.81107\nGeodetic CRS:  WGS 84\n\n# project it to match Manchester boundary\nUK_LSOA <- UK_LSOA %>%\n  st_transform(., 32630)\n\n# read in MSOA and project it\nMSOA <- st_read(here::here(\"prac_7\",\n                           \"MSOA\",\n                           \"Middle_Layer_Super_Output_Areas_December_2011_Generalised_Clipped_Boundaries_in_England_and_Wales.shp\")) %>%\n   st_transform(., 32630)\n\nReading layer `Middle_Layer_Super_Output_Areas_December_2011_Generalised_Clipped_Boundaries_in_England_and_Wales' from data source `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\CASA0023\\CASA0023-book\\prac_7\\MSOA\\Middle_Layer_Super_Output_Areas_December_2011_Generalised_Clipped_Boundaries_in_England_and_Wales.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 7201 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 82675.02 ymin: 5343.575 xmax: 655606.3 ymax: 657533.7\nProjected CRS: OSGB 1936 / British National Grid\n\n#select only MSOA within boundary\nmanchester_MSOA <- MSOA[manchester_boundary, , op=st_within]\n\n#select only LSOA that intersect MSOA\nmanchester_LSOA <- UK_LSOA[manchester_MSOA,]\n:::\nNext, we need to do the extraction with raster::extract(). fun() specifies how to summarise the pixels within the spatial unit (LSOA), na.rm()=TRUE ignores NA values and df=TRUE outputs the result to a dataframe.\n\n# extract mean LST value per LSOA\nLST_per_LSOA <- terra::extract(LST, manchester_LSOA, fun=mean, na.rm=TRUE, df=TRUE)\n\n# add the LSOA ID back\nLST_per_LSOA$FID<-manchester_LSOA$FID\n\n# join the average temp to the sf\nmanchester_LSOA_temp <- manchester_LSOA %>%\n  left_join(.,\n            LST_per_LSOA,\n            by=\"FID\")%>%\n  dplyr::rename(temp=layer)\n\nNow we have the temperature per LSOA, but what about the amount of land considered urban? Here, we will assume that any NDBI value above 0 means the whole pixel is considered urban. raster::extract() can also be used to get all the pixels within each spatial area (LSOA)..\n\n#define urban as NDBI greater than 0\nNDBI_urban<- NDBI > 0\n\n# Sum the pixels that are grater than 0 per LSOA\nNDBI_urban_per_LSOA <- terra::extract(NDBI_urban, manchester_LSOA, na.rm=TRUE, df=TRUE, fun=sum)\n\n# list the pixels per LSOA\nNDBI_per_LSOA_cells <- terra::extract(NDBI_urban, manchester_LSOA, na.rm=TRUE, df=TRUE, cellnumbers=TRUE)\n\n#count the pixels per LSOA\nNDBI_per_LSOA2_cells<- NDBI_per_LSOA_cells %>%\n  count(ID)\n\n#add the LSOA ID to the urban area\nNDBI_urban_per_LSOA$FID<-manchester_LSOA$FID\n\n#add the LSOA ID to the number of cells\nNDBI_per_LSOA2_cells$FID<-manchester_LSOA$FID\n\n#join these two\nUrban_info_LSOA <- NDBI_urban_per_LSOA %>%\n  left_join(.,\n            NDBI_per_LSOA2_cells,\n            by=\"FID\")\n\n# remove what you don't need and rename\nUrban_info_LSOA_core_needed <- Urban_info_LSOA %>%\n  dplyr::rename(urban_count=layer, \n                LSOA_cells=n) %>%\n  dplyr::select(urban_count,\n         LSOA_cells,\n         FID)%>%\n  dplyr::mutate(percent_urban=urban_count/LSOA_cells*100)\n\n# join the data \n# one sf with temp and % urban per LSOA\nmanchester_LSOA_temp_urban <- manchester_LSOA_temp %>%\n  left_join(.,\n             Urban_info_LSOA_core_needed,\n             by=\"FID\")"
  },
  {
    "objectID": "7_temperature.html#mapping",
    "href": "7_temperature.html#mapping",
    "title": "5Â  Temperature",
    "section": "5.12 Mapping",
    "text": "5.12 Mapping\nNow, we could map both temperature (and the % of urban area) within a LSOA individuallyâ€¦.In our map we want to include some place names from Open Street Map, so letâ€™s get those from OSM that is stored on Geofabrik\n\nPlaces <- st_read(here::here(\"prac_7\", \n                                          \"OSM\",\n                                          \"gis_osm_places_free_1.shp\")) %>%\n   st_transform(., 32630)\n\nReading layer `gis_osm_places_free_1' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\CASA0023\\CASA0023-book\\prac_7\\OSM\\gis_osm_places_free_1.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 635 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -2.717277 ymin: 53.33978 xmax: -1.921855 ymax: 53.67854\nGeodetic CRS:  WGS 84\n\nmanchester_Places <- Places[manchester_boundary,]%>%\n  filter(fclass==\"city\")\n\nLetâ€™s make a map, like we did earlier CASA0005 using the tmap package, remember to add a caption in Rmarkdown include the argument fig.cap=\"caption here\" in the code chunk header.\n\n# this first bit makes the box bigger\n# so we can have a north arrow not overlapping the data\n# see: https://www.jla-data.net/eng/adjusting-bounding-box-of-a-tmap-map/\nbbox_new <- st_bbox(manchester_LSOA_temp_urban) # current bounding box\n\nyrange <- bbox_new$ymax - bbox_new$ymin # range of y values\n\nbbox_new[4] <- bbox_new[4] + (0.1 * yrange) # ymax - top\nbbox_new[2] <- bbox_new[2] - (0.1 * yrange) # ymin - bottom\n\n# the plot starts here\nlibrary(tmap)\ntmap_mode(\"plot\")\n# set the new bbox\n# remove bbox=bbox_new to see the difference\ntm1 <- tm_shape(manchester_LSOA_temp_urban, bbox = bbox_new) + \n  tm_polygons(\"temp\",\n              palette=\"OrRd\",\n              legend.hist=TRUE,\n              title=\"Temperature\")+\n  tm_shape(manchester_Places, bbox=bbox_new)+\n  tm_dots(size=0.1, col=\"white\")+\n  tm_text(text=\"name\", size=0.75, ymod=-0.5, col=\"white\", fontface = \"bold\")+\n  #tm_legend(show=FALSE)+\n  tm_layout(frame=FALSE,\n            legend.outside=TRUE)+\n  tm_compass(type = \"arrow\", size=1, position = c(\"left\", \"top\")) +\n  tm_scale_bar(position= c(\"left\", \"bottom\"), breaks=c(0,2,4), text.size = .75)\n  #tm_credits(\"(a)\", position=c(0,0.85), size=1.5)\n\ntm1\n\n\n\n\nAverage temperature per LSOA in Manchester, calcualted from Landsat imagery dated 13/5/19, following the methodology specified by Guha et al.Â 2018\n\n\n\n\nThis could also be repeated for urban area and plotted in one figure, perhaps with an inset map like we previously did. But, we can also now plot two choropleth maps over each other, this is made much easier with the biscale() package, however, now we must use ggplot as opposed to tmap, you can force tmap to do it, but the feature is still in development."
  },
  {
    "objectID": "7_temperature.html#bivariate-mapping-optional",
    "href": "7_temperature.html#bivariate-mapping-optional",
    "title": "5Â  Temperature",
    "section": "5.13 Bivariate mapping (optional)",
    "text": "5.13 Bivariate mapping (optional)\nWhilst bivaraite maps look cool, they donâ€™t tell us anything about the actual data values, simply dividing the data into classes based on the style parameter which we have seen before - jenks, equal and so on. So here I want to produce:\n\nA central bivariate map of LSOAs within Manchester\nThe central bivariate will have the MSOA boundaries and some place names\nTwo plots showing the distribution of the data\n\n\nlibrary(biscale)\nlibrary(cowplot)\nlibrary(sysfonts)\nlibrary(extrafont) \nlibrary(showtext) # more fonts\n#font_add_google(\"Lato\", regular.wt = 300, bold.wt = 700) # I like using Lato for data viz (and everything else...). Open sans is also great for web viewing.\nshowtext_auto()\n\n# create classes\ndata <- bi_class(manchester_LSOA_temp_urban, x = temp, y = percent_urban, style = \"jenks\", dim = 3)\n\n#ggplot map\nmap <- ggplot() +\n geom_sf(data = data, mapping = aes(fill = bi_class), color=NA, lwd = 0.1, show.legend = FALSE) +\n  bi_scale_fill(pal = \"DkViolet\", dim = 3) +\n  geom_sf(data = manchester_MSOA, mapping = aes(fill=NA), color=\"black\", alpha=0, show.legend = FALSE)+\n  geom_sf(data=manchester_Places, mapping=aes(fill=NA), color=\"white\", show.legend = FALSE)+\n  geom_sf_text(data=manchester_Places, aes(label = name, hjust = 0.5, vjust = -0.5),\n               nudge_x = 0, nudge_y = 0,\n               fontface = \"bold\",\n             color = \"white\",\n             show.legend = FALSE,\n             inherit.aes = TRUE)+\n  labs(\n    title = \"\",\n    x=\"\", y=\"\"\n  ) +\n  bi_theme()\n\nlegend <- bi_legend(pal = \"DkViolet\",\n                    dim = 3,\n                    xlab = \"Temperature \",\n                    ylab = \"% Urban\",\n                    size = 8)\n\ncredit<- (\"Landsat dervied temperature and urban area, taken 13/5/19\")\n\n# combine map with legend\nfinalPlot <- ggdraw() +\n  draw_plot(map, 0, 0, 1, 1) +\n  draw_plot(legend, 0.1, 0.1, 0.2, 0.2)\n  #draw_text(credit, 0.68, 0.1, 0.2, 0.2, size=10)\nfinalPlot\n\n\n\n\nThatâ€™s the main bivaraite plot, now to the side plotsâ€¦\n\nurban_box<-ggplot(data, aes(x=bi_class, y=percent_urban, fill=bi_class)) +\n  geom_boxplot()+\n  scale_fill_manual(values=c(\"#CABED0\", \"#BC7C8F\", \"#806A8A\", \"#435786\", \"#AE3A4E\", \"#77324C\", \"#3F2949\", \"#3F2949\"))+\n  labs(x=\"Bivariate class (temp, urban)\", \n       y=\"Urban %\")+\n  theme_light()+\n  theme(legend.position=\"none\") # Remove legend\n\ntemp_violin<-ggplot(data, aes(x=bi_class, y=temp, fill=bi_class))+\n  geom_violin()+\n  scale_fill_manual(values=c(\"#CABED0\", \"#BC7C8F\", \"#806A8A\", \"#435786\", \"#AE3A4E\", \"#77324C\", \"#3F2949\", \"#3F2949\"))+\n  labs(x=\"\", \n       y=\"Temperature\")+\n   guides(fill=guide_legend(title=\"Class\"))+\n  theme_light()+\n  theme(legend.position=\"none\") # Remove legend\n\nJoin them all together in two steps - make the side plots, then join that to the main plot\n\nside <- plot_grid(temp_violin, urban_box, labels=c(\"B\",\"C\"),label_size = 12, ncol=1)\n\nall <- plot_grid(finalPlot, side, labels = c('A'), label_size = 12, ncol = 2,  rel_widths = c(2, 1))\n\nNow this looks a bit rubbish on the page. Usually, you might want to export the map to a .png to use elsewhere, but you will need to resize it. I like to plot the map in the console (type all in the console then press enter), click export in the plots tab, reszie the image and note down the numbers then input them below..\n\nall\n\n\n\ndev.copy(device = png, filename = here::here(\"prac_7\", \"bivaraite.png\"), width = 687, height = 455) \n\npng \n  3 \n\ndev.off()\n\npng \n  2 \n\n\n\n\n\n\n\n\n\n\n\n\n5.13.1 Map notes\n\nThe Manchester outline shapefile doesnâ€™t match up with either the MSOA or LSOA boundaries, this could be solved by making sure the boundary data (e.g.Â city outline) matches any other spatial units.\nWhen selecting the data (within the Manchester boundary) any MSOA shape that was within the boundary selected\nThen any LSOA that intersected the MSOA layer was selected\nTo resolve this you could clip the MSOA layer to the current displayed layer to force a boundary around the map\nA box plot was used as the data was very spread out in the urban % plot, so a violin plot didnâ€™t show much."
  },
  {
    "objectID": "7_temperature.html#considerations",
    "href": "7_temperature.html#considerations",
    "title": "5Â  Temperature",
    "section": "5.14 Considerations",
    "text": "5.14 Considerations\nIf you wanted to explore this type of analysis further then you would need to consider the following:\n\nOther methods for extracting temperature from Landsat data\nOther methods for identifying urban area from Landsat data such as image classificaiton\nThe formula used to calculate emissivity â€” there are many\nThe use of raw satellite data as opposed to remove the effects of the atmosphere. Within this practical we have only used relative spatial indexes (e.g.Â NDVI). However, if you were to use alternative methods it might be more appropriate to use surface reflectance data (also provided by USGS)."
  },
  {
    "objectID": "7_temperature.html#references",
    "href": "7_temperature.html#references",
    "title": "5Â  Temperature",
    "section": "5.15 References",
    "text": "5.15 References\nThanks to former CASA graduate student Dr Matt Ng for providing the outline to the start of this practical\nAvdan, U. and Jovanovska, G., 2016. Algorithm for automated mapping of land surface temperature using LANDSAT 8 satellite data. Journal of Sensors, 2016.\nGuha, S., Govil, H., Dey, A. and Gill, N., 2018. Analytical study of land surface temperature with NDVI and NDBI using Landsat 8 OLI and TIRS data in Florence and Naples city, Italy. European Journal of Remote Sensing, 51(1), pp.667-678.\nWeng, Q., Lu, D. and Schubring, J., 2004. Estimation of land surface temperatureâ€“vegetation abundance relationship for urban heat island studies. Remote sensing of Environment, 89(4), pp.467-483.\nYoung, N.E., Anderson, R.S., Chignell, S.M., Vorster, A.G., Lawrence, R. and Evangelista, P.H., 2017. A survival guide to Landsat preprocessing. Ecology, 98(4), pp.920-932.\nZha, Y., Gao, J. and Ni, S., 2003. Use of normalized difference built-up index in automatically mapping urban areas from TM imagery. International journal of remote sensing, 24(3), pp.583-594."
  },
  {
    "objectID": "7_temperature.html#remote-sensing-refresher",
    "href": "7_temperature.html#remote-sensing-refresher",
    "title": "5Â  Temperature",
    "section": "5.16 Remote sensing refresher",
    "text": "5.16 Remote sensing refresher\nLandsat sensors capture reflected solar energy, convert these data to radiance, then rescale this data into a Digital Number (DN), the latter representing the intensity of the electromagnetic radiation per pixel. The range of possible DN values depends on the sensor radiometric resolution. For example Landsat Thematic Mapper 5 (TM) measures between 0 and 255 (termed 8 bit), whilst Landsat 8 OLI measures between 0 and 65536 (termed 12 bit). These DN values can then be converted into Top of Atmosphere (TOA) radiance and TOA reflectance through available equations and known constants that are preloaded into certain software. The former is how much light the instrument sees in meaningful units whilst the latter removes the effects of the light source. However, TOA reflectance is still influenced by atmospheric effects. These atmospheric effects can be removed through atmospheric correction achievable in software such as ENVI and QGIS to give surface reflectance representing a ratio of the amount of light leaving a target to the amount of light striking it.\nWe must also consider the spectral resolution of satellite imagery, Landsat 8 OLI has 11 spectral bands and as a result is a multi-spectral sensor. As humans we see in the visible part of the electromagnetic spectrum (red-green-blue) â€” this would be three bands of satellite imagery â€” however satellites can take advantage of the rest of the spectrum. Each band of Landsat measures in a certain part of the spectrum to produce a DN value. We can then combine these values to produce â€˜colour compositesâ€™. So a â€˜trueâ€™ colour composite is where red, green and blue Landsat bands are displayed (the visible spectrum). Based on the differing DN values obtained, we can pick out the unique signatures (values of all spectral bands) of each land cover type, termed spectral signature.\nFor more information read Young et al.Â (2017) A survival guide to Landsat preprocessing"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "6Â  Resources",
    "section": "",
    "text": "https://github.com/robmarkcole/satellite-image-deep-learning#techniques\nhttps://urbanspatial.github.io/classifying_satellite_imagery_in_R/#Supervised_Classification\nhttps://opengislab.com/blog/2018/5/14/flood-mapping-with-sentinel-1-data-using-snap-and-qgis"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7Â  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  }
]